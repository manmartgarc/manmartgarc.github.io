<!doctype html><html lang=en dir=auto data-theme=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1314&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Recommender Systems: Content-based Filtering | Manuel Martinez</title><meta name=keywords content="Machine Learning,Recommendation Systems"><meta name=description content="Content-Based Filtering
This paradigm is very straightforward. It uses item features to recommend other items similar to what the user likes, based on their previous actions or explicit feedback. 1 There are many approaches to defining similarity, and implicitly, this is how we are defining pertinence. That is, we think that items that are similar to other items that the user has seen in the past are pertinent to that user based on whatever similarity we have chosen."><meta name=author content="Manuel Martinez"><link rel=canonical href=http://localhost:1314/posts/content-filtering/content-based/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1314/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1314/images/favicon.ico><link rel=icon type=image/png sizes=32x32 href=http://localhost:1314/images/favicon.ico><link rel=apple-touch-icon href=http://localhost:1314/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1314/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1314/posts/content-filtering/content-based/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="http://localhost:1314/posts/content-filtering/content-based/"><meta property="og:site_name" content="Manuel Martinez"><meta property="og:title" content="Recommender Systems: Content-based Filtering"><meta property="og:description" content="Content-Based Filtering This paradigm is very straightforward. It uses item features to recommend other items similar to what the user likes, based on their previous actions or explicit feedback. 1 There are many approaches to defining similarity, and implicitly, this is how we are defining pertinence. That is, we think that items that are similar to other items that the user has seen in the past are pertinent to that user based on whatever similarity we have chosen."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-12T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-12T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Recommendation Systems"><meta name=twitter:card content="summary"><meta name=twitter:title content="Recommender Systems: Content-based Filtering"><meta name=twitter:description content="Content-Based Filtering
This paradigm is very straightforward. It uses item features to recommend other items similar to what the user likes, based on their previous actions or explicit feedback. 1 There are many approaches to defining similarity, and implicitly, this is how we are defining pertinence. That is, we think that items that are similar to other items that the user has seen in the past are pertinent to that user based on whatever similarity we have chosen."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1314/posts/"},{"@type":"ListItem","position":2,"name":"Recommender Systems: Content-based Filtering","item":"http://localhost:1314/posts/content-filtering/content-based/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Recommender Systems: Content-based Filtering","name":"Recommender Systems: Content-based Filtering","description":"Content-Based Filtering This paradigm is very straightforward. It uses item features to recommend other items similar to what the user likes, based on their previous actions or explicit feedback. 1 There are many approaches to defining similarity, and implicitly, this is how we are defining pertinence. That is, we think that items that are similar to other items that the user has seen in the past are pertinent to that user based on whatever similarity we have chosen.\n","keywords":["Machine Learning","Recommendation Systems"],"articleBody":"Content-Based Filtering This paradigm is very straightforward. It uses item features to recommend other items similar to what the user likes, based on their previous actions or explicit feedback. 1 There are many approaches to defining similarity, and implicitly, this is how we are defining pertinence. That is, we think that items that are similar to other items that the user has seen in the past are pertinent to that user based on whatever similarity we have chosen.\nWe will follow the simple example from here. Imagine that we have an app store and we want to recommend apps to our users.\nWe have $m$ apps in the app store. As diligent data-oriented persons we have also engineered $n$ features for each of the $m$ apps. Keeping things simple, we are just using binary features. You slap all these things together into a matrix: $\\underset{m \\times n}{X}$. To keep things simple, you generate $n = 6$ features. The first three have to do with the app’s category, and the latter three are related to the developers. Setting $m = 3$, then we would have three apps with six features each, so that $X$ has dimensions $3 \\times 6$, and it would look like this:\nEducation Casual Health TimeWastr Science R Us Healthcare 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 Or more succinctly:\n$$ X = \\ \\begin{bmatrix} 1 \u0026 0 \u0026 0 \u0026 0 \u0026 1 \u0026 0 \\\\ 0 \u0026 1 \u0026 0 \u0026 1 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \u0026 0 \u0026 1 \u0026 1 \\end{bmatrix} $$\nOkay, so we have the apps in some feature space we defined. What about the users? The key thing about this approach is to represent the user using the same feature space as the apps. This will allow us to directly compute the similarity of the user vector with each of the candidate apps vectors. We will work with our user Alice, and their feature vector is represented by a column vector $\\underset{n \\times 1}{A}$.\nSay that our user Alice, communicated to us when creating their account that they are interested in Educational topics. Since the Education category is feature $j = 0$ then $A[0, 0] = 1$. This is an example of an explicit feature, i.e. the user told us something. We can also use implicit features, such as the download history of the user. Say for example that Alice has downloaded an app developed by Science R Us and Healthcare before. Since we have a feature for each developer team, we set those to 1 in Alice’s feature vector; in this case $A[0, 4] = 1, A[0, 5] = 1$. This means that Alice’s feature column vector looks like this:\n$$ A = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ \\end{bmatrix} $$\nYou might guess that we are going to take a dot product somewhere. Which would work great since we are using binary features, but this is not the only option.\nSimilarity Metrics Collaborative Filtering Content-based Filtering | Google Machine Learning Education ↩︎\n","wordCount":"526","inLanguage":"en","datePublished":"2023-06-12T00:00:00Z","dateModified":"2023-06-12T00:00:00Z","author":[{"@type":"Person","name":"Manuel Martinez"}],"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1314/posts/content-filtering/content-based/"},"publisher":{"@type":"Organization","name":"Manuel Martinez","logo":{"@type":"ImageObject","url":"http://localhost:1314/images/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1314/ accesskey=h title="Manuel Martinez (Alt + H)">Manuel Martinez</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1314/about/ title=About><span>About</span></a></li><li><a href=http://localhost:1314/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1314/projects/ title=Projects><span>Projects</span></a></li><li><a href=http://localhost:1314/cv/ title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1314/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1314/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Recommender Systems: Content-based Filtering
<span class=entry-hint title=Draft><svg height="35" viewBox="0 -960 960 960" fill="currentColor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-meta><span title='2023-06-12 00:00:00 +0000 UTC'>June 12, 2023</span>&nbsp;·&nbsp;<span>3 min</span>&nbsp;·&nbsp;<span>Manuel Martinez</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#content-based-filtering>Content-Based Filtering</a></li><li><a href=#collaborative-filtering>Collaborative Filtering</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h3 id=content-based-filtering>Content-Based Filtering<a hidden class=anchor aria-hidden=true href=#content-based-filtering>#</a></h3><p>This paradigm is very straightforward. It uses item features to recommend other items similar to what the user likes, based on their previous actions or explicit feedback. <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> There are many approaches to defining similarity, and implicitly, this is how we are defining pertinence. That is, we think that items that are similar to other items that the user has seen in the past are pertinent to that user based on whatever <a href=/posts/content-filtering/content-based/#similarity-metrics>similarity</a> we have chosen.</p><p>We will follow the simple example from <a href=https://developers.google.com/machine-learning/recommendation/content-based/basics>here</a>. Imagine that we have an app store and we want to recommend apps to our users.</p><p>We have $m$ apps in the app store. As diligent data-oriented persons we have also engineered $n$ features for each of the $m$ apps. Keeping things simple, we are just using binary features. You slap all these things together into a matrix: $\underset{m \times n}{X}$. To keep things simple, you generate $n = 6$ features. The first three have to do with the app&rsquo;s category, and the latter three are related to the developers. Setting $m = 3$, then we would have three apps with six features each, so that $X$ has dimensions $3 \times 6$, and it would look like this:</p><table><thead><tr><th>Education</th><th>Casual</th><th>Health</th><th>TimeWastr</th><th>Science R Us</th><th>Healthcare</th></tr></thead><tbody><tr><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr><tr><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr></tbody></table><p>Or more succinctly:</p><p>$$
X = \
\begin{bmatrix}
1 & 0 & 0 & 0 & 1 & 0 \\
0 & 1 & 0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 & 1 & 1
\end{bmatrix}
$$</p><p>Okay, so we have the apps in some feature space we defined. What about the users? The key thing about this approach is to represent the user using <em>the same</em> feature space as the apps. This will allow us to directly compute the similarity of the user vector with each of the candidate apps vectors. We will work with our user Alice, and their feature vector is represented by a column vector $\underset{n \times 1}{A}$.</p><p>Say that our user Alice, communicated to us when creating their account that they are interested in Educational topics. Since the <em>Education</em> category is feature $j = 0$ then $A[0, 0] = 1$. This is an example of an explicit feature, i.e. the user told us something. We can also use implicit features, such as the download history of the user. Say for example that Alice has downloaded an app developed by Science R Us and Healthcare before. Since we have a feature for each developer team, we set those to 1 in Alice&rsquo;s feature vector; in this case $A[0, 4] = 1, A[0, 5] = 1$. This means that Alice&rsquo;s feature column vector looks like this:</p><p>$$
A = \begin{bmatrix}
1 \\
0 \\
0 \\
0 \\
1 \\
1 \\
\end{bmatrix}
$$</p><p>You might guess that we are going to take a dot product somewhere. Which would work great since we are using binary features, but this is not the only option.</p><h4 id=similarity-metrics>Similarity Metrics<a hidden class=anchor aria-hidden=true href=#similarity-metrics>#</a></h4><h3 id=collaborative-filtering>Collaborative Filtering<a hidden class=anchor aria-hidden=true href=#collaborative-filtering>#</a></h3><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href="https://developers.google.com/machine-learning/recommendation/content-based/basics#:~:text=Content%2Dbased%20filtering%20uses%20item,previous%20actions%20or%20explicit%20feedback.">Content-based Filtering | Google Machine Learning Education</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1314/tags/machine-learning/>Machine Learning</a></li><li><a href=http://localhost:1314/tags/recommendation-systems/>Recommendation Systems</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1314/posts/coursera/deep-learning-specialization/nn-dl/week1/><span class=title>« Prev</span><br><span>Neural Networks and Deep Learning: Week 1 | Introduction to Deep Learning</span>
</a><a class=next href=http://localhost:1314/posts/content-filtering/what-is/><span class=title>Next »</span><br><span>Recommender Systems: What are they?</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=http://localhost:1314/>Manuel Martinez</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>