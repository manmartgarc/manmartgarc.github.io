<!doctype html><html lang=en dir=auto data-theme=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Improving Deep Learning Networks: Week 3 | Hyperparameter Tuning, Batch Optimization, Programming Frameworks | Manuel Martinez</title><meta name=keywords content="machine learning,deep learning"><meta name=description content="This is the third and final week of the second course of DeepLearning.AI&rsquo;s Deep Learning Specialization offered on Coursera.
This week&rsquo;s topics are:

Hyperparameter Tuning

Tuning Process

Random Search
Coarse-to-fine Grained Search


Using an Appropriate Scale when Searching

Python Implementation


Hyperparameter Tuning in Practice: Pandas vs. Caviar


Batch Normalization

Normalizing Activations in a Network
Fitting Batch Norm into a Neural Network
Why does Batch Norm work?
Batch Norm at Test Time


Multi-class Classification

Softmax Regression
Training a Softmax Classifier


Programming Frameworks


Hyperparameter Tuning
We have seen by now that neural networks have a lot of hyperparameters. Remember that hyperparameters remain fixed during training. This means that the process of finding reasonable hyperparameters, called hyperparameter tuning, is a process that is separate from training your model."><meta name=author content="Manuel Martinez"><link rel=canonical href=http://localhost:1313/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/images/favicon.ico><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/images/favicon.ico><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="http://localhost:1313/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/"><meta property="og:site_name" content="Manuel Martinez"><meta property="og:title" content="Improving Deep Learning Networks: Week 3 | Hyperparameter Tuning, Batch Optimization, Programming Frameworks"><meta property="og:description" content="This is the third and final week of the second course of DeepLearning.AI’s Deep Learning Specialization offered on Coursera.
This week’s topics are:
Hyperparameter Tuning Tuning Process Random Search Coarse-to-fine Grained Search Using an Appropriate Scale when Searching Python Implementation Hyperparameter Tuning in Practice: Pandas vs. Caviar Batch Normalization Normalizing Activations in a Network Fitting Batch Norm into a Neural Network Why does Batch Norm work? Batch Norm at Test Time Multi-class Classification Softmax Regression Training a Softmax Classifier Programming Frameworks Hyperparameter Tuning We have seen by now that neural networks have a lot of hyperparameters. Remember that hyperparameters remain fixed during training. This means that the process of finding reasonable hyperparameters, called hyperparameter tuning, is a process that is separate from training your model."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-21T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-21T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Improving Deep Learning Networks: Week 3 | Hyperparameter Tuning, Batch Optimization, Programming Frameworks"><meta name=twitter:description content="This is the third and final week of the second course of DeepLearning.AI&rsquo;s Deep Learning Specialization offered on Coursera.
This week&rsquo;s topics are:

Hyperparameter Tuning

Tuning Process

Random Search
Coarse-to-fine Grained Search


Using an Appropriate Scale when Searching

Python Implementation


Hyperparameter Tuning in Practice: Pandas vs. Caviar


Batch Normalization

Normalizing Activations in a Network
Fitting Batch Norm into a Neural Network
Why does Batch Norm work?
Batch Norm at Test Time


Multi-class Classification

Softmax Regression
Training a Softmax Classifier


Programming Frameworks


Hyperparameter Tuning
We have seen by now that neural networks have a lot of hyperparameters. Remember that hyperparameters remain fixed during training. This means that the process of finding reasonable hyperparameters, called hyperparameter tuning, is a process that is separate from training your model."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Improving Deep Learning Networks: Week 3 | Hyperparameter Tuning, Batch Optimization, Programming Frameworks","item":"http://localhost:1313/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Improving Deep Learning Networks: Week 3 | Hyperparameter Tuning, Batch Optimization, Programming Frameworks","name":"Improving Deep Learning Networks: Week 3 | Hyperparameter Tuning, Batch Optimization, Programming Frameworks","description":"This is the third and final week of the second course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera.\nThis week\u0026rsquo;s topics are:\nHyperparameter Tuning Tuning Process Random Search Coarse-to-fine Grained Search Using an Appropriate Scale when Searching Python Implementation Hyperparameter Tuning in Practice: Pandas vs. Caviar Batch Normalization Normalizing Activations in a Network Fitting Batch Norm into a Neural Network Why does Batch Norm work? Batch Norm at Test Time Multi-class Classification Softmax Regression Training a Softmax Classifier Programming Frameworks Hyperparameter Tuning We have seen by now that neural networks have a lot of hyperparameters. Remember that hyperparameters remain fixed during training. This means that the process of finding reasonable hyperparameters, called hyperparameter tuning, is a process that is separate from training your model.\n","keywords":["machine learning","deep learning"],"articleBody":"This is the third and final week of the second course of DeepLearning.AI’s Deep Learning Specialization offered on Coursera.\nThis week’s topics are:\nHyperparameter Tuning Tuning Process Random Search Coarse-to-fine Grained Search Using an Appropriate Scale when Searching Python Implementation Hyperparameter Tuning in Practice: Pandas vs. Caviar Batch Normalization Normalizing Activations in a Network Fitting Batch Norm into a Neural Network Why does Batch Norm work? Batch Norm at Test Time Multi-class Classification Softmax Regression Training a Softmax Classifier Programming Frameworks Hyperparameter Tuning We have seen by now that neural networks have a lot of hyperparameters. Remember that hyperparameters remain fixed during training. This means that the process of finding reasonable hyperparameters, called hyperparameter tuning, is a process that is separate from training your model.\nTuning Process As mentioned above, neural networks can have a lot of parameters:\n$\\alpha$: the learning rate. $\\beta$: the EWMA term in momentum. $\\beta_1, \\beta_2, \\epsilon$: the EWMA parameters for Adam. The number of layers $L$. The number of hidden units $n^{[l]}$. Learning rate decay rate if using learning rate decay. The size of mini batches. If it seems daunting, it’s because it is. Hyperparameter tuning is necessary most of the time because good hyperparameters from one problem not always translate to other problems. However, there are some hyperparameters that are usually more important than others. This can help you guide your tuning to focus on the most important ones first.\nIn the course, Andrew defines three tiers of importance for hyperparameter tuning, the first ones being more important than the latter ones:\n$\\alpha$ $\\beta$ if using momentum. Using $\\beta = 0.9$ is a good default. The number of hidden layers and the mini-batch size. Learning rate decay, and $L$. If using Adam the defaults of $\\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon=10^{-8}$ usually work fine.\nNow we know where to focus first. But how do we actually search and evaluate values?\nIn the earlier generation of machine learning algorithms, researchers would usually use grid search. Grid search is an exhaustive search over a manually specified subset of the hyperparameter space, usually using cross validation. It’s called a grid because if you imagine having two hyperparameters $h_1, h_2$, and some range of values and a step-size for each of the ranges $r_1, r_2$, then you can imagine a grid or matrix where each cell is a combination of $h_1, h_2$ over each of their ranges. For example, you might say, I want to search a value for $\\alpha$ in the interval $[0.001, 0.1]$, and I will draw some equidistant samples in that range and evaluate all the values. Notice that the “grid” becomes a matrix, or $n$-dimensional tensor as you add more hyperparameters; this is terrible because the volume of the search space increases exponentially with each additional dimension or hyperparameter.\nRandom Search Staying within the simple example of using two hyperparameters $h_1, h_2$, the problem with grid search might be apparent: you evaluate the same value for each hyperparameter multiple times. If set a grid of $9$ cells, then each value of both $h_1, h_2$ will be tested three times while holding the other constant. This is why the recommendation is to use random search: where each combination of $h_1, h_2$ is unique (the probability of repetition being low if you sample appropriately at random). The following image illustrates this point:\nBergstra and Bengio | Hyperparameter Search\nCoarse-to-fine Grained Search Another, optional, recommendation is to first find a promising range for search and then zoom in within the range. This is called coarse-to-fine grained search.\nUsing an Appropriate Scale when Searching So we know that we should sample the hyperparameter space at random. However, this doesn’t necessarily mean to do it uniformly at random. The reason for this is that the search scale for each hyperparameter matters a lot. Being able to communicate the desired scale to the random sampling process is crucial for efficient search.\nFor some hyperparameters, such as the size of a hidden-layer $l$, $n^{[l]}$, it might be reasonable to sample uniformly at random. Say that you want to search for $n^{[l]}$ over the range $[50, 100]$. Then values like $89, 57, 62, 89, 74$ might be reasonable (actually sampled at random).\nRemember that a continuous uniform distribution has a PDF of $\\frac{1}{b-a}$ where $a,b$ are the minimum and maximum values. Therefore, every value in the range $[a, b]$ has the same probability of being realized.\nThis will not work for a hyperparameter like $\\alpha$, however. Say that we suspect that a good $\\alpha$ value is in the range $[0.0001, 1]$, and we sample uniformly at random. Since every value has the same likelihood of being drawn, this means that values between $[0.1, 1]$ will be sampled with $90\\%$ probability. What about the interval $[0.0001, 0.1)$? These values have only a $10\\%$ probability of being drawn. This is terrible because we are equally interested in values in both ranges.\nThis is why it makes more sense to sample for $\\alpha$ values on the log scale as opposed to the linear scale. When we search over the linear scale and sample uniformly at random, we will spend $10\\%$ of our effort searching over $[0.0001, 0.1)$ and $90\\%$ of our effort searching over $[0.1, 1]$. On the other hand, when using a log-scale and sampling uniformly at random, we will spend an equal amount of effort searching over $[0.0001, 0.001), [0.001, 0.01), [0.01, 0.1), [0.1, 1]$.\nPython Implementation The way to implement the example above is as follows on Python:\nnp.random.seed(1337) r = -4 * np.random.rand() print(10 ** r) 0.0895161303335359 Let’s break this down. Remember that np.random.rand() generates random samples from a uniform distribution over the interval $[0, 1]$. Now, when we want to sample uniformly at random on a log scale, we want to sample uniformly in the exponents. What exponents? In our case, we want to sample uniformly in the range of $[0.0001, 1]$ which is the same as $[10^{-4}, 10^{0}]$. So if we can draw random samples $r$ from a uniform random distribution in the range $[-4, 0]$, then we can plug those into $10^{-r}$ and thus generate the samples in the log scale. How do we get $-4$? It’s as simple as $-4 = \\log_{10}0.0001$.\nLet’s generate $10$ such values for $\\alpha$ in a vectorized way:\nnp.random.seed(1337) rs = -4 * np.random.rand(10) # we draw 10 samples print(10 ** rs) # this is vectorized over `rs` array([2.31880438e-01, 7.71780714e-02, 1.45456272e-02, 5.19993408e-02, 8.44167674e-03, 8.95835560e-02, 1.24640408e-04, 1.17149864e-03, 3.45862195e-01, 2.85036007e-02]) Hyperparameter Tuning in Practice: Pandas vs. Caviar Andrew mentions that there are two ways that hyperparameter tuning happens in practice: pandas vs. caviar. No, it’s not related to the pandas Python package. It’s related to how the animal, the giant panda, has offspring. Pandas usually have very few offspring and therefore put a lot of effort into the upbringing of each (don’t think too heavily on the veracity of this statement). This is contrasted to other species that lay thousands of eggs, where each egg is almost left to chance (again no offense to the regal Giant Pacific Octopus mothers whom sometimes give their lives for their precious 120,000 to 400,000 eggs).\nThe idea is that sometimes your model is too big, and you cannot afford to train multiple instances of your model, so must babysit it like a little panda. That is, adjusting the hyperparameters over longer periods of time. On the other hand, the caviar approach is where you can afford to train multiple models in parallel with different hyperparameters and then pick the winner.\nBatch Normalization You might remember how we mentioned that normalizing your inputs could help our optimization run faster. The issue with this is that even though our input layer is getting normalized values, the outputs of each layer $l$, $A^{[l]}$ are no longer normalized. Batch normalization is applying the same reasoning but on each layer.\nBatch normalization doesn’t have anything to do with batch or mini-batch gradient descent and can be implemented under both approaches.\nA key thing is that we won’t normalize $A^{[l]}$ but $Z^{[l]}$ instead. Let’s go over the steps.\nNormalizing Activations in a Network For a given layer $l$ in our network, under the batch gradient descent approach, we compute:\n$$ \\begin{equation} Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\end{equation} $$\nNow, we will normalize $Z^{[l]}$ via standardization:\n$$ \\begin{equation} Z^{[l]}_{norm} = \\frac{Z^{[l]} - \\bar{Z}^{[l]}}{S^{[l]} + \\epsilon} \\end{equation} $$\nWhere $\\bar{Z}^{[l]}$ is the sample mean of $Z^{[l]}$ and $S^{[l]}$ is the sample standard deviation of $Z^{[l]}$.\nFinally, we compute $\\tilde{Z}^{[l]}$ by:\n$$ \\begin{equation} \\tilde{Z}^{[l]} = \\gamma^{[l]} Z^{[l]}_{norm} + \\beta^{[l]} \\end{equation} $$\nThere should be nothing new until the last step; we just standardized $Z^{[l]}$. But what is this new $\\tilde{Z}^{[l]}$? It’s simply a rescaled and re-shifted version of $Z_{norm}^{[l]}$. Remember that standardizing samples of random variable will make them have approximately mean $0$ and variance $1$ if the random variable is normally distributed. Maybe this particular center and spread is not what’s best for our model. Remember what happens in regularization when $Z_i^{[l]} \\approx 0, \\forall i$ and $g^{[l]} = \\sigma(x)$; maybe centering our $Z^{[l]}$ around $0$ makes our neural network approximately linear.\nHaving $\\gamma^{[l]}, \\beta^{[l]}$ allows us to shift and scale $Z^{[l]}_{norm}$ in a way that improves the performance of our model. Yes, this means that $\\gamma^{[l]}, \\beta^{[l]}$ are new parameters that we can learn via our garden variety gradient descent.\nThink about what happens when $\\gamma^{[l]} = S^{[l]} + \\epsilon$ and $\\beta^{[l]} = \\bar{Z}^{[l]}$. In this case the last step undoes the normalization! This is not relevant to the actual implementation but simply to highlight that the learnable parameters are as powerful as standardizing.\nFitting Batch Norm into a Neural Network So now we have $\\gamma^{[l]}, \\beta^{[l]}$ for each of our hidden layers $l$. Remember that $\\beta^{[l]}$ is different from $b^{[l]}$ and also different from the $\\beta$ parameter used in gradient descent with momentum. We can learn these parameters in the same way we have been learning $W^{[l]}, b^{[l]}$; that is updating them on each step of gradient descent. We can even use EWMA methods such as momentum, RMSProp or Adam. One of the amazing things about these approaches is that they’re generalizable.\nWhen using mini-batch gradient descent, we use the mean and standard deviation of each mini-batch to standardize. However, the $\\gamma^{[l]}, \\beta^{[l]}$ is the same for all mini-batches within a layer $l$.\nA final detail mentioned in the course is that, when using batch normalization, the parameters $b^{[l]}$ are redundant because they are subtracted when standardizing. Therefore, you can drop $b^{[l]}, \\forall l$ from the set of learnable parameters and simply focus on $\\beta^{[l]}$.\nWhy does Batch Norm work? Earlier we discussed how normalization helps the optimization by undoing weird scales in our data. Batch norm works on the exact principle, but not just on the input layer; it applies this idea to all the layers. Remember that the hidden-layers are the “feature generation” layers, so this also means that batch norm undoes any weird scaling issues produced by layer $l-1$ which affect layer $l$, but also layer $l+1$ and so on.\nAnother way to think about this is to think about what happens in the following scenario. Imagine that we train a cat classifier using only cat pictures of cats with black fur. Assuming that our classifier performs well, it might not perform well on tasks where cats are allowed to have different fur colors than black. This is called covariate shift, and it simply means that our training data comes from a different distribution than the testing data. What batch norm does it to weaken the coupling between layer $l$ and layers $1, 2, \\dots, l$, which can be thought of as internal covariate shift.\nFinally, batch norm can act as a slight regularization. This occurs because when using batch norm with mini-batches, there is sampling error in the estimates of the mean and variance of each mini-batch. This noise ends up having a similar effect to dropout, which can result in some slight regularization. The noise added is inversely proportional to the mini-batch size by $O\\left(\\frac{1}{\\sqrt{n}}\\right)$, so that the larger the mini-batch size the less the regularization effect.\nBatch Norm at Test Time When we train our network with mini-batch batch norm, we calculate the mean and variance within each mini-batch. What happens when we want to make predictions or test our model? How can we calculate the mean and variance for a single test example?\nWe don’t. Instead, we keep a running average of the mean and variance for each layer using our favorite EWMA method during training. The last value is the one we use for testing, that is, the latest EWMA estimates for each layer’s mean and variance.\nAndrew also mentions that batch norm is pretty robust to the particular approach you use to estimate the mean and variance when testing time. If you estimate it using your entire training set or if you use an EWMA approach, the results should be very similar.\nMulti-class Classification Softmax Regression So far, all classification examples have been for binary classifiers, i.e. we only have two classes to predict: is this cat or not? What happens when we want to classify many classes, such as cats, dogs, baby chicks and anything else?\nWe denote the number of classes as $C$. In the case above $C = 4$. Ideally we want to go from an input image, to a vector $A^{[L]} = \\hat{y}$, where $\\hat{y}_1 = P(C_1 \\mid x), \\hat{y}_2 = P(C_2 | x), \\dots, \\hat{y}_k = P(C_k | x)$. That is, each entry in $\\hat{y}$ describes the probability that an input $x$ belongs to each class $k \\in C$. Since each class should be independent, it would be nice if:\n$$\\sum_{i=1}^k \\hat{y}_k = 1$$\nThat is, the probabilities for each class should add up to one, that being one of the requirements of a valid sample space. We solved this in the binary case using our trusty old friend the sigmoid $\\sigma(x)$, which maps $\\mathbb{R} \\rightarrow (0, 1)$. But the sigmoid $\\sigma(x)$ takes a single scalar, what can we do if $x$ is a vector? That is, what can we do when we have more than 2 classes? It turns out that there is a generalization of the sigmoid $\\sigma(x)$ called the softmax function.\nThe standard (unit) softmax function $\\sigma: \\mathbb{R}^K \\mapsto (0, 1)^K$. This means that the softmax function $\\sigma$ maps a $K$ dimensional real-valued vector, to a $K$ dimensional vector where each element is in the interval $(0 ,1)$, and it’s defined when $K \\geq 1$ by:\n$$ \\begin{equation} \\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} \\end{equation} $$\nNotice that in our case, $K = C$ where $C$ is the number of classes in our approach. Notice that because of the denominator, which is normalizing, all of our values will add up to $1$, which is exactly what we want.\nLet’s walk over one example. Assume that:\n$$ \\begin{equation} Z^{[l]} = \\begin{bmatrix} 5 \\\\ 2 \\\\ -1 \\\\ 3 \\end{bmatrix} \\end{equation} $$\nNow let’s calculate $A^{[l]} = \\sigma(Z^{[l]})$ using the softmax. We will do this in two steps, first calculate the numerator, $t$:\n$$ \\begin{equation} t^{[l]} = \\exp(Z^{[l]}) = \\begin{bmatrix} e^5 \\\\ e^2 \\\\ e^{-1} \\\\ e^3 \\end{bmatrix} = \\begin{bmatrix} 148.4 \\\\ 7.4 \\\\ 0.4 \\\\ 20.1 \\end{bmatrix} \\end{equation} $$\nNow we normalize $t^{[l]}$ by the sum $\\sum_{j=1}^4 e^{t_j} = 176.3$ to get $A^{[l]}$\n$$ \\begin{equation} A^{[l]} = \\hat{y} = \\frac{t^{[l]}}{176.3} = \\begin{bmatrix} \\frac{148.4}{176.3} \\\\ \\frac{7.4}{176.3} \\\\ \\frac{0.4}{176.3} \\\\ \\frac{20.1}{176.3} \\end{bmatrix} = \\begin{bmatrix} 0.842 \\\\ 0.042 \\\\ 0.002 \\\\ 0.114 \\end{bmatrix} \\end{equation} $$\nNow we can interpret these as probabilities! For example $P(x | C_1) = 0.842, P(x | C_2) = 0.042$ and so on. Also notice that the sum of these is $1$ because of the normalization in the denominator. Of course, we can think of the softmax as just another activation function, albeit a multi-dimensional one.\nThe name softmax comes from the comparison against the “hard max” function, which used in our case would return the vector $[1, 0, 0, 0]$, that is a boolean mask of the maximum value in the vector. The softmax is a “continuous” or “soft” version of that so that the vector we get as output is $[0.842, 0.042, 0.002, 0.114]$.\nYou might be thinking, what if $C=2$? In this case we are back to binary classification. If we compare softmax regression to logistic regression, then when $C=2$ softmax reduces to logistic regression. However, softmax can generalize logistic regression to $C=K$ dimensions or classes.\nTraining a Softmax Classifier It turns out that using negative log loss as your loss function generalizes nicely to many classes, i.e. $C \u003e 2$.\nAnother thing to keep in mind if we are using the vectorized implementation, where:\n$$ Y = [y^{(1)}, y^{(2)}, \\dots, y^{(m)}] $$\nThen both $Y$ and $\\hat{Y}$’s dimensions will be $(C, m)$ now instead. That is each sample $m$ will have it’s own softmax vector describing the probability that it belongs to each class.\nProgramming Frameworks Okay, never program your deep learning framework from scratch unless you are an expert trying to squeeze the latest bits of performance in some kind of specialized hardware. Just go with your favorite deep learning framework package. Hopefully go with one that has a vibrant, active community where you can get support and learn how to use the framework.\nMost of the frameworks differ in their approaches and they ultimately end up being equivalent. Like many other packages, measuring the “best” is hard because the “best” package is not just the fastest, but the easiest to use, the one with the best community and support, etc.\nThe key thing that all frameworks share is that they solve the problem of automatic differentiation. That is you can define some data, some kind of optimizer, some kind of cost function, and then automatically differentiate the cost function with respect to some parameters. For the purposes of the course, all frameworks should be thought of as equivalent. The course goes over a very simple TensorFlow example, but I decided to ignore it since TensorFlow’s documentation has better introductory examples in my opinion.\nNext week’s post is here.\n","wordCount":"3017","inLanguage":"en","datePublished":"2023-06-21T00:00:00Z","dateModified":"2023-06-21T00:00:00Z","author":{"@type":"Person","name":"Manuel Martinez"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/"},"publisher":{"@type":"Organization","name":"Manuel Martinez","logo":{"@type":"ImageObject","url":"http://localhost:1313/images/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Manuel Martinez (Alt + H)">Manuel Martinez</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=About><span>About</span></a></li><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/categories/ title=Categories><span>Categories</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/cv/ title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Improving Deep Learning Networks: Week 3 | Hyperparameter Tuning, Batch Optimization, Programming Frameworks</h1><div class=post-meta><span title='2023-06-21 00:00:00 +0000 UTC'>June 21, 2023</span>&nbsp;·&nbsp;<span>15 min</span>&nbsp;·&nbsp;<span>Manuel Martinez</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#hyperparameter-tuning>Hyperparameter Tuning</a><ul><li><a href=#tuning-process>Tuning Process</a></li><li><a href=#using-an-appropriate-scale-when-searching>Using an Appropriate Scale when Searching</a></li><li><a href=#hyperparameter-tuning-in-practice-pandas-vs-caviar>Hyperparameter Tuning in Practice: Pandas vs. Caviar</a></li></ul></li><li><a href=#batch-normalization>Batch Normalization</a><ul><li><a href=#normalizing-activations-in-a-network>Normalizing Activations in a Network</a></li><li><a href=#fitting-batch-norm-into-a-neural-network>Fitting Batch Norm into a Neural Network</a></li><li><a href=#why-does-batch-norm-work>Why does Batch Norm work?</a></li><li><a href=#batch-norm-at-test-time>Batch Norm at Test Time</a></li></ul></li><li><a href=#multi-class-classification>Multi-class Classification</a><ul><li><a href=#softmax-regression>Softmax Regression</a></li><li><a href=#training-a-softmax-classifier>Training a Softmax Classifier</a></li></ul></li><li><a href=#programming-frameworks>Programming Frameworks</a></li></ul></nav></div></details></div><div class=post-content><p>This is the third and final week of the <a href=https://www.coursera.org/learn/deep-neural-network>second course</a> of DeepLearning.AI&rsquo;s <a href=https://www.coursera.org/specializations/deep-learning>Deep Learning Specialization</a> offered on Coursera.</p><p>This week&rsquo;s topics are:</p><ul><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#hyperparameter-tuning>Hyperparameter Tuning</a><ul><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#tuning-process>Tuning Process</a><ul><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#random-search>Random Search</a></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#coarse-to-fine-grained-search>Coarse-to-fine Grained Search</a></li></ul></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#using-an-appropriate-scale-when-searching>Using an Appropriate Scale when Searching</a><ul><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#python-implementation>Python Implementation</a></li></ul></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#hyperparameter-tuning-in-practice-pandas-vs-caviar>Hyperparameter Tuning in Practice: Pandas vs. Caviar</a></li></ul></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#batch-normalization>Batch Normalization</a><ul><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#normalizing-activations-in-a-network>Normalizing Activations in a Network</a></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#fitting-batch-norm-into-a-neural-network>Fitting Batch Norm into a Neural Network</a></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#why-does-batch-norm-work>Why does Batch Norm work?</a></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#batch-norm-at-test-time>Batch Norm at Test Time</a></li></ul></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#multi-class-classification>Multi-class Classification</a><ul><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#softmax-regression>Softmax Regression</a></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#training-a-softmax-classifier>Training a Softmax Classifier</a></li></ul></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#programming-frameworks>Programming Frameworks</a></li></ul><hr><h2 id=hyperparameter-tuning>Hyperparameter Tuning<a hidden class=anchor aria-hidden=true href=#hyperparameter-tuning>#</a></h2><p>We have seen by now that neural networks have a lot of hyperparameters. Remember that hyperparameters remain fixed during training. This means that the process of finding reasonable hyperparameters, called hyperparameter tuning, is a process that is separate from training your model.</p><h3 id=tuning-process>Tuning Process<a hidden class=anchor aria-hidden=true href=#tuning-process>#</a></h3><p>As mentioned above, neural networks can have a lot of parameters:</p><ul><li>$\alpha$: the learning rate.</li><li>$\beta$: the EWMA term in momentum.</li><li>$\beta_1, \beta_2, \epsilon$: the EWMA parameters for Adam.</li><li>The number of layers $L$.</li><li>The number of hidden units $n^{[l]}$.</li><li>Learning rate decay rate if using learning rate decay.</li><li>The size of mini batches.</li></ul><p>If it seems daunting, it&rsquo;s because it is. Hyperparameter tuning is necessary most of the time because good hyperparameters from one problem not always translate to other problems. However, there are some hyperparameters that are usually more important than others. This can help you guide your tuning to focus on the most important ones first.</p><p>In the course, Andrew defines three tiers of importance for hyperparameter tuning, the first ones being more important than the latter ones:</p><ol><li>$\alpha$</li><li>$\beta$ if using momentum. Using $\beta = 0.9$ is a good default. The number of hidden layers and the mini-batch size.</li><li>Learning rate decay, and $L$.</li></ol><blockquote><p>If using Adam the defaults of $\beta_1 = 0.9, \beta_2 = 0.999, \epsilon=10^{-8}$ usually work fine.</p></blockquote><p>Now we know where to focus first. But how do we actually search and evaluate values?</p><p>In the earlier generation of machine learning algorithms, researchers would usually use <a href=https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search>grid search</a>. Grid search is an exhaustive search over a manually specified subset of the hyperparameter space, usually using cross validation. It&rsquo;s called a grid because if you imagine having two hyperparameters $h_1, h_2$, and some range of values <em>and</em> a step-size for each of the ranges $r_1, r_2$, then you can imagine a grid or matrix where each cell is a combination of $h_1, h_2$ over each of their ranges. For example, you might say, I want to search a value for $\alpha$ in the interval $[0.001, 0.1]$, and I will draw some <em>equidistant</em> samples in that range and evaluate all the values. Notice that the &ldquo;grid&rdquo; becomes a matrix, or $n$-dimensional tensor as you add more hyperparameters; this is terrible because the volume of the search space increases exponentially with each additional dimension or hyperparameter.</p><h4 id=random-search>Random Search<a hidden class=anchor aria-hidden=true href=#random-search>#</a></h4><p>Staying within the simple example of using two hyperparameters $h_1, h_2$, the problem with grid search might be apparent: you evaluate the <em>same</em> value for each hyperparameter multiple times. If set a grid of $9$ cells, then each value of both $h_1, h_2$ will be tested three times while holding the other constant. This is why the recommendation is to use <strong>random search</strong>: where each combination of $h_1, h_2$ is unique (the probability of repetition being low if you sample appropriately at random). The following image illustrates this point:</p><figure><img loading=lazy src=/images/grid-rand-search.png alt="Bergstra and Bengio | Hyperparameter Search" width=75%><figcaption><p><a href=https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf>Bergstra and Bengio | Hyperparameter Search</a></p></figcaption></figure><h4 id=coarse-to-fine-grained-search>Coarse-to-fine Grained Search<a hidden class=anchor aria-hidden=true href=#coarse-to-fine-grained-search>#</a></h4><p>Another, optional, recommendation is to first find a promising range for search and then zoom in within the range. This is called coarse-to-fine grained search.</p><h3 id=using-an-appropriate-scale-when-searching>Using an Appropriate Scale when Searching<a hidden class=anchor aria-hidden=true href=#using-an-appropriate-scale-when-searching>#</a></h3><p>So we know that we should sample the hyperparameter space at random. However, this doesn&rsquo;t necessarily mean to do it uniformly at random. The reason for this is that the <em>search scale</em> for each hyperparameter matters a lot. Being able to communicate the desired scale to the random sampling process is crucial for efficient search.</p><p>For some hyperparameters, such as the size of a hidden-layer $l$, $n^{[l]}$, it might be reasonable to sample uniformly at random. Say that you want to search for $n^{[l]}$ over the range $[50, 100]$. Then values like $89, 57, 62, 89, 74$ might be reasonable (actually sampled at random).</p><blockquote><p>Remember that a continuous uniform distribution has a PDF of $\frac{1}{b-a}$ where $a,b$ are the minimum and maximum values. Therefore, every value in the range $[a, b]$ has the same probability of being realized.</p></blockquote><p>This will not work for a hyperparameter like $\alpha$, however. Say that we suspect that a good $\alpha$ value is in the range $[0.0001, 1]$, and we sample uniformly at random. Since every value has the same likelihood of being drawn, this means that values between $[0.1, 1]$ will be sampled with $90\%$ probability. What about the interval $[0.0001, 0.1)$? These values have only a $10\%$ probability of being drawn. This is terrible because we are <em>equally</em> interested in values in both ranges.</p><p>This is why it makes more sense to sample for $\alpha$ values on the <strong>log scale</strong> as opposed to the <strong>linear scale</strong>. When we search over the linear scale and sample uniformly at random, we will spend $10\%$ of our effort searching over $[0.0001, 0.1)$ and $90\%$ of our effort searching over $[0.1, 1]$. On the other hand, when using a log-scale and sampling uniformly at random, we will spend an <em>equal</em> amount of effort searching over $[0.0001, 0.001), [0.001, 0.01), [0.01, 0.1), [0.1, 1]$.</p><h4 id=python-implementation>Python Implementation<a hidden class=anchor aria-hidden=true href=#python-implementation>#</a></h4><p>The way to implement the example above is as follows on Python:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>1337</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>r</span> <span class=o>=</span> <span class=o>-</span><span class=mi>4</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=mi>10</span> <span class=o>**</span> <span class=n>r</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=mf>0.0895161303335359</span>
</span></span></code></pre></div><p>Let&rsquo;s break this down. Remember that <code>np.random.rand()</code> generates random samples from a uniform distribution over the interval $[0, 1]$. Now, when we want to sample uniformly at random on a log scale, we want to sample uniformly <em>in the exponents</em>. What exponents? In our case, we want to sample uniformly in the range of $[0.0001, 1]$ which is the same as $[10^{-4}, 10^{0}]$. So if we can draw random samples $r$ from a uniform random distribution in the range $[-4, 0]$, then we can plug those into $10^{-r}$ and thus generate the samples in the log scale. How do we get $-4$? It&rsquo;s as simple as $-4 = \log_{10}0.0001$.</p><p>Let&rsquo;s generate $10$ such values for $\alpha$ in a vectorized way:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>1337</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>rs</span> <span class=o>=</span> <span class=o>-</span><span class=mi>4</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>  <span class=c1># we draw 10 samples</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=mi>10</span> <span class=o>**</span> <span class=n>rs</span><span class=p>)</span>  <span class=c1># this is vectorized over `rs`</span>
</span></span><span class=line><span class=cl><span class=n>array</span><span class=p>([</span><span class=mf>2.31880438e-01</span><span class=p>,</span> <span class=mf>7.71780714e-02</span><span class=p>,</span> <span class=mf>1.45456272e-02</span><span class=p>,</span> <span class=mf>5.19993408e-02</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>8.44167674e-03</span><span class=p>,</span> <span class=mf>8.95835560e-02</span><span class=p>,</span> <span class=mf>1.24640408e-04</span><span class=p>,</span> <span class=mf>1.17149864e-03</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>3.45862195e-01</span><span class=p>,</span> <span class=mf>2.85036007e-02</span><span class=p>])</span>
</span></span></code></pre></div><h3 id=hyperparameter-tuning-in-practice-pandas-vs-caviar>Hyperparameter Tuning in Practice: Pandas vs. Caviar<a hidden class=anchor aria-hidden=true href=#hyperparameter-tuning-in-practice-pandas-vs-caviar>#</a></h3><p>Andrew mentions that there are two ways that hyperparameter tuning happens in practice: pandas vs. caviar. No, it&rsquo;s not related to the pandas Python package. It&rsquo;s related to how the animal, the giant panda, has offspring. Pandas usually have very few offspring and therefore put a lot of effort into the upbringing of each (don&rsquo;t think too heavily on the veracity of this statement). This is contrasted to other species that lay thousands of eggs, where each egg is almost left to chance (again no offense to the regal Giant Pacific Octopus mothers whom sometimes give their lives for their precious 120,000 to 400,000 eggs).</p><p>The idea is that sometimes your model is too big, and you cannot afford to train multiple instances of your model, so must babysit it like a little panda. That is, adjusting the hyperparameters over longer periods of time. On the other hand, the caviar approach is where you can afford to train multiple models in parallel with different hyperparameters and then pick the winner.</p><h2 id=batch-normalization>Batch Normalization<a hidden class=anchor aria-hidden=true href=#batch-normalization>#</a></h2><p>You might remember how we mentioned that <a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#normalizing-inputs>normalizing your inputs</a> could help our optimization run faster. The issue with this is that even though our input layer is getting normalized values, the outputs of each layer $l$, $A^{[l]}$ are no longer normalized. Batch normalization is applying the same reasoning but on each <em>layer</em>.</p><blockquote><p>Batch normalization doesn&rsquo;t have anything to do with batch or mini-batch gradient descent and can be implemented under both approaches.</p></blockquote><p>A key thing is that we won&rsquo;t normalize $A^{[l]}$ but $Z^{[l]}$ instead. Let&rsquo;s go over the steps.</p><h3 id=normalizing-activations-in-a-network>Normalizing Activations in a Network<a hidden class=anchor aria-hidden=true href=#normalizing-activations-in-a-network>#</a></h3><p>For a given layer $l$ in our network, under the <strong>batch</strong> gradient descent approach, we compute:</p><p>$$
\begin{equation}
Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}
\end{equation}
$$</p><p>Now, we will normalize $Z^{[l]}$ via standardization:</p><p>$$
\begin{equation}
Z^{[l]}_{norm} = \frac{Z^{[l]} - \bar{Z}^{[l]}}{S^{[l]} + \epsilon}
\end{equation}
$$</p><p>Where $\bar{Z}^{[l]}$ is the sample mean of $Z^{[l]}$ and $S^{[l]}$ is the sample standard deviation of $Z^{[l]}$.</p><p>Finally, we compute $\tilde{Z}^{[l]}$ by:</p><p>$$
\begin{equation}
\tilde{Z}^{[l]} = \gamma^{[l]} Z^{[l]}_{norm} + \beta^{[l]}
\end{equation}
$$</p><p>There should be nothing new until the last step; we just standardized $Z^{[l]}$. But what is this new $\tilde{Z}^{[l]}$? It&rsquo;s simply a rescaled and re-shifted version of $Z_{norm}^{[l]}$. Remember that standardizing samples of random variable will make them have approximately mean $0$ and variance $1$ if the random variable is normally distributed. Maybe this particular center and spread is not what&rsquo;s best for our model. Remember what happens in <a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#why-does-regularization-reduce-overfitting>regularization</a> when $Z_i^{[l]} \approx 0, \forall i$ and $g^{[l]} = \sigma(x)$; maybe centering our $Z^{[l]}$ around $0$ makes our neural network approximately linear.</p><p>Having $\gamma^{[l]}, \beta^{[l]}$ allows us to shift and scale $Z^{[l]}_{norm}$ in a way that improves the performance of our model. Yes, this means that $\gamma^{[l]}, \beta^{[l]}$ are new parameters that we can learn via our garden variety gradient descent.</p><p>Think about what happens when $\gamma^{[l]} = S^{[l]} + \epsilon$ and $\beta^{[l]} = \bar{Z}^{[l]}$. In this case the last step <em>undoes</em> the normalization! This is not relevant to the actual implementation but simply to highlight that the learnable parameters are as powerful as standardizing.</p><h3 id=fitting-batch-norm-into-a-neural-network>Fitting Batch Norm into a Neural Network<a hidden class=anchor aria-hidden=true href=#fitting-batch-norm-into-a-neural-network>#</a></h3><p>So now we have $\gamma^{[l]}, \beta^{[l]}$ for each of our hidden layers $l$. Remember that $\beta^{[l]}$ is different from $b^{[l]}$ and also different from the $\beta$ parameter used in gradient descent with momentum. We can learn these parameters in the same way we have been learning $W^{[l]}, b^{[l]}$; that is updating them on each step of gradient descent. We can <em>even</em> use EWMA methods such as momentum, RMSProp or Adam. One of the amazing things about these approaches is that they&rsquo;re generalizable.</p><blockquote><p>When using <em>mini-batch</em> gradient descent, we use the mean and standard deviation of <em>each mini-batch</em> to standardize. However, the $\gamma^{[l]}, \beta^{[l]}$ is the same for all mini-batches within a layer $l$.</p></blockquote><p>A final detail mentioned in the course is that, when using batch normalization, the parameters $b^{[l]}$ are redundant because they are subtracted when standardizing. Therefore, you can drop $b^{[l]}, \forall l$ from the set of learnable parameters and simply focus on $\beta^{[l]}$.</p><h3 id=why-does-batch-norm-work>Why does Batch Norm work?<a hidden class=anchor aria-hidden=true href=#why-does-batch-norm-work>#</a></h3><p>Earlier we discussed how normalization helps the optimization by undoing weird scales in our data. Batch norm works on the exact principle, but not just on the input layer; it applies this idea to all the layers. Remember that the hidden-layers are the &ldquo;feature generation&rdquo; layers, so this also means that batch norm undoes any weird scaling issues produced by layer $l-1$ which affect layer $l$, but also layer $l+1$ and so on.</p><p>Another way to think about this is to think about what happens in the following scenario. Imagine that we train a cat classifier using <em>only</em> cat pictures of cats with black fur. Assuming that our classifier performs well, it might not perform well on tasks where cats are allowed to have different fur colors than black. This is called <em>covariate shift</em>, and it simply means that our training data comes from a different distribution than the testing data. What batch norm does it to weaken the coupling between layer $l$ and layers $1, 2, \dots, l$, which can be thought of as <em>internal</em> covariate shift.</p><p>Finally, batch norm can act as a slight regularization. This occurs because when using batch norm with mini-batches, there is sampling error in the estimates of the mean and variance of each mini-batch. This noise ends up having a similar effect to dropout, which can result in some slight regularization. The noise added is inversely proportional to the mini-batch size by $O\left(\frac{1}{\sqrt{n}}\right)$, so that the larger the mini-batch size the less the regularization effect.</p><h3 id=batch-norm-at-test-time>Batch Norm at Test Time<a hidden class=anchor aria-hidden=true href=#batch-norm-at-test-time>#</a></h3><p>When we train our network with mini-batch batch norm, we calculate the mean and variance within each mini-batch. What happens when we want to make predictions or test our model? How can we calculate the mean and variance for a single test example?</p><p>We don&rsquo;t. Instead, we keep a running average of the mean and variance for each layer using our favorite EWMA method during training. The last value is the one we use for testing, that is, the latest EWMA estimates for each layer&rsquo;s mean and variance.</p><p>Andrew also mentions that batch norm is pretty robust to the particular approach you use to estimate the mean and variance when testing time. If you estimate it using your entire training set or if you use an EWMA approach, the results should be very similar.</p><h2 id=multi-class-classification>Multi-class Classification<a hidden class=anchor aria-hidden=true href=#multi-class-classification>#</a></h2><h3 id=softmax-regression>Softmax Regression<a hidden class=anchor aria-hidden=true href=#softmax-regression>#</a></h3><p>So far, all classification examples have been for binary classifiers, i.e. we only have two classes to predict: is this cat or not? What happens when we want to classify many classes, such as cats, dogs, baby chicks and anything else?</p><p>We denote the number of classes as $C$. In the case above $C = 4$. Ideally we want to go from an input image, to a vector $A^{[L]} = \hat{y}$, where $\hat{y}_1 = P(C_1 \mid x), \hat{y}_2 = P(C_2 | x), \dots, \hat{y}_k = P(C_k | x)$. That is, each entry in $\hat{y}$ describes the probability that an input $x$ belongs to <em>each</em> class $k \in C$. Since each class should be independent, it would be nice if:</p><p>$$\sum_{i=1}^k \hat{y}_k = 1$$</p><p>That is, the probabilities for each class should add up to one, that being one of the requirements of a valid sample space. We solved this in the binary case using our trusty old friend the sigmoid $\sigma(x)$, which maps $\mathbb{R} \rightarrow (0, 1)$. But the sigmoid $\sigma(x)$ takes a single scalar, what can we do if $x$ is a vector? That is, what can we do when we have more than 2 classes? It turns out that there is a generalization of the sigmoid $\sigma(x)$ called the <a href=https://en.wikipedia.org/wiki/Softmax_function>softmax function</a>.</p><p>The standard (unit) softmax function $\sigma: \mathbb{R}^K \mapsto (0, 1)^K$. This means that the softmax function $\sigma$ maps a $K$ dimensional real-valued vector, to a $K$ dimensional vector where each element is in the interval $(0 ,1)$, and it&rsquo;s defined when $K \geq 1$ by:</p><p>$$
\begin{equation}
\sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
\end{equation}
$$</p><p>Notice that in our case, $K = C$ where $C$ is the number of classes in our approach. Notice that because of the denominator, which is normalizing, all of our values will add up to $1$, which is exactly what we want.</p><p>Let&rsquo;s walk over one example. Assume that:</p><p>$$
\begin{equation}
Z^{[l]} = \begin{bmatrix}
5 \\
2 \\
-1 \\
3
\end{bmatrix}
\end{equation}
$$</p><p>Now let&rsquo;s calculate $A^{[l]} = \sigma(Z^{[l]})$ using the softmax. We will do this in two steps, first calculate the numerator, $t$:</p><p>$$
\begin{equation}
t^{[l]} = \exp(Z^{[l]}) = \begin{bmatrix}
e^5 \\
e^2 \\
e^{-1} \\
e^3
\end{bmatrix} = \begin{bmatrix}
148.4 \\
7.4 \\
0.4 \\
20.1
\end{bmatrix}
\end{equation}
$$</p><p>Now we normalize $t^{[l]}$ by the sum $\sum_{j=1}^4 e^{t_j} = 176.3$ to get $A^{[l]}$</p><p>$$
\begin{equation}
A^{[l]} = \hat{y} = \frac{t^{[l]}}{176.3} = \begin{bmatrix}
\frac{148.4}{176.3} \\
\frac{7.4}{176.3} \\
\frac{0.4}{176.3} \\
\frac{20.1}{176.3}
\end{bmatrix} = \begin{bmatrix}
0.842 \\
0.042 \\
0.002 \\
0.114
\end{bmatrix}
\end{equation}
$$</p><p>Now we can interpret these as probabilities! For example $P(x | C_1) = 0.842, P(x | C_2) = 0.042$ and so on. Also notice that the sum of these is $1$ because of the normalization in the denominator. Of course, we can think of the softmax as just another activation function, albeit a multi-dimensional one.</p><blockquote><p>The name softmax comes from the comparison against the &ldquo;hard max&rdquo; function, which used in our case would return the vector $[1, 0, 0, 0]$, that is a boolean mask of the maximum value in the vector. The softmax is a &ldquo;continuous&rdquo; or &ldquo;soft&rdquo; version of that so that the vector we get as output is $[0.842, 0.042, 0.002, 0.114]$.</p></blockquote><p>You might be thinking, what if $C=2$? In this case we are back to binary classification. If we compare softmax regression to logistic regression, then when $C=2$ softmax reduces to logistic regression. However, softmax can generalize logistic regression to $C=K$ dimensions or classes.</p><h3 id=training-a-softmax-classifier>Training a Softmax Classifier<a hidden class=anchor aria-hidden=true href=#training-a-softmax-classifier>#</a></h3><p>It turns out that using negative log loss as your loss function generalizes nicely to many classes, i.e. $C > 2$.</p><p>Another thing to keep in mind if we are using the vectorized implementation, where:</p><p>$$
Y = [y^{(1)}, y^{(2)}, \dots, y^{(m)}]
$$</p><p>Then both $Y$ and $\hat{Y}$&rsquo;s dimensions will be $(C, m)$ now instead. That is each sample $m$ will have it&rsquo;s own softmax vector describing the probability that it belongs to each class.</p><h2 id=programming-frameworks>Programming Frameworks<a hidden class=anchor aria-hidden=true href=#programming-frameworks>#</a></h2><p>Okay, never program your deep learning framework from scratch unless you are an expert trying to squeeze the latest bits of performance in some kind of specialized hardware. Just go with your favorite deep learning framework package. Hopefully go with one that has a vibrant, active community where you can get support and learn how to use the framework.</p><p>Most of the frameworks differ in their approaches and they ultimately end up being equivalent. Like many other packages, measuring the &ldquo;best&rdquo; is hard because the &ldquo;best&rdquo; package is not just the fastest, but the easiest to use, the one with the best community and support, etc.</p><p>The key thing that all frameworks share is that they solve the problem of automatic differentiation. That is you can define some data, some kind of optimizer, some kind of cost function, and then automatically differentiate the cost function with respect to some parameters. For the purposes of the course, all frameworks should be thought of as equivalent. The course goes over a <em>very</em> simple TensorFlow example, but I decided to ignore it since TensorFlow&rsquo;s documentation has better introductory examples in my opinion.</p><p>Next week&rsquo;s post is <a href=http://localhost:1313/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/>here</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/machine-learning/>Machine Learning</a></li><li><a href=http://localhost:1313/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/><span class=title>« Prev</span><br><span>Improving Deep Learning Networks: Week 2 | Optimization Algorithms</span>
</a><a class=next href=http://localhost:1313/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/><span class=title>Next »</span><br><span>Structuring ML Projects: Week 1 | ML Strategy</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=http://localhost:1313/>Manuel Martinez</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>