<!doctype html><html lang=en dir=auto data-theme=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Improving Deep Learning Networks: Week 1 | Practical Aspects of Deep Learning | Manuel Martinez</title><meta name=keywords content="machine learning,deep learning"><meta name=description content="This is the first week in the second course of DeepLearning.AI&rsquo;s Deep Learning Specialization offered on Coursera. The course deals with hyperparameter tuning, regularization and the optimization of the training process. The optimization ranges from computational complexity to performance. This is again a pretty technical week, where you will benefit a lot from doing the programming exercises.
This week&rsquo;s topics are:

Setting up our Machine Learning problem

Train / Dev / Test sets
Bias-Variance Tradeoff
Basic Recipe for Machine Learning


Regularizing our Neural Network

Regularization
Why Does Regularization Reduce Overfitting?
Dropout Regularization
Understanding Dropout
Other Regularization Methods


Setting up our Optimization Problem
Normalizing Inputs
Vanishing and Exploding Gradients
Weight Initialization


Setting up our Machine Learning problem
Train / Dev / Test sets
Machine learning projects are highly iterative. That is, you try something new, see how it does and then adjust; very much like gradient descent. Therefore, you want the iteration time to be quick so that you can try as many things as quickly as possible, without affecting the final performance of the model. Part of this is setting up your datasets correctly so that you can efficiently iterate over different approaches."><meta name=author content="Manuel Martinez"><link rel=canonical href=http://localhost:1313/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/images/favicon.ico><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/images/favicon.ico><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="http://localhost:1313/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/"><meta property="og:site_name" content="Manuel Martinez"><meta property="og:title" content="Improving Deep Learning Networks: Week 1 | Practical Aspects of Deep Learning"><meta property="og:description" content="This is the first week in the second course of DeepLearning.AI’s Deep Learning Specialization offered on Coursera. The course deals with hyperparameter tuning, regularization and the optimization of the training process. The optimization ranges from computational complexity to performance. This is again a pretty technical week, where you will benefit a lot from doing the programming exercises.
This week’s topics are:
Setting up our Machine Learning problem Train / Dev / Test sets Bias-Variance Tradeoff Basic Recipe for Machine Learning Regularizing our Neural Network Regularization Why Does Regularization Reduce Overfitting? Dropout Regularization Understanding Dropout Other Regularization Methods Setting up our Optimization Problem Normalizing Inputs Vanishing and Exploding Gradients Weight Initialization Setting up our Machine Learning problem Train / Dev / Test sets Machine learning projects are highly iterative. That is, you try something new, see how it does and then adjust; very much like gradient descent. Therefore, you want the iteration time to be quick so that you can try as many things as quickly as possible, without affecting the final performance of the model. Part of this is setting up your datasets correctly so that you can efficiently iterate over different approaches."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-18T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-18T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Improving Deep Learning Networks: Week 1 | Practical Aspects of Deep Learning"><meta name=twitter:description content="This is the first week in the second course of DeepLearning.AI&rsquo;s Deep Learning Specialization offered on Coursera. The course deals with hyperparameter tuning, regularization and the optimization of the training process. The optimization ranges from computational complexity to performance. This is again a pretty technical week, where you will benefit a lot from doing the programming exercises.
This week&rsquo;s topics are:

Setting up our Machine Learning problem

Train / Dev / Test sets
Bias-Variance Tradeoff
Basic Recipe for Machine Learning


Regularizing our Neural Network

Regularization
Why Does Regularization Reduce Overfitting?
Dropout Regularization
Understanding Dropout
Other Regularization Methods


Setting up our Optimization Problem
Normalizing Inputs
Vanishing and Exploding Gradients
Weight Initialization


Setting up our Machine Learning problem
Train / Dev / Test sets
Machine learning projects are highly iterative. That is, you try something new, see how it does and then adjust; very much like gradient descent. Therefore, you want the iteration time to be quick so that you can try as many things as quickly as possible, without affecting the final performance of the model. Part of this is setting up your datasets correctly so that you can efficiently iterate over different approaches."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Improving Deep Learning Networks: Week 1 | Practical Aspects of Deep Learning","item":"http://localhost:1313/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Improving Deep Learning Networks: Week 1 | Practical Aspects of Deep Learning","name":"Improving Deep Learning Networks: Week 1 | Practical Aspects of Deep Learning","description":"This is the first week in the second course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. The course deals with hyperparameter tuning, regularization and the optimization of the training process. The optimization ranges from computational complexity to performance. This is again a pretty technical week, where you will benefit a lot from doing the programming exercises.\nThis week\u0026rsquo;s topics are:\nSetting up our Machine Learning problem Train / Dev / Test sets Bias-Variance Tradeoff Basic Recipe for Machine Learning Regularizing our Neural Network Regularization Why Does Regularization Reduce Overfitting? Dropout Regularization Understanding Dropout Other Regularization Methods Setting up our Optimization Problem Normalizing Inputs Vanishing and Exploding Gradients Weight Initialization Setting up our Machine Learning problem Train / Dev / Test sets Machine learning projects are highly iterative. That is, you try something new, see how it does and then adjust; very much like gradient descent. Therefore, you want the iteration time to be quick so that you can try as many things as quickly as possible, without affecting the final performance of the model. Part of this is setting up your datasets correctly so that you can efficiently iterate over different approaches.\n","keywords":["machine learning","deep learning"],"articleBody":"This is the first week in the second course of DeepLearning.AI’s Deep Learning Specialization offered on Coursera. The course deals with hyperparameter tuning, regularization and the optimization of the training process. The optimization ranges from computational complexity to performance. This is again a pretty technical week, where you will benefit a lot from doing the programming exercises.\nThis week’s topics are:\nSetting up our Machine Learning problem Train / Dev / Test sets Bias-Variance Tradeoff Basic Recipe for Machine Learning Regularizing our Neural Network Regularization Why Does Regularization Reduce Overfitting? Dropout Regularization Understanding Dropout Other Regularization Methods Setting up our Optimization Problem Normalizing Inputs Vanishing and Exploding Gradients Weight Initialization Setting up our Machine Learning problem Train / Dev / Test sets Machine learning projects are highly iterative. That is, you try something new, see how it does and then adjust; very much like gradient descent. Therefore, you want the iteration time to be quick so that you can try as many things as quickly as possible, without affecting the final performance of the model. Part of this is setting up your datasets correctly so that you can efficiently iterate over different approaches.\nIf you have a data set $M$ of $|m|$ samples then you usually want to split it into three parts.\nTraining set: $M_{train} \\subseteq M$. The set of samples that will be used to learn the parameters of our chosen hypothesis $h(X, \\theta)$, i.e. your model parameters via SGD. Holdout/Cross-validation/Development set: $M_{dev} \\subseteq M$. The set of samples that will be used to evaluate a hypothesis class $H$, i.e. your model hyperparameters via hyperparameter tuning. Test set: $M_{test} \\subseteq M$. The set of samples that will be used the best hypothesis you’ve found so far on completely unseen data that belongs to the same distribution as the dev set. Note that these three subsets are disjoint, i.e. $M_{train} \\cap M_{test} \\cap M_{dev} = \\emptyset$\nHistorically most researchers would split the set $M$ using a 70%/30% training and testing respectively. If using a dev set, a 60%/20%/20% split for training, dev and test. This was reasonable for datasets that have sizes in the order of 10,000. If you have a dataset $M$ with 1,000,000 training samples, then it can be reasonable to use a 98%/1%/1% training, dev and test split. You can read more about how big a sample has to be for certain statistical properties to kick in, but a number around 10,000 is considered relatively safe.\nOne assumption is that all the training samples $m \\in M$ come from the same distribution, which implies that each of the splits also belong to the same distribution. This is critical because you don’t want to develop a model with pictures of cats and then test it on picture of dogs. A key thing is to make sure that your dev and test sets come from the same distribution. Having the training set come from a different distribution is more lax, as long as the training distribution is a superset of the dev and test sets. In some cases it might be okay to not have a test set, but measuring variance will be harder.\nBias-Variance Tradeoff The bias/variance tradeoff is a topic in machine learning and statistics. The whole idea of supervised learning is that, if you do it correctly, your model can generalize beyond the training set, i.e. samples it has not seen during training and still do a good job. We think that there are some parameters $\\theta$ that minimize our cost function. We don’t know what that is, and the only way to know that is to have infinite training samples. But we need to make due with our estimate $\\hat{\\theta}$, which implies that $\\theta \\neq \\hat{\\theta}$. When we talk about the bias-variance tradeoff we are describing the errors, i.e. $\\epsilon = \\theta - \\hat{\\theta}$ that are produced by our model that uses our estimates of the parameters. In particular, we are describing the mean and variance of the error distribution. The bias is a part of how far $\\hat{\\theta}$ is from $\\theta$ on average, while the variance measures the spread of the errors.\nIt turns out that we can characterize our expected test errors even further by decomposing them into three components (skipping some math):\nVariance: $E_{x, D}\\left[\\left(h_D(x) - \\bar{h}(x)\\right)^2\\right]$ $\\text{Bias}^2$: $E_x\\left[\\left(\\bar{h}(x) - \\bar{y}(x)\\right)^2\\right]$ Noise: $E_{x,y}\\left[\\left(\\bar{y}(x) - y\\right)^2\\right]$ Keep in mind:\nVariance measures how much our classifier $h_D$ changes when we train it on a different training set $D$. How “over-specialized” is our classifier to a particular training set $D$? 1 Notice that $h$ and $h_D$ are random variables, and so this measures the spread of a classifier trained on each possible sample $D$ drawn from $P^n$ versus the average classifier $\\bar{h}(x)$. Bias measures the inherent error from our classifier if we had infinite training data. 1 How far is our estimator’s expected value $\\bar{y}(x)$ from the expected classifier? Noise measures some irreducible error, which is an intrinsic property of the data. This related to Bayes error rate, which is the lowest possible error rate for any classifier of random outcomes. Graphical Illustration of bias and variance\nWe know that bias and variance are the components of our model’s expected test error. But why is it a tradeoff? It turns out that model complexity is related to both bias and variance. A model with low complexity (always guess the same) will have high error, and the majority of the error will come from the bias term. This means that as we decrease the complexity of a model, our error will increase, but most of the increase will come from the bias term. Conversely, a model with high complexity (guessing randomly) will have high error, and the majority of the error will come from the variance term. This means that as we increase model complexity, our error will also increase, but most the increase will come from the variance term. Somewhere in the middle, there is a sweet spot, a combination of bias and variance that has the minimum total error.\nWhen we say that a model has high bias, we say that it’s underfitting. It’s too basic and not being complex enough to learn the training data, underfitting it. On the other hand, when we say that a model has high variance we say that a model is overfitting the training data, not being able to generalize beyond it, overfitting it. This is where we come back to our dataset splits.\nBefore moving on, we have to re-introduce the irreducible error term from the previous section. For every classification task, there is some amount of error that cannot be done away with. This is also called Bayes error rate, and it’s a theoretical limit that no classifier can surpass. Similarly, there is a human-level error achieved by the best a human can do, whether a group or individual, and this is called the human-level error. The best classifier can be better or worse than human-level error but never better than Bayes error rate. For the following section we assume that the human-level error is approximately $0$ but more than the Bayes error rate.\nEvaluating the performance of your model on the training and dev set can help you diagnose whether your model has high variance or high bias or both very quickly. Let $\\epsilon_{train}, \\epsilon_{dev}$ the error of your model on the training and dev sets respectively.\n$e_{train} = 1\\%, e_{dev} = 11\\%$: Our model is doing really well on the training set but much poorer on the development set. It smells of high variance since the model is not able to generalize from the training set to the dev set. It’s therefore overfitting the training set. $e_{train} = 15\\%, e_{dev} = 16\\%$: Assuming that human-level error is approximately $0$, then this looks like high bias. The model is very far from human-level error on the training set, therefore it’s not fitting our training data enough. It’s therefore underfitting the training set. However, it’s generalizing quite well to the dev set, therefore it might have low variance. $e_{train} = 15\\%, e_{dev} = 30\\%$: Assuming that human-level error is approximately $0$, then this looks like high bias again. But because it’s not generalizing well, it also smells of high variance because of the discrepancy in performance between the test and dev sets. $e_{train} = 0.5\\%, e_{dev} = 1\\%$: Assuming that human-level error is approximately $0$, then this looks like both low bias and low variance. It’s very close to the Bayes’ error rate, and also generalizes well to the dev set. The key assumption is that human-level error is approximately $0$, which implies that Bayes error has to be between $0$ and the human-level error. If for example the Bayes error rate is $15\\%$ then the second case would not be high variance, because we are actually close to the best possible classifier.\nIf you’re wondering about how you can simultaneously underfit and overfit your training data, think about this:\nHigh Bias and High Variance\nBasic Recipe for Machine Learning Ideally we want a low-bias and low-variance model. But how do we get there?\nDo we have high bias? How is our performance on the training set relative to Bayes error rate? If high bias: Increase the size/depth of your network, adding model complexity, increasing variance but lowering bias. Train it longer, better estimates of the parameters, lower bias and variance. Do we have high variance? How is our performance on the dev set relative to the training set and Bayes error rate? Get more data, better estimates of the parameters, lower bias and variance. Regularization, decreasing model complexity, increasing bias but lowering variance. Following these steps, i.e. fixing high bias before high variance is the basic recipe for machine learning.\nIt’s mentioned in the course that in the deep learning era, there is less of a tradeoff. The ability to have bigger models and get more data are both tools that don’t expose the bias-variance tradeoff, so usually these are the first things to check when encountering high bias or high variance. In general training a bigger network almost never hurts, as long as you add proper regularization, keeping the model complexity from exploding.\nRegularizing our Neural Network Regularization Regularization is a tool you employ when you suspect your model is suffering from high variance or overfitting. Regularization will reduce your model complexity by penalizing model complexity when evaluating its performance and therefore generating a “simpler” model. It turns out that you can add a term to your cost function, which is a measure of model complexity, so that more complex models result in a higher cost, therefore driving down complexity. But how do we measure model complexity? Let’s answer this using the logistic regression case:\nLet’s recall our cost function:\n$$ \\begin{equation} \\mathcal{J}(w, b) = \\frac{1}{m} \\sum_{i = 1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) \\end{equation} $$\nWe would like to add something to the expression above that measures model complexity. One way of measuring model complexity is literally to look at the magnitude of the parameters, literally how big they are. Remember that in logistic regression we have one theta $\\theta_i$ for each feature in our data. This implies that if coefficients blow up in magnitude, that is, get very big or very small, they are having a lot of effect on the output. This is simply because the relationship between parameters and features is multiplicative and additive, i.e. it’s linear.\nWe can check how big the parameters are by summing over each of them, let’s call that $|w|$:\n$$ \\begin{equation} |w| = \\sum_{j=1}^{N_x} w_j \\end{equation} $$\nThe issue with the above is that some $w_j$ might be positive and others might be negative, undoing each of their effects in the summation. The solution to this problem is to use a norm. A norm is a function that maps the real or complex numbers into the non-negative real numbers. An important geometrical idea related to linear algebra is that the norm is a generalization of distance from the origin: it’s proportional to scaling, it doesn’t violate a form of the triangle inequality, and is zero only at the origin. 2 We would also like to generalize this to more dimensions, which is handy when our parameters is not just a vector but matrices that describe some vector space that has some “size”. When talking about the norm of a matrix, the term Matrix norm is used, and the euclidean norm equivalent is called the Frobenius norm. It has a different name because you have to do some work before generalizing from one to many dimensions and some brilliant mathematicians figured it out.\nOne of the most common norms is to take the squares to only get positive numbers. This is called the L2 Norm, or Euclidean norm for being related the definition of distance in Euclidean geometry, remember the guy Pythagoras?. It’s denoted $||w||_2$. When working with the square of the L2 norm, its denoted $||w||_2^2$. It’s defined by:\n$$ ||w||_2 = \\sqrt{\\sum_{j=1}^{N_x} w_j^2} = \\sqrt{w^Tw} $$\nAlso:\n$$ ||w||^2_2 = w^Tw $$\nAdding this to our cost function makes it looks like this now:\n$$ \\begin{equation} \\mathcal{J}(w, b) = \\frac{1}{m} \\sum_{i = 1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + ||w||_2^2 \\end{equation} $$\nWhy are we ignoring the other parameter $b$? Because it’s a scalar and not a vector. $w \\in \\mathbb{R}^{N_x}$, which means $w$ has $N_x$ elements. While $b \\in \\mathbb{R}$, which means that $b$ has only one element. This means that if we have $N_x$ features, then $b$ has to be $N_x$ times bigger than the $w$ norm to influence the same amount. You can also think about how the space or volume of a $N_x$ dimensional hypercube grows as opposed to a $1$ dimensional line. So we usually ignore $b$.\nWe now have a way of penalizing model complexity by assigning a higher cost to a more complex model. What would also be great is to have a way of scaling the effect of the regularization. For example, we don’t want a model with a very small $w$ because the smallest is where $w_i = 0$, we want to retain some complexity but not too much.\nWe do this by introducing a hyperparameter $\\lambda$ called the regularization parameter, which is a scalar that scales the regularization effect:\n$$ \\begin{equation} \\mathcal{J}(w, b) = \\frac{1}{m} \\sum_{i = 1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\lambda||w||_2^2 \\end{equation} $$\nWe can find a good $\\lambda$ by hyperparameter tuning using the dev set. By comparing the loss between the training set and the dev set, we can find a good regularization value that balances simplicity and complexity, or bias and variance.\nWe also want to scale the effect of the regularization by the size of our training data $m$. Remember, we have a high variance problem we are trying to solve by penalizing a complex model. However, our model will be more complex as we add more training data. So we want our regularization to be stronger when our training data is small, but decrease as our training data is larger. We can just divide $\\lambda$ by $m$ to have a linearly decreasing effect:\n$$ \\begin{equation} \\mathcal{J}(w, b) = \\frac{1}{m} \\sum_{i = 1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{m}||w||_2^2 \\end{equation} $$\nFinally, for differentiation reasons, we add an easy way to cancel some terms:\n$$ \\begin{equation} \\mathcal{J}(w, b) = \\frac{1}{m} \\sum_{i = 1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}||w||_2^2 \\end{equation} $$\nThink about what happens to $w$ after we introduce this term to our cost function. It means it will prefer smaller parameters than bigger ones, and if it has to chose between two big ones, it will choose the one that is the least costly. In other words the model will let some parameters be large as long as they earn their keep (in terms of model performance) and decrease the features that are not that relevant. In this sense, regularization can be thought of as a type of feature selection. With the L2 norm the parameters that don’t earn their keep approach zero, but never quite get to be zero.\nBut the L2 norm is not the only norm. What if you use some other norm? Another popular norm is the L1-Norm:\n$$ ||x||_1 = \\sum_{j=1}^{N_x} |x_i| $$\nJust like the L2-norm, it maps the real or complex numbers into the non-negative real numbers. We can also use this in our cost function:\n$$ \\begin{equation} \\mathcal{J}(w, b) = \\frac{1}{m} \\sum_{i = 1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}||w||_1 \\end{equation} $$\nUnlike the L2-norm, the L1-norm does actually send the parameters that don’t earn their keep to zero. This means that $w$ will end up being sparser. Which implies that the L1-norm works as a sort of model compression. In practice however, the L2 norm is used more commonly so that should be your first go to.\nWe mentioned the Frobenius norm as the generalization of the L2-norm to higher dimensions. The L2-norm works in logistic regression precisely because $w$ is a vector. When we have matrices of parameters, $W^{[l]}$ as is the case for a neural network, we need to use the grown-up’s norm. The Frobenius norm is a reasonable extension, instead of summing over a vector’s single dimension, you sum over both dimensions in a matrix:\n$$ ||W^{[l]}||_F = \\sqrt{\\sum_{i=1}^{n^{[l - 1]}}\\sum_{j=1}^{n^{[l]}}w_{ij}^{[l]}} $$\nAlso:\n$$ ||W^{[l]}||^2_F = \\sum_{i=1}^{n^{[l - 1]}}\\sum_{j=1}^{n^{[l]}}w_{ij}^{[l]} $$\nSo for a $L$-layered neural network our cost function will look like:\n$$ \\begin{equation} \\mathcal{J}(W^{[1]}, b^{[l]}, \\dots, W^{[l]}, b{[l]}) = \\frac{1}{m} \\sum_{i = 1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\sum_{l=1}^L ||W^{[l]}||^2_F \\end{equation} $$\nA couple of things to mention:\nNotice that now the cost function is a function of all layers of our network, $W^{[l]}, b^{[l]}$. This is in contrast to logistic regression where we only have one pair of $W, b$. Also notice that in the regularization term we need to sum over all layers of our network. Again we are ignoring the $b^{[l]}$ and only focusing on the $W^{[l]}$ parameters. The L2-norm is also usually called weight decay. This is because when you differentiate the loss function, which now includes the regularization term, with respect to the parameters, it will scale down $W^{[l]}$ by $1 - \\frac{\\alpha\\lambda}{m}$ on every step. Making those weights smaller, or decaying the weights if you please.\nIf you’re coming from econ-land, you might have heard of Lasso and Ridge regressions. Lasso is a linear regression that uses the L1-norm, while Ridge is a linear regression that uses the L2-norm.\nWhy Does Regularization Reduce Overfitting? We went over how regularization is equivalent to penalizing model complexity, and how model complexity can be measured by the magnitude of the parameters of your model, which is the mechanism through which we implement regularization. The intuition here is that parameters with high values assign high sensitivity to the features being multiplied by those parameters. The high sensitivity is what results in the high variance, and by penalizing the magnitude of the parameters is how we reduce overfitting.\nAnother great way of thinking about it is to think about what happens in the activation functions when the values are big or small.\nRemember that $Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$. This, we pass through our activation function $g^{[l]}$ so that we get $A^{[l]} = g^{[l]}(Z^{[l]})$. Taking as an example the sigmoid activation function $\\sigma(x)$, think about what happens when the inputs $Z^{[l]}$ are small. How can $Z^{[l]}$ be small? If we set $\\lambda$ to be huge, then most of our $W^{[l]}$ will be small, therefore making $Z^{[l]}$ small as well. When we pass small values, that is values close to $0$ to the sigmoid function, the outputs are approximately linear. It is only when $|x| \\geq 2$ that $\\sigma(x)$ (approximately) becomes non-linear. So in a way, setting $W{[l]}$ to be small undoes all the non-linear magic that you got from adding more layers to your network.\nDropout Regularization It turns out that measuring the “size” of your parameters is not the only way to measure model complexity. Conversely, minimizing the size of your parameters is also not the only way to punish model complexity. Dropout achieves a similar result by doing something more intuitive to some.\nIn a nutshell, dropout randomly deletes some fraction of nodes in each hidden layer on each forward-backward pass. Since on each forward-backwards pass you are “turning off” some hidden units, the model trained on each iteration is literally smaller. The random component of the dropout guarantees that all parts of the model are affected equally on expectation. So what do we mean exactly by “deleting” nodes? We mean setting them to 0. It really is that simple.\nMore formally, there is some probability $p$ that represents probability for a single hidden unit to be kept untouched. $p$ is referred to as the keep probability. Conversely, $1 - p$ is the “dropout” probability. Notice that the events are independent of each other. That is each hidden unit, i.e. each node is dropped independently relative to all the other units in that layer, and also from other layers.\nSay that you have a layer with $50$ hidden units, and that $p = 0.8$, so that you will be dropping out $1 - 0.8 = 0.2$ of the units in a hidden layer. So on expectation you will be dropping $50 * 0.2 = 10$ nodes. Let’s also think that this layer is $l = 3$. Let’s think about how this will affect the next layer.\nThe next layer’s computation will be:\n$$ \\begin{equation} Z^{[4]} = W^{[4]}A^{[3]} + b^{[4]} \\end{equation} $$\nWhere $A^{[3]}$ is the output of the layer with $50$ hidden units, and we shut down $20\\%$ of them. This means that $A^{[3]}$ is reduced by $20\\%$, which also implies that $Z^{[4]}$ is reduced by $20\\%$ because $Z^{[4]}$ is just a linear combination of $A^{[3]}$. In order to avoid this problem propagating throughout our network making everything small, we need to roughly “refund” the lost value to $A^{[3]}$ before passing it to the next layer. We do this simply by:\n$$ \\begin{equation} A^{[3*]} = \\frac{A^{[3]}}{p} \\end{equation} $$\nRemember that dividing anything by a number less than $1$ will make it bigger. Therefore:\n$$ \\begin{equation} Z^{[4]} = W^{[4]}A^{[3*]} + b^{[4]} \\end{equation} $$\nWill make $Z^{[4]}$ be roughly the same as it was originally. This scaling technique is called inverted dropout.\nThe intuition behind this rescaling is that when you set some hidden-units to zero, you are shifting the expected value of $A^{[3]}$, which is an issue when you do testing. Because when you evaluate your network with the test set, you do not do any dropout, the difference in expected values becomes a problem. Therefore, the rescaling ameliorates the scaling issue between training and testing.\nFinally, you can apply a different value of $p$ to different layers, so that you have a vector $p$ where $p^{[l]}$ is the probability of keeping units in hidden layer $l$. This faces you with a tradeoff of having more hyperparameters, so it should be used wisely.\nUnderstanding Dropout So why does dropout behave like regularization? It seems crazy to “kill” some nodes at random, it’s the very definition of chaotic evil. It turns out that by doing this, the model will learn not to rely on any specific feature, because it might not be there the next time. The model will respond to dropout by spreading out the weights across the network, effectively reducing their magnitude. This is why dropout can be thought of as regularization.\nAn interesting approach to building resilience in engineering systems is that of Chaos Engineering. It’s similar in spirit to dropout, whereby randomly disabling some parts of a system, and forcing designers to deal with random failures will produce a more robust system.\nOther Regularization Methods Another regularization method is that of data augmentation. For example if you have images as your training data, you might think about applying some geometric distortions (flipping, scaling, etc.). This works as regularization by injecting more variance into your training set, making your model smarter by making it harder for it to put all its eggs in one basket (focusing too much on some parameters).\nAnother method is early stopping. This amounts to regularization because early stopping might amount to stopping training before $W{[l]}$ gets “too big”. What is too big? You won’t know. The problem with early stopping is that it conflates two processes that should be orthogonal (independent): doing well on your training samples and not overfitting. There’s more information about orthogonalizing your goals in the next course.\nSetting up our Optimization Problem One thing is to make sure that your model has the right combination of bias and variance, which amounts to learning the problem at hand well enough and also being able to generalize to unseen samples. Now, we turn our attention to making the optimization process (learning the problem) easier and more efficient.\nNormalizing Inputs Normalization is one of those topics that means different things to different people. 3 Let’s disambiguate the term and also talk about why normalizing our inputs might be helpful for our optimization process.\nAs the name implies, normalization is the process of making something normal. Of course, there are many ways of making something “normal”, and we haven’t even defined what “normal” is. Let’s focus on one of the ways to normalize things.\nIn the context of statistics, normalization usually refers to standardizing, which is where we calculate a standard score for a random variable $X$. It’s called the standard score because its units are standard deviations.\nStandardizing a random variable $X$ is pretty easy and intuitive:\n$$ \\begin{equation} z = \\frac{x - \\mu}{\\sigma} \\end{equation} $$\nWhere $\\mu$ is the mean of the population and $\\sigma$ is the standard deviation of the population. Because we don’t know those values, and we usually estimate them, we usually work with z-scores, which are the sample analogues:\n$$ \\begin{equation} z = \\frac{x - \\bar{x}}{S} \\end{equation} $$\nWhere $\\bar{x}$ is the sample mean and $S$ is the sample standard deviation. If your features are normally distributed, then the transformed variables will approximately mean zero and standard deviation of one.\nThe process is intuitive, for each observation we first ask: how far is this observation from the sample mean. Whatever that distance is, positive or negative, we scale it by the standard deviation. So that the result is: an observation is $z$ standard deviations from the mean.\nBut why do this? The purpose of this process is to convert different random variables that have different scales and shifts, into the same units. The units will be standard deviations for that particular random variable. So that $z_i$ will tell you how many standard deviations observation $i$ is from the mean. Positive values will be above the mean and negative values will be below the mean. Using this approach we can negate the effects of having different features with wildly different unit scales and shapes.\nWhen you normalize your training set, you want to use standardization on each of your features separately. However, when you are testing, you want to use the same $\\bar{x}$ and $S$ that you calculated during training. You don’t want to scale and shift your training data separately from your testing data. You want the opposite, you want your training and testing data to go through the same transformations.\nHow is this related to optimization? How does normalizing scales and shifts in our features affect our optimization? It all has to do with gradient descent and how the updates are made.\nGradient Descent with and without features scaling\nThe contour plot above shows an imaginary two-dimensional cost function $J(F_1, F_2)$. Notice that if we have a pair of unscaled features and one of the features’ scale is larger than the other, then the cost function will look squished. Gradient descent will take many tiny steps because it’s considering both features at the same time. Since one feature is big and the other one small, the same step size is big in one but small on the other. This means that the larger feature will dominate the update. However, if you scale your features with standardization for example, then the cost function might look like the one on the right. Since all features are in a standard scale, no feature will dominate the update, allowing for less oscillation, and therefore faster convergence.\nVanishing and Exploding Gradients Vanishing and exploding gradients sound magical, if not outright dangerous. What this means is that when we train very deep networks, and we take the derivatives for gradient descent, the magnitude of these derivatives either “vanish” to $0$ or “explode” towards $\\infty$. How does this disaster come about?\nThink about a very deep $L$-layered neural network. Imagine also that all $W^{[l]}$ are the same:\n$$ \\begin{equation} W^{[l]} = \\begin{bmatrix} 1.5 \u0026 0 \\\\ 0 \u0026 1.5 \\end{bmatrix} \\end{equation} $$\nAlso imagine that all $b^{[l]} = 0$.\nSince all $W^{[l]}$ are the same, then whatever our features $x$ is, we get that:\n$$ \\hat{y} = W^{[l]L}x $$\nThat is the $W^{[l]}$ is multiplied by itself $L$ times. In our case this amounts to $1.5^L$, which literally grows exponentially with the number of layers. Imagine now that instead of $1.5$ on the diagonal of $W^{[l]}$ we have a number less than 0, i.e. $0.5$. With the same problem setup, now the values will be $0.5^L$ which is an expression that shrinks exponentially in the number of layers.\nIn either case, this is a huge problem. With exploding gradients, the gradient descent steps are gigantic, and with vanishing gradients the gradient descent steps are tiny. Either way, converging the minimum of our cost function will take a long, long time. We tackle this issue by being more careful in the way that we initialize our parameters.\nWeight Initialization In order to avoid exploding or vanishing gradients, we need to be more careful as to how we initialize our parameters. The main idea here is that the more hidden units in a hidden-layer, the smaller each $w^{[l]}_j$ you want. This is to keep large hidden layers and small hidden layers in around the same output scale.\nWe will still initialize our parameters to random values, but we will scale them by a term, $s$, that is proportional to the size of the inputs coming into a layer $l$. The whole point of this is to adjust the variance of $W^{[l]}$ so that it’s proportional to inputs, $n^{[l-1]}$:\n$$ \\begin{equation} W^{[l]} = \\texttt{np.random.randn(shape)} \\times s \\end{equation} $$\nThe choice is $s$ depends on $g^{[l]}(x)$, the activation function for layer $l$:\nIn the case that $g^{[l]}(x) = \\text{ReLU}(x)$, then $s = \\sqrt{\\frac{2}{n^{[l-1]}}}$. In the case that $g^{[l]}(x) = \\text{tanh}(x)$, then $s = \\sqrt{\\frac{1}{n^{[l-1]}}}$, which is also called Xavier initialization. Another variant for $\\text{tanh}(x)$ is $s = \\sqrt{\\frac{2}{n^{[l-1]} + n^{[l]}}}$ Next week’s post is here.\nBias-Variance Tradeoff | Cornell University ↩︎ ↩︎\nNorm | Wikipedia ↩︎\nNormalization | Wikipedia ↩︎\n","wordCount":"5135","inLanguage":"en","datePublished":"2023-06-18T00:00:00Z","dateModified":"2023-06-18T00:00:00Z","author":{"@type":"Person","name":"Manuel Martinez"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/"},"publisher":{"@type":"Organization","name":"Manuel Martinez","logo":{"@type":"ImageObject","url":"http://localhost:1313/images/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Manuel Martinez (Alt + H)">Manuel Martinez</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=About><span>About</span></a></li><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/categories/ title=Categories><span>Categories</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/cv/ title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Improving Deep Learning Networks: Week 1 | Practical Aspects of Deep Learning</h1><div class=post-meta><span title='2023-06-18 00:00:00 +0000 UTC'>June 18, 2023</span>&nbsp;·&nbsp;<span>25 min</span>&nbsp;·&nbsp;<span>Manuel Martinez</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#setting-up-our-machine-learning-problem>Setting up our Machine Learning problem</a><ul><li><a href=#train--dev--test-sets>Train / Dev / Test sets</a></li><li><a href=#bias-variance-tradeoff>Bias-Variance Tradeoff</a></li><li><a href=#basic-recipe-for-machine-learning>Basic Recipe for Machine Learning</a></li></ul></li><li><a href=#regularizing-our-neural-network>Regularizing our Neural Network</a><ul><li><a href=#regularization>Regularization</a></li><li><a href=#why-does-regularization-reduce-overfitting>Why Does Regularization Reduce Overfitting?</a></li><li><a href=#dropout-regularization>Dropout Regularization</a></li><li><a href=#understanding-dropout>Understanding Dropout</a></li><li><a href=#other-regularization-methods>Other Regularization Methods</a></li></ul></li><li><a href=#setting-up-our-optimization-problem>Setting up our Optimization Problem</a></li><li><a href=#normalizing-inputs>Normalizing Inputs</a></li><li><a href=#vanishing-and-exploding-gradients>Vanishing and Exploding Gradients</a></li><li><a href=#weight-initialization>Weight Initialization</a></li></ul></nav></div></details></div><div class=post-content><p>This is the first week in the <a href=https://www.coursera.org/learn/deep-neural-network>second course</a> of DeepLearning.AI&rsquo;s <a href=https://www.coursera.org/specializations/deep-learning>Deep Learning Specialization</a> offered on Coursera. The course deals with hyperparameter tuning, regularization and the optimization of the training process. The optimization ranges from computational complexity to performance. This is again a pretty technical week, where you will benefit a lot from doing the programming exercises.</p><p>This week&rsquo;s topics are:</p><ul><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#setting-up-our-machine-learning-problem>Setting up our Machine Learning problem</a><ul><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#train--dev--test-sets>Train / Dev / Test sets</a></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#bias-variance-tradeoff>Bias-Variance Tradeoff</a></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#basic-recipe-for-machine-learning>Basic Recipe for Machine Learning</a></li></ul></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#regularizing-our-neural-network>Regularizing our Neural Network</a><ul><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#regularization>Regularization</a></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#why-does-regularization-reduce-overfitting>Why Does Regularization Reduce Overfitting?</a></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#dropout-regularization>Dropout Regularization</a></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#understanding-dropout>Understanding Dropout</a></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#other-regularization-methods>Other Regularization Methods</a></li></ul></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#setting-up-our-optimization-problem>Setting up our Optimization Problem</a></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#normalizing-inputs>Normalizing Inputs</a></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#vanishing-and-exploding-gradients>Vanishing and Exploding Gradients</a></li><li><a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#weight-initialization>Weight Initialization</a></li></ul><hr><h2 id=setting-up-our-machine-learning-problem>Setting up our Machine Learning problem<a hidden class=anchor aria-hidden=true href=#setting-up-our-machine-learning-problem>#</a></h2><h3 id=train--dev--test-sets>Train / Dev / Test sets<a hidden class=anchor aria-hidden=true href=#train--dev--test-sets>#</a></h3><p>Machine learning projects are highly iterative. That is, you try something new, see how it does and then adjust; very much like gradient descent. Therefore, you want the iteration time to be quick so that you can try as many things as quickly as possible, without affecting the final performance of the model. Part of this is setting up your datasets correctly so that you can efficiently iterate over different approaches.</p><p>If you have a data set $M$ of $|m|$ samples then you usually want to split it into three parts.</p><ol><li>Training set: $M_{train} \subseteq M$. The set of samples that will be used to learn the parameters of our chosen hypothesis $h(X, \theta)$, i.e. your model parameters via SGD.</li><li>Holdout/Cross-validation/Development set: $M_{dev} \subseteq M$. The set of samples that will be used to evaluate a hypothesis class $H$, i.e. your model hyperparameters via hyperparameter tuning.</li><li>Test set: $M_{test} \subseteq M$. The set of samples that will be used the best hypothesis you&rsquo;ve found so far on completely unseen data that belongs to the same distribution as the dev set.</li></ol><p>Note that these three subsets are disjoint, i.e. $M_{train} \cap M_{test} \cap M_{dev} = \emptyset$</p><p>Historically most researchers would split the set $M$ using a 70%/30% training and testing respectively. If using a dev set, a 60%/20%/20% split for training, dev and test. This was reasonable for datasets that have sizes in the order of 10,000. If you have a dataset $M$ with 1,000,000 training samples, then it can be reasonable to use a 98%/1%/1% training, dev and test split. You can read more about how big a sample has to be for certain statistical properties to kick in, but a number around 10,000 is considered relatively safe.</p><p>One assumption is that all the training samples $m \in M$ come from the same distribution, which implies that each of the splits also belong to the same distribution. This is critical because you don&rsquo;t want to develop a model with pictures of cats and then test it on picture of dogs. A key thing is to make sure that your dev and test sets come from the same distribution. Having the training set come from a different distribution is more lax, as long as the training distribution is a superset of the dev and test sets. In some cases it might be okay to not have a test set, but measuring variance will be harder.</p><h3 id=bias-variance-tradeoff>Bias-Variance Tradeoff<a hidden class=anchor aria-hidden=true href=#bias-variance-tradeoff>#</a></h3><p>The bias/variance tradeoff is a topic in machine learning and statistics. The whole idea of supervised learning is that, if you do it correctly, your model can generalize <em>beyond</em> the training set, i.e. samples it has not seen during training and still do a good job. We think that there are some parameters $\theta$ that minimize our cost function. We don&rsquo;t know what that is, and the only way to know that is to have <em>infinite</em> training samples. But we need to make due with our estimate $\hat{\theta}$, which implies that $\theta \neq \hat{\theta}$. When we talk about the bias-variance tradeoff we are describing the errors, i.e. $\epsilon = \theta - \hat{\theta}$ that are produced by our model that uses our estimates of the parameters. In particular, we are describing the mean and variance of the error distribution. The bias is a part of how far $\hat{\theta}$ is from $\theta$ on average, while the variance measures the spread of the errors.</p><p>It turns out that we can characterize our expected test errors even further by decomposing them into three components (skipping some math):</p><ol><li>Variance: $E_{x, D}\left[\left(h_D(x) - \bar{h}(x)\right)^2\right]$</li><li>$\text{Bias}^2$: $E_x\left[\left(\bar{h}(x) - \bar{y}(x)\right)^2\right]$</li><li>Noise: $E_{x,y}\left[\left(\bar{y}(x) - y\right)^2\right]$</li></ol><p>Keep in mind:</p><ol><li>Variance measures how much our classifier $h_D$ changes when we train it on a different training set $D$. How &ldquo;over-specialized&rdquo; is our classifier to a particular training set $D$? <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> Notice that $h$ and $h_D$ are random variables, and so this measures the spread of a classifier trained on each possible sample $D$ drawn from $P^n$ versus the average classifier $\bar{h}(x)$.</li><li>Bias measures the inherent error from our classifier <em>if</em> we had infinite training data. <sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> How far is our estimator&rsquo;s expected value $\bar{y}(x)$ from the expected classifier?</li><li>Noise measures some irreducible error, which is an intrinsic property of the data. This related to Bayes error rate, which is the <em>lowest</em> possible error rate for any classifier of <em>random</em> outcomes.</li></ol><figure><img loading=lazy src=/images/bullseye.png alt="Graphical Illustration of bias and variance" width=50%><figcaption><p><a href=http://scott.fortmann-roe.com/docs/BiasVariance.html>Graphical Illustration of bias and variance</a></p></figcaption></figure><p>We know that bias and variance are the components of our model&rsquo;s expected test error. But why is it a tradeoff? It turns out that <em>model complexity</em> is related to both bias and variance. A model with low complexity (always guess the same) will have high error, and the majority of the error will come from the bias term. This means that as we decrease the complexity of a model, our error will increase, but most of the increase will come from the bias term. Conversely, a model with high complexity (guessing randomly) will have high error, and the majority of the error will come from the variance term. This means that as we increase model complexity, our error will <em>also</em> increase, but most the increase will come from the variance term. Somewhere in the middle, there is a sweet spot, a combination of bias and variance that has the minimum total error.</p><p>When we say that a model has high bias, we say that it&rsquo;s underfitting. It&rsquo;s too basic and not being complex enough to learn the training data, underfitting it. On the other hand, when we say that a model has high variance we say that a model is overfitting the training data, not being able to generalize beyond it, overfitting it. This is where we come back to our dataset splits.</p><p>Before moving on, we have to re-introduce the irreducible error term from the previous section. For every classification task, there is some amount of error that cannot be done away with. This is also called Bayes error rate, and it&rsquo;s a theoretical limit that no classifier can surpass. Similarly, there is a human-level error achieved by the best a human can do, whether a group or individual, and this is called the human-level error. The best classifier can be better or worse than human-level error but never better than Bayes error rate. For the following section we assume that the human-level error is approximately $0$ but more than the Bayes error rate.</p><p>Evaluating the performance of your model on the training and dev set can help you diagnose whether your model has high variance or high bias or both very quickly. Let $\epsilon_{train}, \epsilon_{dev}$ the error of your model on the training and dev sets respectively.</p><ul><li>$e_{train} = 1\%, e_{dev} = 11\%$: Our model is doing really well on the training set but much poorer on the development set. It smells of high variance since the model is not able to generalize from the training set to the dev set. It&rsquo;s therefore overfitting the training set.</li><li>$e_{train} = 15\%, e_{dev} = 16\%$: Assuming that human-level error is approximately $0$, then this looks like high bias. The model is very far from human-level error on the training set, therefore it&rsquo;s not fitting our training data enough. It&rsquo;s therefore underfitting the training set. However, it&rsquo;s generalizing quite well to the dev set, therefore it might have low variance.</li><li>$e_{train} = 15\%, e_{dev} = 30\%$: Assuming that human-level error is approximately $0$, then this looks like high bias again. But because it&rsquo;s not generalizing well, it also smells of high variance because of the discrepancy in performance between the test and dev sets.</li><li>$e_{train} = 0.5\%, e_{dev} = 1\%$: Assuming that human-level error is approximately $0$, then this looks like both low bias and low variance. It&rsquo;s very close to the Bayes&rsquo; error rate, and also generalizes well to the dev set.</li></ul><p>The key assumption is that human-level error is approximately $0$, which implies that Bayes error has to be between $0$ and the human-level error. If for example the Bayes error rate is $15\%$ then the second case would not be high variance, because we are actually close to the best possible classifier.</p><p>If you&rsquo;re wondering about how you can simultaneously underfit and overfit your training data, think about this:</p><figure><img loading=lazy src=/images/highbv.png alt="High Bias and High Variance" width=50%><figcaption><p><a href=https://www.coursera.org/learn/deep-neural-network>High Bias and High Variance</a></p></figcaption></figure><h3 id=basic-recipe-for-machine-learning>Basic Recipe for Machine Learning<a hidden class=anchor aria-hidden=true href=#basic-recipe-for-machine-learning>#</a></h3><p>Ideally we want a low-bias and low-variance model. But how do we get there?</p><ol><li>Do we have high bias? How is our performance on the training set relative to Bayes error rate?<ul><li>If high bias:<ul><li>Increase the size/depth of your network, adding model complexity, increasing variance but lowering bias.</li><li>Train it longer, better estimates of the parameters, lower bias and variance.</li></ul></li></ul></li><li>Do we have high variance? How is our performance on the dev set relative to the training set and Bayes error rate?<ul><li>Get more data, better estimates of the parameters, lower bias and variance.</li><li>Regularization, decreasing model complexity, increasing bias but lowering variance.</li></ul></li></ol><p>Following these steps, i.e. fixing high bias before high variance is the basic recipe for machine learning.</p><p>It&rsquo;s mentioned in the course that in the deep learning era, there is less of a tradeoff. The ability to have bigger models and get more data are both tools that don&rsquo;t expose the bias-variance tradeoff, so usually these are the first things to check when encountering high bias or high variance. In general training a bigger network almost never hurts, <strong>as long</strong> as you add proper regularization, keeping the model complexity from exploding.</p><h2 id=regularizing-our-neural-network>Regularizing our Neural Network<a hidden class=anchor aria-hidden=true href=#regularizing-our-neural-network>#</a></h2><h3 id=regularization>Regularization<a hidden class=anchor aria-hidden=true href=#regularization>#</a></h3><p>Regularization is a tool you employ when you suspect your model is suffering from high variance or overfitting. Regularization will reduce your model complexity by penalizing model complexity when evaluating its performance and therefore generating a &ldquo;simpler&rdquo; model. It turns out that you can add a term to your cost function, which is a measure of model complexity, so that more complex models result in a higher cost, therefore driving down complexity. But how do we measure model complexity? Let&rsquo;s answer this using the logistic regression case:</p><p>Let&rsquo;s recall our cost function:</p><p>$$
\begin{equation}
\mathcal{J}(w, b) = \frac{1}{m} \sum_{i = 1}^m \mathcal{L}(\hat{y}^{(i)}, y^{(i)})
\end{equation}
$$</p><p>We would like to add something to the expression above that measures model complexity. One way of measuring model complexity is literally to look at the magnitude of the parameters, literally how big they are. Remember that in logistic regression we have one theta $\theta_i$ for each feature in our data. This implies that if coefficients blow up in magnitude, that is, get very big or very small, they are having a lot of effect on the output. This is simply because the relationship between parameters and features is multiplicative and additive, i.e. it&rsquo;s linear.</p><p>We can check how big the parameters are by summing over each of them, let&rsquo;s call that $|w|$:</p><p>$$
\begin{equation}
|w| = \sum_{j=1}^{N_x} w_j
\end{equation}
$$</p><p>The issue with the above is that some $w_j$ might be positive and others might be negative, undoing each of their effects in the summation. The solution to this problem is to use a <a href=https://en.wikipedia.org/wiki/Norm_%28mathematics%29>norm</a>. A norm is a function that maps the real or complex numbers into the non-negative real numbers. An important geometrical idea related to linear algebra is that the norm is a generalization of distance from the origin: it&rsquo;s proportional to scaling, it doesn&rsquo;t violate a form of the triangle inequality, and is zero only at the origin. <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> We would also like to generalize this to more dimensions, which is handy when our parameters is not just a vector but matrices that describe some vector space that has some &ldquo;size&rdquo;. When talking about the norm of a matrix, the term <a href=https://en.wikipedia.org/wiki/Matrix_norm>Matrix norm</a> is used, and the euclidean norm equivalent is called the <a href=https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm>Frobenius norm</a>. It has a different name because you have to do some work before generalizing from one to many dimensions and some brilliant mathematicians figured it out.</p><p>One of the most common norms is to take the squares to only get positive numbers. This is called the <a href=https://archive.lib.msu.edu/crcmath/math/math/l/l002.htm>L2 Norm</a>, or Euclidean norm for being related the definition of distance in Euclidean geometry, remember the guy Pythagoras?. It&rsquo;s denoted $||w||_2$. When working with the square of the L2 norm, its denoted $||w||_2^2$. It&rsquo;s defined by:</p><p>$$
||w||_2 = \sqrt{\sum_{j=1}^{N_x} w_j^2} = \sqrt{w^Tw}
$$</p><p>Also:</p><p>$$
||w||^2_2 = w^Tw
$$</p><p>Adding this to our cost function makes it looks like this now:</p><p>$$
\begin{equation}
\mathcal{J}(w, b) = \frac{1}{m} \sum_{i = 1}^m \mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + ||w||_2^2
\end{equation}
$$</p><p>Why are we ignoring the other parameter $b$? Because it&rsquo;s a scalar and not a vector. $w \in \mathbb{R}^{N_x}$, which means $w$ has $N_x$ elements. While $b \in \mathbb{R}$, which means that $b$ has only one element. This means that if we have $N_x$ features, then $b$ has to be $N_x$ times bigger than the $w$ norm to influence the same amount. You can also think about how the space or volume of a $N_x$ dimensional hypercube grows as opposed to a $1$ dimensional line. So we usually ignore $b$.</p><p>We now have a way of penalizing model complexity by assigning a higher cost to a more complex model. What would also be great is to have a way of scaling the effect of the regularization. For example, we don&rsquo;t want a model with a very small $w$ because the smallest is where $w_i = 0$, we want to retain some complexity but not too much.</p><p>We do this by introducing a hyperparameter $\lambda$ called the regularization parameter, which is a scalar that scales the regularization effect:</p><p>$$
\begin{equation}
\mathcal{J}(w, b) = \frac{1}{m} \sum_{i = 1}^m \mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \lambda||w||_2^2
\end{equation}
$$</p><p>We can find a good $\lambda$ by hyperparameter tuning using the dev set. By comparing the loss between the training set and the dev set, we can find a good regularization value that balances simplicity and complexity, or bias and variance.</p><p>We also want to scale the effect of the regularization by the size of our training data $m$. Remember, we have a high variance problem we are trying to solve by penalizing a complex model. However, our model will be more complex as we add more training data. So we want our regularization to be stronger when our training data is small, but decrease as our training data is larger. We can just divide $\lambda$ by $m$ to have a linearly decreasing effect:</p><p>$$
\begin{equation}
\mathcal{J}(w, b) = \frac{1}{m} \sum_{i = 1}^m \mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{m}||w||_2^2
\end{equation}
$$</p><p>Finally, for differentiation reasons, we add an easy way to cancel some terms:</p><p>$$
\begin{equation}
\mathcal{J}(w, b) = \frac{1}{m} \sum_{i = 1}^m \mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}||w||_2^2
\end{equation}
$$</p><p>Think about what happens to $w$ after we introduce this term to our cost function. It means it will prefer smaller parameters than bigger ones, and if it has to chose between two big ones, it will choose the one that is the least costly. In other words the model will let some parameters be large as long as they earn their keep (in terms of model performance) and decrease the features that are not that relevant. In this sense, regularization can be thought of as a type of feature selection. With the L2 norm the parameters that don&rsquo;t earn their keep approach zero, but never quite get to be zero.</p><p>But the L2 norm is not the only norm. What if you use some other norm? Another popular norm is the <a href=https://archive.lib.msu.edu/crcmath/math/math/l/l001.htm>L1-Norm</a>:</p><p>$$
||x||_1 = \sum_{j=1}^{N_x} |x_i|
$$</p><p>Just like the L2-norm, it maps the real or complex numbers into the non-negative real numbers. We can also use this in our cost function:</p><p>$$
\begin{equation}
\mathcal{J}(w, b) = \frac{1}{m} \sum_{i = 1}^m \mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}||w||_1
\end{equation}
$$</p><p>Unlike the L2-norm, the L1-norm does actually send the parameters that don&rsquo;t earn their keep to zero. This means that $w$ will end up being <em>sparser</em>. Which implies that the L1-norm works as a sort of model compression. In practice however, the L2 norm is used more commonly so that should be your first go to.</p><p>We mentioned the Frobenius norm as the generalization of the L2-norm to higher dimensions. The L2-norm works in logistic regression precisely because $w$ is a vector. When we have matrices of parameters, $W^{[l]}$ as is the case for a neural network, we need to use the grown-up&rsquo;s norm. The Frobenius norm is a reasonable extension, instead of summing over a vector&rsquo;s single dimension, you sum over both dimensions in a matrix:</p><p>$$
||W^{[l]}||_F = \sqrt{\sum_{i=1}^{n^{[l - 1]}}\sum_{j=1}^{n^{[l]}}w_{ij}^{[l]}}
$$</p><p>Also:</p><p>$$
||W^{[l]}||^2_F = \sum_{i=1}^{n^{[l - 1]}}\sum_{j=1}^{n^{[l]}}w_{ij}^{[l]}
$$</p><p>So for a $L$-layered neural network our cost function will look like:</p><p>$$
\begin{equation}
\mathcal{J}(W^{[1]}, b^{[l]}, \dots, W^{[l]}, b{[l]}) = \frac{1}{m} \sum_{i = 1}^m \mathcal{L}(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m} \sum_{l=1}^L ||W^{[l]}||^2_F
\end{equation}
$$</p><p>A couple of things to mention:</p><ul><li>Notice that now the cost function is a function of all layers of our network, $W^{[l]}, b^{[l]}$. This is in contrast to logistic regression where we only have one pair of $W, b$.</li><li>Also notice that in the regularization term we need to sum over all layers of our network. Again we are ignoring the $b^{[l]}$ and only focusing on the $W^{[l]}$ parameters.</li></ul><p>The L2-norm is also usually called <em>weight decay</em>. This is because when you differentiate the loss function, which now includes the regularization term, with respect to the parameters, it will scale down $W^{[l]}$ by $1 - \frac{\alpha\lambda}{m}$ on every step. Making those weights smaller, or decaying the weights if you please.</p><blockquote><p>If you&rsquo;re coming from econ-land, you might have heard of Lasso and Ridge regressions. Lasso is a linear regression that uses the L1-norm, while Ridge is a linear regression that uses the L2-norm.</p></blockquote><h3 id=why-does-regularization-reduce-overfitting>Why Does Regularization Reduce Overfitting?<a hidden class=anchor aria-hidden=true href=#why-does-regularization-reduce-overfitting>#</a></h3><p>We went over how regularization is equivalent to penalizing model complexity, and how model complexity can be measured by the magnitude of the parameters of your model, which is the mechanism through which we implement regularization. The intuition here is that parameters with high values assign <em>high sensitivity</em> to the features being multiplied by those parameters. The high sensitivity is what results in the high variance, and by penalizing the magnitude of the parameters is how we reduce overfitting.</p><p>Another great way of thinking about it is to think about what happens in the activation functions when the values are big or small.</p><p>Remember that $Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$. This, we pass through our activation function $g^{[l]}$ so that we get $A^{[l]} = g^{[l]}(Z^{[l]})$. Taking as an example the sigmoid activation function $\sigma(x)$, think about what happens when the inputs $Z^{[l]}$ are small. How can $Z^{[l]}$ be small? If we set $\lambda$ to be huge, then most of our $W^{[l]}$ will be small, therefore making $Z^{[l]}$ small as well. When we pass small values, that is values close to $0$ to the sigmoid function, the outputs are approximately <em>linear</em>. It is only when $|x| \geq 2$ that $\sigma(x)$ (approximately) becomes non-linear. So in a way, setting $W{[l]}$ to be small <em>undoes</em> all the non-linear magic that you got from adding more layers to your network.</p><h3 id=dropout-regularization>Dropout Regularization<a hidden class=anchor aria-hidden=true href=#dropout-regularization>#</a></h3><p>It turns out that measuring the &ldquo;size&rdquo; of your parameters is not the only way to measure model complexity. Conversely, minimizing the size of your parameters is also not the only way to punish model complexity. Dropout achieves a similar result by doing something more intuitive to some.</p><p>In a nutshell, dropout <em>randomly</em> deletes some fraction of nodes in each hidden layer on each forward-backward pass. Since on each forward-backwards pass you are &ldquo;turning off&rdquo; some hidden units, the model trained on each iteration is literally smaller. The random component of the dropout guarantees that all parts of the model are affected equally on expectation. So what do we mean exactly by &ldquo;deleting&rdquo; nodes? We mean setting them to 0. It really is that simple.</p><p>More formally, there is some probability $p$ that represents probability for <em>a single hidden unit</em> to be kept untouched. $p$ is referred to as the <em>keep probability</em>. Conversely, $1 - p$ is the &ldquo;dropout&rdquo; probability. Notice that the events are <em>independent</em> of each other. That is each <em>hidden unit</em>, i.e. each node is dropped independently relative to all the other units in that layer, and also from other layers.</p><p>Say that you have a layer with $50$ hidden units, and that $p = 0.8$, so that you will be dropping out $1 - 0.8 = 0.2$ of the units in a hidden layer. So on expectation you will be dropping $50 * 0.2 = 10$ nodes. Let&rsquo;s also think that this layer is $l = 3$. Let&rsquo;s think about how this will affect the next layer.</p><p>The next layer&rsquo;s computation will be:</p><p>$$
\begin{equation}
Z^{[4]} = W^{[4]}A^{[3]} + b^{[4]}
\end{equation}
$$</p><p>Where $A^{[3]}$ is the output of the layer with $50$ hidden units, and we shut down $20\%$ of them. This means that $A^{[3]}$ is reduced by $20\%$, which also implies that $Z^{[4]}$ is reduced by $20\%$ because $Z^{[4]}$ is just a linear combination of $A^{[3]}$. In order to avoid this problem propagating throughout our network making everything small, we need to roughly &ldquo;refund&rdquo; the lost value to $A^{[3]}$ before passing it to the next layer. We do this simply by:</p><p>$$
\begin{equation}
A^{[3*]} = \frac{A^{[3]}}{p}
\end{equation}
$$</p><p>Remember that dividing anything by a number less than $1$ will make it bigger. Therefore:</p><p>$$
\begin{equation}
Z^{[4]} = W^{[4]}A^{[3*]} + b^{[4]}
\end{equation}
$$</p><p>Will make $Z^{[4]}$ be <em>roughly</em> the same as it was originally. This scaling technique is called <a href=https://machinelearning.wtf/terms/inverted-dropout/>inverted dropout</a>.</p><p>The intuition behind this rescaling is that when you set some hidden-units to zero, you are shifting the expected value of $A^{[3]}$, which is an issue when you do testing. Because when you evaluate your network with the test set, you <strong>do not do any dropout</strong>, the difference in expected values becomes a problem. Therefore, the rescaling ameliorates the scaling issue between training and testing.</p><p>Finally, you can apply a different value of $p$ to different layers, so that you have a vector $p$ where $p^{[l]}$ is the probability of keeping units in hidden layer $l$. This faces you with a tradeoff of having more hyperparameters, so it should be used wisely.</p><h3 id=understanding-dropout>Understanding Dropout<a hidden class=anchor aria-hidden=true href=#understanding-dropout>#</a></h3><p>So why does dropout behave like regularization? It seems crazy to &ldquo;kill&rdquo; some nodes at random, it&rsquo;s the very definition of chaotic evil. It turns out that by doing this, the model will learn not to rely on any specific feature, because it might not be there the next time. The model will respond to dropout by <em>spreading</em> out the weights across the network, effectively reducing their magnitude. This is why dropout can be thought of as regularization.</p><blockquote><p>An interesting approach to building resilience in engineering systems is that of <a href=https://principlesofchaos.org/>Chaos Engineering</a>. It&rsquo;s similar in spirit to dropout, whereby randomly disabling some parts of a system, and forcing designers to deal with random failures will produce a more robust system.</p></blockquote><h3 id=other-regularization-methods>Other Regularization Methods<a hidden class=anchor aria-hidden=true href=#other-regularization-methods>#</a></h3><p>Another regularization method is that of data augmentation. For example if you have images as your training data, you might think about applying some geometric distortions (flipping, scaling, etc.). This works as regularization by injecting more variance into your training set, making your model smarter by making it harder for it to put all its eggs in one basket (focusing too much on some parameters).</p><p>Another method is early stopping. This amounts to regularization because early stopping might amount to stopping training <em>before</em> $W{[l]}$ gets &ldquo;too big&rdquo;. What is too big? You won&rsquo;t know. The problem with early stopping is that it conflates two processes that should be orthogonal (independent): doing well on your training samples and <em>not</em> overfitting. There&rsquo;s more information about orthogonalizing your goals in the next course.</p><h2 id=setting-up-our-optimization-problem>Setting up our Optimization Problem<a hidden class=anchor aria-hidden=true href=#setting-up-our-optimization-problem>#</a></h2><p>One thing is to make sure that your model has the right combination of bias and variance, which amounts to learning the problem at hand well enough and also being able to generalize to unseen samples. Now, we turn our attention to making the optimization process (learning the problem) easier and more efficient.</p><h2 id=normalizing-inputs>Normalizing Inputs<a hidden class=anchor aria-hidden=true href=#normalizing-inputs>#</a></h2><p>Normalization is one of those topics that means different things to different people. <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> Let&rsquo;s disambiguate the term and also talk about why normalizing our inputs might be helpful for our optimization process.</p><p>As the name implies, normalization is the process of making something normal. Of course, there are many ways of making something &ldquo;normal&rdquo;, and we haven&rsquo;t even defined what &ldquo;normal&rdquo; is. Let&rsquo;s focus on one of the ways to normalize things.</p><p>In the context of statistics, normalization usually refers to <em>standardizing</em>, which is where we calculate a <a href=https://en.wikipedia.org/wiki/Standard_score><em>standard score</em></a> for a random variable $X$. It&rsquo;s called the <em>standard</em> score because its units are standard deviations.</p><p>Standardizing a random variable $X$ is pretty easy and intuitive:</p><p>$$
\begin{equation}
z = \frac{x - \mu}{\sigma}
\end{equation}
$$</p><p>Where $\mu$ is the mean of the population and $\sigma$ is the standard deviation of the population. Because we don&rsquo;t know those values, and we usually estimate them, we usually work with z-scores, which are the sample analogues:</p><p>$$
\begin{equation}
z = \frac{x - \bar{x}}{S}
\end{equation}
$$</p><p>Where $\bar{x}$ is the sample mean and $S$ is the sample standard deviation. If your features are normally distributed, then the transformed variables will approximately mean zero and standard deviation of one.</p><p>The process is intuitive, for each observation we first ask: how far is this observation from the sample mean. Whatever that distance is, positive or negative, we scale it by the standard deviation. So that the result is: an observation is $z$ standard deviations from the mean.</p><p>But why do this? The purpose of this process is to convert <em>different</em> random variables that have <em>different</em> scales and shifts, into the same units. The units will be standard deviations for that particular random variable. So that $z_i$ will tell you how many standard deviations observation $i$ is from the mean. Positive values will be above the mean and negative values will be below the mean. Using this approach we can negate the effects of having different features with wildly different unit scales and shapes.</p><p>When you normalize your training set, you want to use <a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#normalizing-inputs>standardization</a> on each of your features separately. However, when you are testing, you want to use the same $\bar{x}$ and $S$ that you calculated during training. You don&rsquo;t want to scale and shift your training data separately from your testing data. You want the opposite, you want your training and testing data to go through <em>the same</em> transformations.</p><p>How is this related to optimization? How does normalizing scales and shifts in our features affect our optimization? It all has to do with gradient descent and how the updates are made.</p><figure><img loading=lazy src=/images/04_25.jpg alt="Gradient Descent with and without features scaling" width=50%><figcaption><p><a href=https://livebook.manning.com/book/deep-learning-for-vision-systems/chapter-4/v-3/79>Gradient Descent with and without features scaling</a></p></figcaption></figure><p>The contour plot above shows an imaginary two-dimensional cost function $J(F_1, F_2)$. Notice that if we have a pair of unscaled features and one of the features&rsquo; scale is larger than the other, then the cost function will look squished. Gradient descent will take many tiny steps because it&rsquo;s considering <em>both</em> features at the same time. Since one feature is big and the other one small, the same step size is big in one but small on the other. This means that the larger feature will dominate the update. However, if you scale your features with standardization for example, then the cost function might look like the one on the right. Since all features are in a standard scale, no feature will dominate the update, allowing for less oscillation, and therefore faster convergence.</p><h2 id=vanishing-and-exploding-gradients>Vanishing and Exploding Gradients<a hidden class=anchor aria-hidden=true href=#vanishing-and-exploding-gradients>#</a></h2><p>Vanishing and exploding gradients sound magical, if not outright dangerous. What this means is that when we train very deep networks, and we take the derivatives for gradient descent, the magnitude of these derivatives either &ldquo;vanish&rdquo; to $0$ or &ldquo;explode&rdquo; towards $\infty$. How does this disaster come about?</p><p>Think about a very deep $L$-layered neural network. Imagine also that all $W^{[l]}$ are the same:</p><p>$$
\begin{equation}
W^{[l]} = \begin{bmatrix}
1.5 & 0 \\
0 & 1.5
\end{bmatrix}
\end{equation}
$$</p><p>Also imagine that all $b^{[l]} = 0$.</p><p>Since all $W^{[l]}$ are the same, then whatever our features $x$ is, we get that:</p><p>$$
\hat{y} = W^{[l]L}x
$$</p><p>That is the $W^{[l]}$ is multiplied by itself $L$ times. In our case this amounts to $1.5^L$, which literally grows exponentially with the number of layers. Imagine now that instead of $1.5$ on the diagonal of $W^{[l]}$ we have a number less than 0, i.e. $0.5$. With the same problem setup, now the values will be $0.5^L$ which is an expression that <em>shrinks</em> exponentially in the number of layers.</p><p>In either case, this is a huge problem. With exploding gradients, the gradient descent steps are gigantic, and with vanishing gradients the gradient descent steps are tiny. Either way, converging the minimum of our cost function will take a long, long time. We tackle this issue by being more careful in the way that we initialize our parameters.</p><h2 id=weight-initialization>Weight Initialization<a hidden class=anchor aria-hidden=true href=#weight-initialization>#</a></h2><p>In order to avoid exploding or vanishing gradients, we need to be more careful as to how we initialize our parameters. The main idea here is that the more hidden units in a hidden-layer, the smaller each $w^{[l]}_j$ you want. This is to keep large hidden layers and small hidden layers in around the same output scale.</p><p>We will still initialize our parameters to random values, but we will <em>scale</em> them by a term, $s$, that is proportional to the size of the inputs coming into a layer $l$. The whole point of this is to adjust the variance of $W^{[l]}$ so that it&rsquo;s proportional to inputs, $n^{[l-1]}$:</p><p>$$
\begin{equation}
W^{[l]} = \texttt{np.random.randn(shape)} \times s
\end{equation}
$$</p><p>The choice is $s$ depends on $g^{[l]}(x)$, the activation function for layer $l$:</p><ul><li>In the case that $g^{[l]}(x) = \text{ReLU}(x)$, then $s = \sqrt{\frac{2}{n^{[l-1]}}}$.</li><li>In the case that $g^{[l]}(x) = \text{tanh}(x)$, then $s = \sqrt{\frac{1}{n^{[l-1]}}}$, which is also called <a href=https://cs230.stanford.edu/section/4/>Xavier initialization</a>.<ul><li>Another variant for $\text{tanh}(x)$ is $s = \sqrt{\frac{2}{n^{[l-1]} + n^{[l]}}}$</li></ul></li></ul><p>Next week&rsquo;s post is <a href=http://localhost:1313/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/>here</a>.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html>Bias-Variance Tradeoff | Cornell University</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://en.wikipedia.org/wiki/Norm_%28mathematics%29>Norm | Wikipedia</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://en.wikipedia.org/wiki/Normalization_%28statistics%29>Normalization | Wikipedia</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/machine-learning/>Machine Learning</a></li><li><a href=http://localhost:1313/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/coursera/deep-learning-specialization/nn-dl/week4/><span class=title>« Prev</span><br><span>Neural Networks and Deep Learning: Week 4 | Deep Neural Networks</span>
</a><a class=next href=http://localhost:1313/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/><span class=title>Next »</span><br><span>Improving Deep Learning Networks: Week 2 | Optimization Algorithms</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=http://localhost:1313/>Manuel Martinez</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>