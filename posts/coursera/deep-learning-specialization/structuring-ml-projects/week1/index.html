<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Structuring ML Projects: Week 1 | ML Strategy | Manuel Martinez</title><meta name=keywords content="machine learning,deep learning"><meta name=description content="This is the first week of the third course of DeepLearning.AI&rsquo;s Deep Learning Specialization offered on Coursera. This course is less technical than the previous two and focuses instead on general principles and intuition related to machine learning projects.
This week&rsquo;s topics are:

Introduction to ML Strategy

Why ML Strategy
Orthogonalization


Setting Up our Goal

Single Number Evaluation Metric
Satisficing and Optimizing Metrics
Train/Dev/Test Distributions
Size of Dev and Test Sets
When to Change Dev/Test Sets and Metrics?


Comparing to Human-Level Performance

Why Human-level performance?
Avoidable Bias
Understanding Human-level Performance
Surpassing Human-level Performance
Improving your Model Performance




Introduction to ML Strategy
Why ML Strategy
Whenever we are working on a machine learning project, and after we have completed our first iteration on the approach, there might be many things to try next. Should we get more data? Try regularization? Try a bigger network? So many things to choose from. This is why we need high-level heuristics to guide our strategy."><meta name=author content="Manuel Martinez"><link rel=canonical href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://manmartgarc.github.io/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://manmartgarc.github.io/images/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://manmartgarc.github.io/images/favicon.ico><link rel=apple-touch-icon href=https://manmartgarc.github.io/apple-touch-icon.png><link rel=mask-icon href=https://manmartgarc.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/"><meta property="og:site_name" content="Manuel Martinez"><meta property="og:title" content="Structuring ML Projects: Week 1 | ML Strategy"><meta property="og:description" content="This is the first week of the third course of DeepLearning.AI’s Deep Learning Specialization offered on Coursera. This course is less technical than the previous two and focuses instead on general principles and intuition related to machine learning projects.
This week’s topics are:
Introduction to ML Strategy Why ML Strategy Orthogonalization Setting Up our Goal Single Number Evaluation Metric Satisficing and Optimizing Metrics Train/Dev/Test Distributions Size of Dev and Test Sets When to Change Dev/Test Sets and Metrics? Comparing to Human-Level Performance Why Human-level performance? Avoidable Bias Understanding Human-level Performance Surpassing Human-level Performance Improving your Model Performance Introduction to ML Strategy Why ML Strategy Whenever we are working on a machine learning project, and after we have completed our first iteration on the approach, there might be many things to try next. Should we get more data? Try regularization? Try a bigger network? So many things to choose from. This is why we need high-level heuristics to guide our strategy."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-23T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-23T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Structuring ML Projects: Week 1 | ML Strategy"><meta name=twitter:description content="This is the first week of the third course of DeepLearning.AI&rsquo;s Deep Learning Specialization offered on Coursera. This course is less technical than the previous two and focuses instead on general principles and intuition related to machine learning projects.
This week&rsquo;s topics are:

Introduction to ML Strategy

Why ML Strategy
Orthogonalization


Setting Up our Goal

Single Number Evaluation Metric
Satisficing and Optimizing Metrics
Train/Dev/Test Distributions
Size of Dev and Test Sets
When to Change Dev/Test Sets and Metrics?


Comparing to Human-Level Performance

Why Human-level performance?
Avoidable Bias
Understanding Human-level Performance
Surpassing Human-level Performance
Improving your Model Performance




Introduction to ML Strategy
Why ML Strategy
Whenever we are working on a machine learning project, and after we have completed our first iteration on the approach, there might be many things to try next. Should we get more data? Try regularization? Try a bigger network? So many things to choose from. This is why we need high-level heuristics to guide our strategy."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://manmartgarc.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Structuring ML Projects: Week 1 | ML Strategy","item":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Structuring ML Projects: Week 1 | ML Strategy","name":"Structuring ML Projects: Week 1 | ML Strategy","description":"This is the first week of the third course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. This course is less technical than the previous two and focuses instead on general principles and intuition related to machine learning projects.\nThis week\u0026rsquo;s topics are:\nIntroduction to ML Strategy Why ML Strategy Orthogonalization Setting Up our Goal Single Number Evaluation Metric Satisficing and Optimizing Metrics Train/Dev/Test Distributions Size of Dev and Test Sets When to Change Dev/Test Sets and Metrics? Comparing to Human-Level Performance Why Human-level performance? Avoidable Bias Understanding Human-level Performance Surpassing Human-level Performance Improving your Model Performance Introduction to ML Strategy Why ML Strategy Whenever we are working on a machine learning project, and after we have completed our first iteration on the approach, there might be many things to try next. Should we get more data? Try regularization? Try a bigger network? So many things to choose from. This is why we need high-level heuristics to guide our strategy.\n","keywords":["machine learning","deep learning"],"articleBody":"This is the first week of the third course of DeepLearning.AI’s Deep Learning Specialization offered on Coursera. This course is less technical than the previous two and focuses instead on general principles and intuition related to machine learning projects.\nThis week’s topics are:\nIntroduction to ML Strategy Why ML Strategy Orthogonalization Setting Up our Goal Single Number Evaluation Metric Satisficing and Optimizing Metrics Train/Dev/Test Distributions Size of Dev and Test Sets When to Change Dev/Test Sets and Metrics? Comparing to Human-Level Performance Why Human-level performance? Avoidable Bias Understanding Human-level Performance Surpassing Human-level Performance Improving your Model Performance Introduction to ML Strategy Why ML Strategy Whenever we are working on a machine learning project, and after we have completed our first iteration on the approach, there might be many things to try next. Should we get more data? Try regularization? Try a bigger network? So many things to choose from. This is why we need high-level heuristics to guide our strategy.\nOrthogonalization Because there are so many things that could be changed simultaneously, a key idea is that of orthogonalization. In the context of computer science, orthogonality refers to a system design property whereby changing one thing changes that one thing only and nothing else. For example, changing the brightness setting on your phone changes only the brightness, and not whether your phone is on silent or not. We can say that these two components are orthogonal.\nIn our machine learning setting, we want to define some things that we can change in an orthogonal way. For example, we want to reduce bias, and reduce bias only; without any side effects. Some of the goals that we want to engage in during a project can be (in order):\nFit training set well on cost function. Fit dev set well on cost function. Fit test set well on cost function. Performs well on the real world. We can make progress toward each of these goals in an orthogonal way via different processes.\nAs a counter-example to orthogonalization think about early stopping. With early stopping you will fit your training set less well but also fit your dev set better; that is changing two things at once. This is why early stopping is usually not a suggested approach in the course.\nSetting Up our Goal Single Number Evaluation Metric The key idea to making progress in our project is to be able to quickly compare different approaches. We can do this by defining a single real-valued evaluation metric.\nAs an example, think that you have two classifiers: $A, B$, and the following performance metrics:\n$A$ - Precision: $95\\%$, Recall: $90\\%$ $B$ - Precision: $98\\%$, Recall: $85\\%$ Remember that precision tells you the proportion of true positives out of true positives and false positives, while recall tells you the proportion of true positives out of true positives and false negatives.\nYou might have an application where recall is more important than precision: classifying flight-risk when setting bails. In this case your single metric would be recall. However, if you’re not sure, how can you compare classifiers $A$ and $B$?\nIn this case we can combine precision and recall into the F1-score which is the harmonic mean of both precision and recall.\nAnother example is wanting to implement a model that performs good over different geographical regions. If you have $K$ geographical regions you might have $K$ different evaluation metrics, one for each region. The solution here is to take the average and compare the average of each classifier across all regions and pick the best one.\nThey key take away here is to have a single evaluation metric, and also to have a dev set. With these two things you can quickly iterate over different approaches and pick the best performing one.\nSatisficing and Optimizing Metrics Satisficing is a decision-making strategy introduced by the legendary Herbert A. Simon in 1947. It differs from optimizing metrics by the fact that satisficing metrics don’t have to be the best but just good enough instead.\nIn our setting, an optimizing metric might be the performance of our classifier. Whereas latency or training time might be a satisficing metric. Being able to clearly think of which goals are which will help us to iterate faster.\nWe could also have our model performance to be a satisficing metric: we are okay with some number of bad predictions over some period of time.\nThe main takeaway is that if you have $N$ metrics, you should have $1$ optimizing metric and $N-1$ satisficing metrics. This is because optimizing for two metrics is complicated and also not guaranteed to be aligned.\nTrain/Dev/Test Distributions Very simple: make sure our dev and test set come from the same distribution. If we train a classifier to predict defaulting on a loan, having high-income zip codes in our dev set and low-income zip codes in our test sets is a terrible idea. The dev and test sets should reflect the data we expect to get in the future, and is relative to the main application of our machine learning model. In practice this usually means randomly shuffling your data before splitting it.\nSize of Dev and Test Sets This was covered before as well.\nThe key takeaway is to make sure that our test set is big enough to reduce the probability that the result obtained is not due to random chance. This could be a lot less than $30\\%$ of your data if you have a lot of data.\nIn some settings not having a test set might be okay, but it will never be better than having a test set.\nWhen to Change Dev/Test Sets and Metrics? If we find that our algorithm is performing “well” but when being used has side effects, we should definitely change our splits and/or metrics.\nThe example in the course goes over a cat classifier, where classifier $A$ performs better than classifier $B$. However, $A$ also shows some illicit content to some users, while $B$ does not. Clearly you should use $B$. But how can we incorporate this new preference, not showing illicit content, into our metric?\nThe idea is to first think about how to express the new preference, i.e. a penalization for illicit content. Afterwards we can separately (orthogonalization) think about how to do well on this new metric.\nThe key takeaway here is to not marry a metric and/or data split. We should be able to rapidly pivot if we gain new information about our problem and how our model is performing in relationship to our problem.\nComparing to Human-Level Performance Why Human-level performance? Humans are very skilled at some tasks. Remember there are three performance levels for any classification task:\nBayes error rate: The theoretical limit that a perfect classifier would achieve when using random data. It’s not $0$. Human-level error: The performance of the best human(s) at the task. It will be higher than Bayes error rate. Training error: The performance of our model on the training set. It might be lower than human-level performance or higher, but never lower than Bayes error rate. Knowing the human-level performance is extremely useful, because it allows us to reason about our training error. If our model is doing better than human-level error, then we might be close to Bayes error rate and improving training set performance might result in overfitting. On the other hand, if we are below human-level error, then it means that we should keep trying to improve our model’s performance on the training set. Depending on whether our model is above or below human-level performance can tell us which thing we should focus on next.\nAvoidable Bias The idea of comparing our model’s performance to that of humans is very powerful. In general, human-level performance is very close to Bayes error rate; which means that we can, in general, treat human-level performance as the best we can do. The difference between our model’s performance and human-level performance (assuming it’s a proxy for Bayes error rate) is called avoidable bias. It’s called avoidable because, in general, we should be able to perform as good as human-level performance.\nTo cement this idea, and how the comparison can affect the direction of your project, let’s go over a simple example. Imagine that we have some classification task with the following performance numbers:\nHuman-level performance: $1\\%$, Training Error: $8\\%$, Dev error: $10\\%$ Because there is still about $8\\% - 1\\% = 7\\%$ avoidable bias, we should focus on reducing bias. Train a larger model, train for longer, reduce regularization, etc. Human-level performance: $7.5\\%$, Training Error: $8\\%$, Dev error: $10\\%$ Because the avoidable bias is smaller than the difference between the dev error and the training error, we should focus more on reducing variance. That is the difference between the dev error and the training error. So how come we reached different conclusions on two classifiers that have the same performance? The only thing that is different is the human-level performance. Because we are thinking of human-level performance as a proxy for Bayes error rate, then if the human-level performance changes, then everything changes. If we don’t know human performance, we must make some educated guess about it.\nThe key thing here is to look at the difference between training error and human-level error, which we call avoidable bias. On the other hand, the difference between the dev error and the training error is a measure of variance. We should tackle whichever is greater first.\nUnderstanding Human-level Performance When talking about human-level performance, we usually mean the best performing individual human or group of humans. We don’t take whether it takes a village, we just want to know what’s the best that the human species can do. We care about this for two reasons:\nIt will inform us about how much we should expect our training error to be. It will give us an upper bound of the Bayes error rate. If you have a bias problem (you haven’t reached human-level performance yet) focus on this first. If we are relatively close to human-level performance but have a variance problem, focus on that instead.\nSurpassing Human-level Performance We have been talking about human-level performance for a while now, and you might be thinking: haven’t some models beaten humans? Yes, but only in certain problems. Of course the set of problems that have been solved with better-than-human performance is changing every day.\nWithin our context, the thing to keep in mind is that once our model is doing better than human-level performance, we might be very close to Bayes error rate; therefore any improvements in the training error might be very costly, and might actually make our model overfit.\nImproving your Model Performance Okay, so we introduced all these topics above; now how do we put them together?\nWhen doing supervised learning, there are two fundamental assumptions, and how good those assumptions are being met will tell us what do next:\nWe can fit the training set pretty well: This means that we have eliminated all or almost all avoidable bias. The training set performance generalizes pretty well to the dev/test set. This means that we have found a good amount of variance in our model. So if we still have an avoidable bias problem we can try the following:\nTraining a bigger model Train longer and/or with better optimization algorithms Try a specialized NN architecture and/or improve hyperparameters If we solved the bias problem, but we still have a variance problem we can try:\nTry getting more data Add more regularization Try a specialized NN architecture and/or improve hyperparameters Next week’s post is here.\n","wordCount":"1937","inLanguage":"en","datePublished":"2023-06-23T00:00:00Z","dateModified":"2023-06-23T00:00:00Z","author":{"@type":"Person","name":"Manuel Martinez"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/"},"publisher":{"@type":"Organization","name":"Manuel Martinez","logo":{"@type":"ImageObject","url":"https://manmartgarc.github.io/images/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://manmartgarc.github.io/ accesskey=h title="Manuel Martinez (Alt + H)">Manuel Martinez</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://manmartgarc.github.io/about/ title=About><span>About</span></a></li><li><a href=https://manmartgarc.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://manmartgarc.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://manmartgarc.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://manmartgarc.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://manmartgarc.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://manmartgarc.github.io/cv/ title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://manmartgarc.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://manmartgarc.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Structuring ML Projects: Week 1 | ML Strategy</h1><div class=post-meta><span title='2023-06-23 00:00:00 +0000 UTC'>June 23, 2023</span>&nbsp;·&nbsp;<span>10 min</span>&nbsp;·&nbsp;<span>Manuel Martinez</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction-to-ml-strategy>Introduction to ML Strategy</a><ul><li><a href=#why-ml-strategy>Why ML Strategy</a></li><li><a href=#orthogonalization>Orthogonalization</a></li></ul></li><li><a href=#setting-up-our-goal>Setting Up our Goal</a><ul><li><a href=#single-number-evaluation-metric>Single Number Evaluation Metric</a></li><li><a href=#satisficing-and-optimizing-metrics>Satisficing and Optimizing Metrics</a></li><li><a href=#traindevtest-distributions>Train/Dev/Test Distributions</a></li><li><a href=#size-of-dev-and-test-sets>Size of Dev and Test Sets</a></li><li><a href=#when-to-change-devtest-sets-and-metrics>When to Change Dev/Test Sets and Metrics?</a></li></ul></li><li><a href=#comparing-to-human-level-performance>Comparing to Human-Level Performance</a><ul><li><a href=#why-human-level-performance>Why Human-level performance?</a></li><li><a href=#avoidable-bias>Avoidable Bias</a></li><li><a href=#understanding-human-level-performance>Understanding Human-level Performance</a></li><li><a href=#surpassing-human-level-performance>Surpassing Human-level Performance</a></li><li><a href=#improving-your-model-performance>Improving your Model Performance</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>This is the first week of the <a href="https://www.coursera.org/learn/machine-learning-projects?specialization=deep-learning">third course</a> of DeepLearning.AI&rsquo;s <a href=https://www.coursera.org/specializations/deep-learning>Deep Learning Specialization</a> offered on Coursera. This course is less technical than the previous two and focuses instead on general principles and intuition related to machine learning projects.</p><p>This week&rsquo;s topics are:</p><ul><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#introduction-to-ml-strategy>Introduction to ML Strategy</a><ul><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#why-ml-strategy>Why ML Strategy</a></li><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#orthogonalization>Orthogonalization</a></li></ul></li><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#setting-up-our-goal>Setting Up our Goal</a><ul><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#single-number-evaluation-metric>Single Number Evaluation Metric</a></li><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#satisficing-and-optimizing-metrics>Satisficing and Optimizing Metrics</a></li><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#traindevtest-distributions>Train/Dev/Test Distributions</a></li><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#size-of-dev-and-test-sets>Size of Dev and Test Sets</a></li><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#when-to-change-devtest-sets-and-metrics>When to Change Dev/Test Sets and Metrics?</a></li></ul></li><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#comparing-to-human-level-performance>Comparing to Human-Level Performance</a><ul><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#why-human-level-performance>Why Human-level performance?</a></li><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#avoidable-bias>Avoidable Bias</a></li><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#understanding-human-level-performance>Understanding Human-level Performance</a></li><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#surpassing-human-level-performance>Surpassing Human-level Performance</a></li><li><a href=/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#improving-your-model-performance>Improving your Model Performance</a></li></ul></li></ul><hr><h2 id=introduction-to-ml-strategy>Introduction to ML Strategy<a hidden class=anchor aria-hidden=true href=#introduction-to-ml-strategy>#</a></h2><h3 id=why-ml-strategy>Why ML Strategy<a hidden class=anchor aria-hidden=true href=#why-ml-strategy>#</a></h3><p>Whenever we are working on a machine learning project, and after we have completed our first iteration on the approach, there might be many things to try next. Should we get more data? Try regularization? Try a bigger network? So many things to choose from. This is why we need high-level heuristics to guide our strategy.</p><h3 id=orthogonalization>Orthogonalization<a hidden class=anchor aria-hidden=true href=#orthogonalization>#</a></h3><p>Because there are so many things that could be changed simultaneously, a key idea is that of orthogonalization. In the context of computer science, orthogonality refers to a system design property whereby changing one thing changes that one thing only and nothing else. For example, changing the brightness setting on your phone changes only the brightness, and not whether your phone is on silent or not. We can say that these two components are orthogonal.</p><p>In our machine learning setting, we want to define some things that we can change in an orthogonal way. For example, we want to reduce bias, and reduce bias only; without any side effects. Some of the goals that we want to engage in during a project can be (in order):</p><ol><li>Fit training set well on cost function.</li><li>Fit dev set well on cost function.</li><li>Fit test set well on cost function.</li><li>Performs well on the real world.</li></ol><p>We can make progress toward each of these goals in an orthogonal way via different processes.</p><p>As a counter-example to orthogonalization think about early stopping. With early stopping you will fit your training set <em>less</em> well but also fit your dev set <em>better</em>; that is changing two things at once. This is why early stopping is usually not a suggested approach in the course.</p><h2 id=setting-up-our-goal>Setting Up our Goal<a hidden class=anchor aria-hidden=true href=#setting-up-our-goal>#</a></h2><h3 id=single-number-evaluation-metric>Single Number Evaluation Metric<a hidden class=anchor aria-hidden=true href=#single-number-evaluation-metric>#</a></h3><p>The key idea to making progress in our project is to be able to quickly compare different approaches. We can do this by defining a <em>single</em> real-valued evaluation metric.</p><p>As an example, think that you have two classifiers: $A, B$, and the following performance metrics:</p><ul><li>$A$ - Precision: $95\%$, Recall: $90\%$</li><li>$B$ - Precision: $98\%$, Recall: $85\%$</li></ul><blockquote><p>Remember that precision tells you the proportion of true positives out of true positives and false positives, while recall tells you the proportion of true positives out of true positives and false negatives.</p></blockquote><p>You might have an application where recall is more important than precision: classifying flight-risk when setting bails. In this case your single metric would be recall. However, if you&rsquo;re not sure, how can you compare classifiers $A$ and $B$?</p><p>In this case we can combine precision and recall into the <a href=https://en.wikipedia.org/wiki/F-score#Definition>F1-score</a> which is the harmonic mean of both precision and recall.</p><p>Another example is wanting to implement a model that performs good over different geographical regions. If you have $K$ geographical regions you might have $K$ different evaluation metrics, one for each region. The solution here is to take the average and compare the average of each classifier across all regions and pick the best one.</p><p>They key take away here is to have a single evaluation metric, and also to have a dev set. With these two things you can quickly iterate over different approaches and pick the best performing one.</p><h3 id=satisficing-and-optimizing-metrics>Satisficing and Optimizing Metrics<a hidden class=anchor aria-hidden=true href=#satisficing-and-optimizing-metrics>#</a></h3><p><a href=https://en.wikipedia.org/wiki/Satisficing>Satisficing</a> is a decision-making strategy introduced by the legendary Herbert A. Simon in 1947. It differs from optimizing metrics by the fact that satisficing metrics don&rsquo;t have to be the <em>best</em> but just <em>good enough</em> instead.</p><p>In our setting, an optimizing metric might be the performance of our classifier. Whereas latency or training time might be a satisficing metric. Being able to clearly think of which goals are which will help us to iterate faster.</p><blockquote><p>We could also have our model performance to be a satisficing metric: we are okay with some number of bad predictions over some period of time.</p></blockquote><p>The main takeaway is that if you have $N$ metrics, you should have $1$ optimizing metric and $N-1$ satisficing metrics. This is because optimizing for two metrics is complicated and also not guaranteed to be aligned.</p><h3 id=traindevtest-distributions>Train/Dev/Test Distributions<a hidden class=anchor aria-hidden=true href=#traindevtest-distributions>#</a></h3><p>Very simple: make sure our dev and test set come from the same distribution. If we train a classifier to predict defaulting on a loan, having high-income zip codes in our dev set and low-income zip codes in our test sets is a terrible idea. The dev and test sets should reflect the data we <em>expect</em> to get in the future, and is relative to the main application of our machine learning model. In practice this usually means randomly shuffling your data before splitting it.</p><h3 id=size-of-dev-and-test-sets>Size of Dev and Test Sets<a hidden class=anchor aria-hidden=true href=#size-of-dev-and-test-sets>#</a></h3><p>This was <a href=/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#train--dev--test-sets>covered before</a> as well.</p><p>The key takeaway is to make sure that our test set is <em>big enough</em> to reduce the probability that the result obtained is not due to random chance. This could be a lot less than $30\%$ of your data if you have a lot of data.</p><p>In some settings not having a test set might be okay, but it will never be better than having a test set.</p><h3 id=when-to-change-devtest-sets-and-metrics>When to Change Dev/Test Sets and Metrics?<a hidden class=anchor aria-hidden=true href=#when-to-change-devtest-sets-and-metrics>#</a></h3><p>If we find that our algorithm is performing &ldquo;well&rdquo; but when being used has side effects, we should <em>definitely</em> change our splits and/or metrics.</p><p>The example in the course goes over a cat classifier, where classifier $A$ performs better than classifier $B$. However, $A$ also shows some illicit content to some users, while $B$ does not. Clearly you should use $B$. But how can we incorporate this new preference, not showing illicit content, into our metric?</p><p>The idea is to first think about how to express the new preference, i.e. a penalization for illicit content. Afterwards we can separately (orthogonalization) think about how to do well on this new metric.</p><p>The key takeaway here is to not marry a metric and/or data split. We should be able to rapidly pivot if we gain new information about our problem and how our model is performing in relationship to our problem.</p><h2 id=comparing-to-human-level-performance>Comparing to Human-Level Performance<a hidden class=anchor aria-hidden=true href=#comparing-to-human-level-performance>#</a></h2><h3 id=why-human-level-performance>Why Human-level performance?<a hidden class=anchor aria-hidden=true href=#why-human-level-performance>#</a></h3><p>Humans are very skilled at some tasks. Remember there are three performance levels for any classification task:</p><ol><li>Bayes error rate: The theoretical limit that a perfect classifier would achieve when using random data. It&rsquo;s not $0$.</li><li>Human-level error: The performance of the best human(s) at the task. It will be higher than Bayes error rate.</li><li>Training error: The performance of our model on the training set. It might be lower than human-level performance or higher, but never lower than Bayes error rate.</li></ol><p>Knowing the human-level performance is extremely useful, because it allows us to reason about our training error. If our model is doing better than human-level error, then we might be close to Bayes error rate and improving training set performance might result in overfitting. On the other hand, if we are below human-level error, then it means that we should keep trying to improve our model&rsquo;s performance on the training set. Depending on whether our model is above or below human-level performance can tell us which thing we should focus on next.</p><h3 id=avoidable-bias>Avoidable Bias<a hidden class=anchor aria-hidden=true href=#avoidable-bias>#</a></h3><p>The idea of comparing our model&rsquo;s performance to that of humans is very powerful. In general, human-level performance is very close to Bayes error rate; which means that we can, in general, treat human-level performance as the best we can do. The difference between our model&rsquo;s performance and human-level performance (assuming it&rsquo;s a proxy for Bayes error rate) is called <strong>avoidable bias</strong>. It&rsquo;s called avoidable because, in general, we should be able to perform as good as human-level performance.</p><p>To cement this idea, and how the comparison can affect the direction of your project, let&rsquo;s go over a simple example. Imagine that we have some classification task with the following performance numbers:</p><ul><li>Human-level performance: $1\%$, Training Error: $8\%$, Dev error: $10\%$<ul><li>Because there is still about $8\% - 1\% = 7\%$ avoidable bias, we should focus on reducing bias. Train a larger model, train for longer, reduce regularization, etc.</li></ul></li><li>Human-level performance: $7.5\%$, Training Error: $8\%$, Dev error: $10\%$<ul><li>Because the avoidable bias is smaller than the difference between the dev error and the training error, we should focus more on reducing variance. That is the difference between the dev error and the training error.</li></ul></li></ul><p>So how come we reached different conclusions on two classifiers that have the same performance? The only thing that is different is the human-level performance. Because we are thinking of human-level performance as a proxy for Bayes error rate, then if the human-level performance changes, then everything changes. If we don&rsquo;t know human performance, we must make some educated guess about it.</p><p>The key thing here is to look at the difference between training error and human-level error, which we call <em>avoidable bias</em>. On the other hand, the difference between the dev error and the training error is a measure of <em>variance</em>. We should tackle whichever is greater first.</p><h3 id=understanding-human-level-performance>Understanding Human-level Performance<a hidden class=anchor aria-hidden=true href=#understanding-human-level-performance>#</a></h3><p>When talking about human-level performance, we usually mean the <em>best</em> performing individual human or group of humans. We don&rsquo;t take whether it takes a village, we just want to know what&rsquo;s the best that the human species can do. We care about this for two reasons:</p><ol><li>It will inform us about how much we should expect our training error to be.</li><li>It will give us an upper bound of the Bayes error rate.</li></ol><p>If you have a bias problem (you haven&rsquo;t reached human-level performance yet) focus on this first. If we are relatively close to human-level performance but have a variance problem, focus on that instead.</p><h3 id=surpassing-human-level-performance>Surpassing Human-level Performance<a hidden class=anchor aria-hidden=true href=#surpassing-human-level-performance>#</a></h3><p>We have been talking about human-level performance for a while now, and you might be thinking: haven&rsquo;t some models beaten humans? Yes, but only in certain problems. Of course the set of problems that have been solved with better-than-human performance is changing every day.</p><p>Within our context, the thing to keep in mind is that once our model is doing better than human-level performance, we might be very close to Bayes error rate; therefore any improvements in the training error might be very costly, and might actually make our model overfit.</p><h3 id=improving-your-model-performance>Improving your Model Performance<a hidden class=anchor aria-hidden=true href=#improving-your-model-performance>#</a></h3><p>Okay, so we introduced all these topics above; now how do we put them together?</p><p>When doing supervised learning, there are two fundamental assumptions, and how good those assumptions are being met will tell us what do next:</p><ol><li>We can fit the training set pretty well:<ul><li>This means that we have eliminated all or almost all avoidable bias.</li></ul></li><li>The training set performance generalizes pretty well to the dev/test set.<ul><li>This means that we have found a good amount of variance in our model.</li></ul></li></ol><p>So if we still have an avoidable bias problem we can try the following:</p><ul><li>Training a bigger model</li><li>Train longer and/or with better optimization algorithms</li><li>Try a specialized NN architecture and/or improve hyperparameters</li></ul><p>If we solved the bias problem, but we still have a variance problem we can try:</p><ul><li>Try getting more data</li><li>Add more regularization</li><li>Try a specialized NN architecture and/or improve hyperparameters</li></ul><p>Next week&rsquo;s post is <a href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/>here</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://manmartgarc.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=https://manmartgarc.github.io/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=prev href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/><span class=title>« Prev</span><br><span>Improving Deep Learning Networks: Week 3 | Hyperparameter Tuning, Batch Optimization, Programming Frameworks</span>
</a><a class=next href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/><span class=title>Next »</span><br><span>Structuring ML Projects: Week 2 | ML Strategy</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://manmartgarc.github.io/>Manuel Martinez</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>