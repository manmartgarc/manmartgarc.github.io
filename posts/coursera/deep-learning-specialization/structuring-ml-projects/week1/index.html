<!DOCTYPE html>
<html lang="en">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <title>
  Structuring ML Projects: Week 1 | ML Strategy · Manuel Martinez
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Manuel Martinez">
<meta name="description" content="This is the first week of the third course of DeepLearning.AI&rsquo;s Deep Learning Specialization offered on Coursera. This course is less technical than the previous two, and focuses instead on general principles and intuition related to machine learning projects.
This week&rsquo;s topics are:

Introduction to ML Strategy

Why ML Strategy
Orthogonalization


Setting Up our Goal

Single Number Evaluation Metric
Satisficing and Optimizing Metrics
Train/Dev/Test Distributions
Size of Dev and Test Sets
When to Change Dev/Test Sets and Metrics?


Comparing to Human-Level Performance

Why Human-level performance?
Avoidable Bias
Understanding Human-level Performance
Surpassing Human-level Performance
Improving your Model Performance





  Introduction to ML Strategy
  
    
    Link to heading
  


  Why ML Strategy
  
    
    Link to heading
  

Whenever we are working on a machine learning project, and after we have completed our first iteration on the approach, there might be many things to try next. Should we get more data? Try regularization? Try a bigger network? So many things to choose from. This is why we need high-level heuristics to guide our strategy.">
<meta name="keywords" content="blog,developer,personal">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Structuring ML Projects: Week 1 | ML Strategy">
  <meta name="twitter:description" content="This is the first week of the third course of DeepLearning.AI’s Deep Learning Specialization offered on Coursera. This course is less technical than the previous two, and focuses instead on general principles and intuition related to machine learning projects.
This week’s topics are:
Introduction to ML Strategy Why ML Strategy Orthogonalization Setting Up our Goal Single Number Evaluation Metric Satisficing and Optimizing Metrics Train/Dev/Test Distributions Size of Dev and Test Sets When to Change Dev/Test Sets and Metrics? Comparing to Human-Level Performance Why Human-level performance? Avoidable Bias Understanding Human-level Performance Surpassing Human-level Performance Improving your Model Performance Introduction to ML Strategy Link to heading Why ML Strategy Link to heading Whenever we are working on a machine learning project, and after we have completed our first iteration on the approach, there might be many things to try next. Should we get more data? Try regularization? Try a bigger network? So many things to choose from. This is why we need high-level heuristics to guide our strategy.">

<meta property="og:url" content="http://localhost:1313/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/">
  <meta property="og:site_name" content="Manuel Martinez">
  <meta property="og:title" content="Structuring ML Projects: Week 1 | ML Strategy">
  <meta property="og:description" content="This is the first week of the third course of DeepLearning.AI’s Deep Learning Specialization offered on Coursera. This course is less technical than the previous two, and focuses instead on general principles and intuition related to machine learning projects.
This week’s topics are:
Introduction to ML Strategy Why ML Strategy Orthogonalization Setting Up our Goal Single Number Evaluation Metric Satisficing and Optimizing Metrics Train/Dev/Test Distributions Size of Dev and Test Sets When to Change Dev/Test Sets and Metrics? Comparing to Human-Level Performance Why Human-level performance? Avoidable Bias Understanding Human-level Performance Surpassing Human-level Performance Improving your Model Performance Introduction to ML Strategy Link to heading Why ML Strategy Link to heading Whenever we are working on a machine learning project, and after we have completed our first iteration on the approach, there might be many things to try next. Should we get more data? Try regularization? Try a bigger network? So many things to choose from. This is why we need high-level heuristics to guide our strategy.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-06-23T00:00:00+00:00">
    <meta property="article:modified_time" content="2023-06-23T00:00:00+00:00">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Deep Learning">




<link rel="canonical" href="http://localhost:1313/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.css" media="screen">






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.css" media="screen">
  



 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon.ico" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon.ico" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="http://localhost:1313/">
      Manuel Martinez
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Posts</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/projects/">Projects</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/cv/">CV</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://localhost:1313/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/">
              Structuring ML Projects: Week 1 | ML Strategy
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2023-06-23T00:00:00Z">
                June 23, 2023
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              10-minute read
            </span>
          </div>
          
          <div class="categories">
  <i class="fa-solid fa-folder" aria-hidden="true"></i>
    <a href="/categories/coursera/">Coursera</a>
      <span class="separator">•</span>
    <a href="/categories/deep-learning-specialization/">Deep Learning Specialization</a>
      <span class="separator">•</span>
    <a href="/categories/structuring-ml-projects/">Structuring ML Projects</a></div>

          <div class="tags">
  <i class="fa-solid fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/machine-learning/">Machine Learning</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/deep-learning/">Deep Learning</a>
    </span></div>

        </div>
      </header>

      <div class="post-content">
        
        <p>This is the first week of the <a href="https://www.coursera.org/learn/machine-learning-projects?specialization=deep-learning"  class="external-link" target="_blank" rel="noopener">third course</a> of DeepLearning.AI&rsquo;s <a href="https://www.coursera.org/specializations/deep-learning"  class="external-link" target="_blank" rel="noopener">Deep Learning Specialization</a> offered on Coursera. This course is less technical than the previous two, and focuses instead on general principles and intuition related to machine learning projects.</p>
<p>This week&rsquo;s topics are:</p>
<ul>
<li><a href="#introduction-to-ml-strategy" >Introduction to ML Strategy</a>
<ul>
<li><a href="#why-ml-strategy" >Why ML Strategy</a></li>
<li><a href="#orthogonalization" >Orthogonalization</a></li>
</ul>
</li>
<li><a href="#setting-up-our-goal" >Setting Up our Goal</a>
<ul>
<li><a href="#single-number-evaluation-metric" >Single Number Evaluation Metric</a></li>
<li><a href="#satisficing-and-optimizing-metrics" >Satisficing and Optimizing Metrics</a></li>
<li><a href="#traindevtest-distributions" >Train/Dev/Test Distributions</a></li>
<li><a href="#size-of-dev-and-test-sets" >Size of Dev and Test Sets</a></li>
<li><a href="#when-to-change-devtest-sets-and-metrics" >When to Change Dev/Test Sets and Metrics?</a></li>
</ul>
</li>
<li><a href="#comparing-to-human-level-performance" >Comparing to Human-Level Performance</a>
<ul>
<li><a href="#why-human-level-performance" >Why Human-level performance?</a></li>
<li><a href="#avoidable-bias" >Avoidable Bias</a></li>
<li><a href="#understanding-human-level-performance" >Understanding Human-level Performance</a></li>
<li><a href="#surpassing-human-level-performance" >Surpassing Human-level Performance</a></li>
<li><a href="#improving-your-model-performance" >Improving your Model Performance</a></li>
</ul>
</li>
</ul>
<hr>
<h2 id="introduction-to-ml-strategy">
  Introduction to ML Strategy
  <a class="heading-link" href="#introduction-to-ml-strategy">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<h3 id="why-ml-strategy">
  Why ML Strategy
  <a class="heading-link" href="#why-ml-strategy">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Whenever we are working on a machine learning project, and after we have completed our first iteration on the approach, there might be many things to try next. Should we get more data? Try regularization? Try a bigger network? So many things to choose from. This is why we need high-level heuristics to guide our strategy.</p>
<h3 id="orthogonalization">
  Orthogonalization
  <a class="heading-link" href="#orthogonalization">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Because there are so many things that could be changed simultaneously, a key idea is that of orthogonalization. In the context of computer science, orthogonality refers to a system design property whereby changing one thing changes that one thing only and nothing else. For example, changing the brightness setting on your phone changes only the brightness, and not whether your phone is on silent or not. We can say that these two components are orthogonal.</p>
<p>In our machine learning setting, we want to define some things that we can change in an orthogonal way. For example, we want to reduce bias, and reduce bias only; without any side effects. Some of the goals that we want to engage in during a project can be (in order):</p>
<ol>
<li>Fit training set well on cost function.</li>
<li>Fit dev set well on cost function.</li>
<li>Fit test set well on cost function.</li>
<li>Performs well on the real world.</li>
</ol>
<p>We can make progress toward each of these goals in an orthogonal way via different processes.</p>
<p>As a counter-example to orthogonalization think about early stopping. With early stopping you will fit your training set <em>less</em> well but also fit your dev set <em>better</em>; that is changing two things at once. This is why early stopping is usually not a suggested approach in the course.</p>
<h2 id="setting-up-our-goal">
  Setting Up our Goal
  <a class="heading-link" href="#setting-up-our-goal">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<h3 id="single-number-evaluation-metric">
  Single Number Evaluation Metric
  <a class="heading-link" href="#single-number-evaluation-metric">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>The key idea to making progress in our project is to be able to quickly compare different approaches. We can do this by defining a <em>single</em> real-valued evaluation metric.</p>
<p>As an example, think that you have two classifiers: $A, B$, and the following performance metrics:</p>
<ul>
<li>$A$ - Precision: $95\%$, Recall: $90\%$</li>
<li>$B$ - Precision: $98\%$, Recall: $85\%$</li>
</ul>
<blockquote>
<p>Remember that precision tells you the proportion of true positives out of true positives and false positives, while recall tells you the proportion of true positives out of true positives and false negatives.</p></blockquote>
<p>You might have an application where recall is more important than precision: classifying flight-risk when setting bails. In this case your single metric would be recall. However, if you&rsquo;re not sure, how can you compare classifiers $A$ and $B$?</p>
<p>In this case we can combine precision and recall into the <a href="https://en.wikipedia.org/wiki/F-score#Definition"  class="external-link" target="_blank" rel="noopener">F1-score</a> which is the harmonic mean of both precision and recall.</p>
<p>Another example is wanting to implement a model that performs good over different geographical regions. If you have $K$ geographical regions you might have $K$ different evaluation metrics, one for each region. The solution here is to take the average and compare the average of each classifier across all regions and pick the best one.</p>
<p>They key take away here is to have a single evaluation metric, and also to have a dev set. With these two things you can quickly iterate over different approaches and pick the best performing one.</p>
<h3 id="satisficing-and-optimizing-metrics">
  Satisficing and Optimizing Metrics
  <a class="heading-link" href="#satisficing-and-optimizing-metrics">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p><a href="https://en.wikipedia.org/wiki/Satisficing"  class="external-link" target="_blank" rel="noopener">Satisficing</a> is a decision-making strategy introduced by the legendary Herbert A. Simon in 1947. It differs from optimizing metrics by the fact that satisficing metrics don&rsquo;t have to be the <em>best</em> but just <em>good enough</em> instead.</p>
<p>In our setting, an optimizing metric might be the performance of our classifier. Whereas latency or training time might be a satisficing metric. Being able to clearly think of which goals are which will help us to iterate faster.</p>
<blockquote>
<p>We could also have our model performance to be a satisficing metric: we are okay with some number of bad predictions over some period of time.</p></blockquote>
<p>The main takeaway is that if you have $N$ metrics, you should have $1$ optimizing metric and $N-1$ satisficing metrics. This is because optimizing for two metrics is complicated and also not guaranteed to be aligned.</p>
<h3 id="traindevtest-distributions">
  Train/Dev/Test Distributions
  <a class="heading-link" href="#traindevtest-distributions">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Very simple: make sure our dev and test set come from the same distribution. If we train a classifier to predict defaulting on a loan, having high-income zip codes in our dev set and low-income zip codes in our test sets is a terrible idea. The dev and test sets should reflect the data we <em>expect</em> to get in the future, and is relative to the main application of our machine learning model. In practice this usually means randomly shuffling your data before splitting it.</p>
<h3 id="size-of-dev-and-test-sets">
  Size of Dev and Test Sets
  <a class="heading-link" href="#size-of-dev-and-test-sets">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>This was <a href="/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#train--dev--test-sets" >covered before</a> as well.</p>
<p>The key takeaway is to make sure that our test set is <em>big enough</em> to reduce the probability that the result obtained is not due to random chance. This could be a lot less than $30\%$ of your data if you have a lot of data.</p>
<p>In some settings not having a test set might be okay, but it will never be better than having a test set.</p>
<h3 id="when-to-change-devtest-sets-and-metrics">
  When to Change Dev/Test Sets and Metrics?
  <a class="heading-link" href="#when-to-change-devtest-sets-and-metrics">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>If we find that our algorithm is performing &ldquo;well&rdquo; but when being used has side effects, we should <em>definitely</em> change our splits and/or metrics.</p>
<p>The example in the course goes over a cat classifier, where classifier $A$ performs better than classifier $B$. However, $A$ also shows some illicit content to some users, while $B$ does not. Clearly you should use $B$. But how can we incorporate this new preference, not showing illicit content, into our metric?</p>
<p>The idea is to first think about how to express the new preference, i.e. a penalization for illicit content. Afterwards we can separately (orthogonalization) think about how to do well on this new metric.</p>
<p>The key takeaway here is to not marry a metric and/or data split. We should be able to rapidly pivot if we gain new information about our problem and how our model is performing in relationship to our problem.</p>
<h2 id="comparing-to-human-level-performance">
  Comparing to Human-Level Performance
  <a class="heading-link" href="#comparing-to-human-level-performance">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<h3 id="why-human-level-performance">
  Why Human-level performance?
  <a class="heading-link" href="#why-human-level-performance">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Humans are very skilled at some tasks. Remember there are three performance levels for any classification task:</p>
<ol>
<li>Bayes error rate: The theoretical limit that a perfect classifier would achieve when using random data. It&rsquo;s not $0$.</li>
<li>Human-level error: The performance of the best human(s) at the task. It will be higher than Bayes error rate.</li>
<li>Training error: The performance of our model on the training set. It might be lower than human-level performance or higher, but never lower than Bayes error rate.</li>
</ol>
<p>Knowing the human-level performance is extremely useful, because it allows us to reason about our training error. If our model is doing better than human-level error, then we might be close to Bayes error rate and improving training set performance might result in overfitting. On the other hand, if we are below human-level error, then it means that we should keep trying to improve our model&rsquo;s performance on the training set. Depending on whether our model is above or below human-level performance can tell us which thing we should focus on next.</p>
<h3 id="avoidable-bias">
  Avoidable Bias
  <a class="heading-link" href="#avoidable-bias">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>The idea of comparing our model&rsquo;s performance to that of humans is very powerful. In general, human-level performance is very close to Bayes error rate; which means that we can, in general, treat human-level performance as the best we can do. The difference between our model&rsquo;s performance and human-level performance (assuming it&rsquo;s a proxy for Bayes error rate) is called <strong>avoidable bias</strong>. It&rsquo;s called avoidable because, in general, we should be able to perform as good as human-level performance.</p>
<p>To cement this idea, and how the comparison can affect the direction of your project, let&rsquo;s go over a simple example. Imagine that we have some classification task with the following performance numbers:</p>
<ul>
<li>Human-level performance: $1\%$, Training Error: $8\%$, Dev error: $10\%$
<ul>
<li>Because there is still about $8\% - 1\% = 7\%$ avoidable bias, we should focus on reducing bias. Train a larger model, train for longer, reduce regularization, etc.</li>
</ul>
</li>
<li>Human-level performance: $7.5\%$, Training Error: $8\%$, Dev error: $10\%$
<ul>
<li>Because the avoidable bias is smaller than the difference between the dev error and the training error, we should focus more on reducing variance. That is the difference between the dev error and the training error.</li>
</ul>
</li>
</ul>
<p>So how come we reached different conclusions on two classifiers that have the same performance? The only thing that is different is the human-level performance. Because we are thinking of human-level performance as a proxy for Bayes error rate, then if the human-level performance changes, then everything changes. If we don&rsquo;t know human performance, we must make some educated guess about it.</p>
<p>The key thing here is to look at the difference between training error and human-level error, which we call <em>avoidable bias</em>. On the other hand, the difference between the dev error and the training error is a measure of <em>variance</em>. We should tackle whichever is greater first.</p>
<h3 id="understanding-human-level-performance">
  Understanding Human-level Performance
  <a class="heading-link" href="#understanding-human-level-performance">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>When talking about human-level performance, we usually mean the <em>best</em> performing individual human or group of humans. We don&rsquo;t take whether it takes a village, we just want to know what&rsquo;s the best that the human species can do. We care about this for two reasons:</p>
<ol>
<li>It will inform us about how much we should expect our training error to be.</li>
<li>It will give us an upper bound of the Bayes error rate.</li>
</ol>
<p>If you have a bias problem (you haven&rsquo;t reached human-level performance yet) focus on this first. If we are relatively close to human-level performance but have a variance problem, focus on that instead.</p>
<h3 id="surpassing-human-level-performance">
  Surpassing Human-level Performance
  <a class="heading-link" href="#surpassing-human-level-performance">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>We have been talking about human-level performance for a while now, and you might be thinking: haven&rsquo;t some models beaten humans? Yes, but only in certain problems. Of course the set of problems that have been solved with better-than-human performance is changing every day.</p>
<p>Within our context, the thing to keep in mind is that once our model is doing better than human-level performance, we might be very close to Bayes error rate; therefore any improvements in the training error might be very costly, and might actually make our model overfit.</p>
<h3 id="improving-your-model-performance">
  Improving your Model Performance
  <a class="heading-link" href="#improving-your-model-performance">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Okay, so we introduced all these topics above; now how do we put them together?</p>
<p>When doing supervised learning, there are two fundamental assumptions, and how good those assumptions are being met will tell us what do next:</p>
<ol>
<li>We can fit the training set pretty well:
<ul>
<li>This means that we have eliminated all or almost all avoidable bias.</li>
</ul>
</li>
<li>The training set performance generalizes pretty well to the dev/test set.
<ul>
<li>This means that we have found a good amount of variance in our model.</li>
</ul>
</li>
</ol>
<p>So if we still have an avoidable bias problem we can try the following:</p>
<ul>
<li>Training a bigger model</li>
<li>Train longer and/or with better optimization algorithms</li>
<li>Try a specialized NN architecture and/or improve hyperparameters</li>
</ul>
<p>If we solved the bias problem, but we still have a variance problem we can try:</p>
<ul>
<li>Try getting more data</li>
<li>Add more regularization</li>
<li>Try a specialized NN architecture and/or improve hyperparameters</li>
</ul>
<p>Next week&rsquo;s post is <a href="http://localhost:1313/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/" >here</a>.</p>

      </div>


      <footer>
        


        <div id="disqus_thread"></div>
<script>
  window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "https-manmartgarc-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
    
    document.addEventListener('themeChanged', function (e) { 
        if (document.readyState == 'complete') {
          DISQUS.reset({ reload: true, config: disqus_config });
        }
    });
</script>
        
        
        
        
        
        
      </footer>
    </article>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"
    integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"
    integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body,
      {
        delimiters: [
          {left: '$$', right: '$$', display:true},
          {left: '$', right: '$', display:false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ]
      }
    );"></script>
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2023 -
    
    2025
     Manuel Martinez 
    ·
    
      Licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA-4.0</a>
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.js"></script>
  

  

  


  

  

  

  

  

  

  

  

  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NJLM0F56ZC"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-NJLM0F56ZC');
</script>


  

  

  

  

  

  

  

  
</body>
</html>
