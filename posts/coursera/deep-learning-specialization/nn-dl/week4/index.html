<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Neural Networks and Deep Learning: Week 4 | Deep Neural Networks | Manuel Martinez</title><meta name=keywords content="machine learning,deep learning"><meta name=description content="Final week of this course. Again, this week is pretty technical and a lot of the learning is done while coding up your own examples via the weekly programming assignments. The purpose of this week is to extend previous weeks&rsquo; ideas into $L$-layered networks.
This week&rsquo;s topics are:

Deep L-Layer neural network
Getting your matrix dimensions right
Why deep representations?
Parameters and Hyperparameters


Deep L-Layer neural network
The number of hidden layers in a neural network determine whether it is &ldquo;shallow&rdquo; or &ldquo;deep&rdquo;. Exactly how many layers is deep or shallow is not set in stone."><meta name=author content="Manuel Martinez"><link rel=canonical href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://manmartgarc.github.io/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://manmartgarc.github.io/images/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://manmartgarc.github.io/images/favicon.ico><link rel=apple-touch-icon href=https://manmartgarc.github.io/apple-touch-icon.png><link rel=mask-icon href=https://manmartgarc.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/"><meta property="og:site_name" content="Manuel Martinez"><meta property="og:title" content="Neural Networks and Deep Learning: Week 4 | Deep Neural Networks"><meta property="og:description" content="Final week of this course. Again, this week is pretty technical and a lot of the learning is done while coding up your own examples via the weekly programming assignments. The purpose of this week is to extend previous weeks’ ideas into $L$-layered networks.
This week’s topics are:
Deep L-Layer neural network Getting your matrix dimensions right Why deep representations? Parameters and Hyperparameters Deep L-Layer neural network The number of hidden layers in a neural network determine whether it is “shallow” or “deep”. Exactly how many layers is deep or shallow is not set in stone."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-16T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-16T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Neural Networks and Deep Learning: Week 4 | Deep Neural Networks"><meta name=twitter:description content="Final week of this course. Again, this week is pretty technical and a lot of the learning is done while coding up your own examples via the weekly programming assignments. The purpose of this week is to extend previous weeks&rsquo; ideas into $L$-layered networks.
This week&rsquo;s topics are:

Deep L-Layer neural network
Getting your matrix dimensions right
Why deep representations?
Parameters and Hyperparameters


Deep L-Layer neural network
The number of hidden layers in a neural network determine whether it is &ldquo;shallow&rdquo; or &ldquo;deep&rdquo;. Exactly how many layers is deep or shallow is not set in stone."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://manmartgarc.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Neural Networks and Deep Learning: Week 4 | Deep Neural Networks","item":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Neural Networks and Deep Learning: Week 4 | Deep Neural Networks","name":"Neural Networks and Deep Learning: Week 4 | Deep Neural Networks","description":"Final week of this course. Again, this week is pretty technical and a lot of the learning is done while coding up your own examples via the weekly programming assignments. The purpose of this week is to extend previous weeks\u0026rsquo; ideas into $L$-layered networks.\nThis week\u0026rsquo;s topics are:\nDeep L-Layer neural network Getting your matrix dimensions right Why deep representations? Parameters and Hyperparameters Deep L-Layer neural network The number of hidden layers in a neural network determine whether it is \u0026ldquo;shallow\u0026rdquo; or \u0026ldquo;deep\u0026rdquo;. Exactly how many layers is deep or shallow is not set in stone.\n","keywords":["machine learning","deep learning"],"articleBody":"Final week of this course. Again, this week is pretty technical and a lot of the learning is done while coding up your own examples via the weekly programming assignments. The purpose of this week is to extend previous weeks’ ideas into $L$-layered networks.\nThis week’s topics are:\nDeep L-Layer neural network Getting your matrix dimensions right Why deep representations? Parameters and Hyperparameters Deep L-Layer neural network The number of hidden layers in a neural network determine whether it is “shallow” or “deep”. Exactly how many layers is deep or shallow is not set in stone.\nMore notation is introduced to have an explicit way of communicating two things:\nThe number of layers The “height” of each layer, i.e. how many hidden units are in each layer. Therefore, the following notation is introduced:\n$L$ is the number of layers in your network. This includes the output layer, but it does not include the input layer (i.e. your features). The simple reason is that the input layer is usually layer $0$, so that your output layer is layer $L$. $n^{[l]}$ denotes the number of hidden units or nodes in layer $l$. This is the “height” of your hidden layer. $a^{[l]}$ is the corresponding activation for layer $l$. Remember that layers can have different activation functions so that $A^{[l]} = g^{[l]}(Z^{[l]})$, where $g^{[l]}$ is the activation function used in layer $l$. A key thing to remember is that for any layer, you calculate $A^{[l]}$ as: $$ \\begin{equation} Z^{[l]} = g^{[l]}(W^{[l]}A^{[l - 1]} + b^{[l]}) \\end{equation} $$\nRemembering that the input layer is usually denoted $A^{[0]} = X$.\nThis is pretty much all there is to deeper networks. The caveat however, is that this applies to fully-connected layers or feed-forward neural networks. In the fourth course, we will go over different architectures which try to represent information in more novel ways.\nGetting your matrix dimensions right If you’re implementing this from scratch, making sure that your dimensions are right is usually the first step in debugging. In other architectures such as CNNs it’s more involved, and therefore it becomes more important to keep track of the dimensions, at least in your head. Here are the key things to keep in mind:\nThe dimensions of arrays in vector land:\n$W^{[l]} = (n^{[l]}, n^{[l - 1]})$ $b^{[l]} = (n^{[l]}, 1)$ $z^{[l]} = a^{[l]} = (n^{[l]}, 1)$ In a single equation:\n$$ \\begin{equation} \\underset{(n^{[l]}, 1)}{z^{[l]}} = \\underset{(n^{[l]}, n^{[l -1]})}{W^{[l]}}\\underset{(n^{[l - 1]}, 1)}{a^{[l - 1]}} + \\underset{(n^{[l]}, 1)}{b^{[l]}} \\end{equation} $$\nNow, in matrix land:\n$W^{[l]} = (n^{[l]}, n^{[l - 1]})$ (remains the same) $b^{[l]} = (n^{[l]}, 1)$ (remains the same) $Z^{[l]} = A^{[l]} = (n^{[l]}, m)$, where $m$ is the number of training samples. Again, in a single equation:\n$$ \\begin{equation} \\underset{(n^{[l]}, m)}{Z^{[l]}} = \\underset{(n^{[l]}, n^{[l - 1]})}{W^{[l]}}\\underset{(n^{[l - 1]}, m)}{A^{[l-1]}} + \\underset{(n^{[l]}, 1)}{b^{[l]}} \\end{equation} $$\nNotice that adding $b^{[l]}$ to the product $W^{[l]}A^{[l-1]}$ is done via broadcasting with NumPy!\nA note about taking derivatives is that the derivatives should be the same dimensions as the arrays.\nWhy deep representations? We have been mentioning how deep neural networks amount to automatic feature generation, and how this is what really set deep neural networks apart from other contemporaneous models. Therefore, a key thing is that they do not just need to be “big”, but they need to be deep, i.e. have hidden layers. How many? Keep reading.\nThe basic idea is that as you run the features through your network, they are combined into more abstract features. The example used in the course is going from raw audio to phonemes, from phonemes to words, from words to sentences; and of course the magic is that you, the programmer, don’t to know what these are, but the machinery figures them out by itself by optimization.\nAn important idea mentioned in the course is that of the relationship between depth and height, that is the number of hidden units. First think of neural networks as trying to learn a function $f(x)$ that maps a vector $x \\mapsto y$. Now, there are some functions that you can estimate using “small” $L$-layer neural networks. However, if you want to use a shallower network and keep the same level of performance in the estimation, you will need to use exponentially more hidden units.\nMore precisely, if you have $n$ features, and you want to compute the exclusive or (XOR) of all the features, you will need a network which has a depth in the order of $O(\\log n)$, where $n$ is also the number of nodes in your network. On the other hand, if you’re forced to use a shallow network, i.e. logistic regression, you will need $O(2^n)$ nodes. In summary, adding layers to your network is much more computationally efficient than growing the size of a hidden layer. Authors Mhaskar, Liao and Poggio show this at much more detail in their paper.\nParameters and Hyperparameters The parameters $W^{[l]}, b^{[l]}$ are the things you derive via training. On the other hand hyperparameters are fixed during training and must be set before training.\nA good way to think about the difference between parameters and hyperparameters comes from Tamara Broderick’s slides. In machine learning we usually want to evaluate a hypothesis $h(x)$ that maps $x \\mapsto y$. A hypothesis can belong to a hypothesis class. A hypothesis class $H$ used by a learning algorithm is the set of all classifiers considered by it. 1 For example a linear classifier considers all classifiers whose decision boundary is linear. During training the algorithm will search within the hypothesis class for a particular hypothesis that minimizes the cost. Hyperparameters are related to the hypothesis class, while parameters are related to a particular hypothesis.\nHyperparameters are usually selected via hyperparameter tuning, which is the topic of the next course.\nSome hyperparameters in neural networks are:\nLearning rate $\\alpha$: the step-size of gradient descent. Number of iterations: how many times will the model observe the entire training set. Number of hidden layers. Size of hidden layers. Which activation functions and on which layers. As you can probably see, there are a lot of hyperparameters to search over, especially considering the size of the set of all combinations.\nThe first week of the next course’s post can be found here.\nHypothesis Class | Carnegie Mellon University ↩︎\n","wordCount":"1045","inLanguage":"en","datePublished":"2023-06-16T00:00:00Z","dateModified":"2023-06-16T00:00:00Z","author":{"@type":"Person","name":"Manuel Martinez"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/"},"publisher":{"@type":"Organization","name":"Manuel Martinez","logo":{"@type":"ImageObject","url":"https://manmartgarc.github.io/images/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://manmartgarc.github.io/ accesskey=h title="Manuel Martinez (Alt + H)">Manuel Martinez</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://manmartgarc.github.io/about/ title=About><span>About</span></a></li><li><a href=https://manmartgarc.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://manmartgarc.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://manmartgarc.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://manmartgarc.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://manmartgarc.github.io/cv/ title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://manmartgarc.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://manmartgarc.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Neural Networks and Deep Learning: Week 4 | Deep Neural Networks</h1><div class=post-meta><span title='2023-06-16 00:00:00 +0000 UTC'>June 16, 2023</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>Manuel Martinez</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#deep-l-layer-neural-network>Deep L-Layer neural network</a></li><li><a href=#getting-your-matrix-dimensions-right>Getting your matrix dimensions right</a></li><li><a href=#why-deep-representations>Why deep representations?</a></li><li><a href=#parameters-and-hyperparameters>Parameters and Hyperparameters</a></li></ul></nav></div></details></div><div class=post-content><p>Final week of this course. Again, this week is pretty technical and a lot of the learning is done while coding up your own examples via the weekly programming assignments. The purpose of this week is to extend previous weeks&rsquo; ideas into $L$-layered networks.</p><p>This week&rsquo;s topics are:</p><ul><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week4/#deep-l-layer-neural-network>Deep L-Layer neural network</a></li><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week4/#getting-your-matrix-dimensions-right>Getting your matrix dimensions right</a></li><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week4/#why-deep-representations>Why deep representations?</a></li><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week4/#parameters-and-hyperparameters>Parameters and Hyperparameters</a></li></ul><hr><h2 id=deep-l-layer-neural-network>Deep L-Layer neural network<a hidden class=anchor aria-hidden=true href=#deep-l-layer-neural-network>#</a></h2><p>The number of hidden layers in a neural network determine whether it is &ldquo;shallow&rdquo; or &ldquo;deep&rdquo;. Exactly how many layers is deep or shallow is not set in stone.</p><p>More notation is introduced to have an explicit way of communicating two things:</p><ol><li>The number of layers</li><li>The &ldquo;height&rdquo; of each layer, i.e. how many hidden units are in each layer.</li></ol><p>Therefore, the following notation is introduced:</p><ul><li>$L$ is the number of layers in your network. This includes the output layer, but it does not include the input layer (i.e. your features). The simple reason is that the input layer is usually layer $0$, so that your output layer is layer $L$.</li><li>$n^{[l]}$ denotes the number of hidden units or nodes in layer $l$. This is the &ldquo;height&rdquo; of your hidden layer.</li><li>$a^{[l]}$ is the corresponding activation for layer $l$. Remember that layers can have different activation functions so that $A^{[l]} = g^{[l]}(Z^{[l]})$, where $g^{[l]}$ is the activation function used in layer $l$.</li></ul><p>A key thing to remember is that for any layer, you calculate $A^{[l]}$ as:
$$
\begin{equation}
Z^{[l]} = g^{[l]}(W^{[l]}A^{[l - 1]} + b^{[l]})
\end{equation}
$$</p><p>Remembering that the input layer is usually denoted $A^{[0]} = X$.</p><p>This is pretty much all there is to deeper networks. The caveat however, is that this applies to fully-connected layers or feed-forward neural networks. In the fourth course, we will go over different architectures which try to represent information in more novel ways.</p><h2 id=getting-your-matrix-dimensions-right>Getting your matrix dimensions right<a hidden class=anchor aria-hidden=true href=#getting-your-matrix-dimensions-right>#</a></h2><p>If you&rsquo;re implementing this from scratch, making sure that your dimensions are right is usually the first step in debugging. In other architectures such as CNNs it&rsquo;s more involved, and therefore it becomes more important to keep track of the dimensions, at least in your head. Here are the key things to keep in mind:</p><p>The dimensions of arrays in vector land:</p><ul><li>$W^{[l]} = (n^{[l]}, n^{[l - 1]})$</li><li>$b^{[l]} = (n^{[l]}, 1)$</li><li>$z^{[l]} = a^{[l]} = (n^{[l]}, 1)$</li></ul><p>In a single equation:</p><p>$$
\begin{equation}
\underset{(n^{[l]}, 1)}{z^{[l]}} = \underset{(n^{[l]}, n^{[l -1]})}{W^{[l]}}\underset{(n^{[l - 1]}, 1)}{a^{[l - 1]}} + \underset{(n^{[l]}, 1)}{b^{[l]}}
\end{equation}
$$</p><p>Now, in matrix land:</p><ul><li>$W^{[l]} = (n^{[l]}, n^{[l - 1]})$ (remains the same)</li><li>$b^{[l]} = (n^{[l]}, 1)$ (remains the same)</li><li>$Z^{[l]} = A^{[l]} = (n^{[l]}, m)$, where $m$ is the number of training samples.</li></ul><p>Again, in a single equation:</p><p>$$
\begin{equation}
\underset{(n^{[l]}, m)}{Z^{[l]}} = \underset{(n^{[l]}, n^{[l - 1]})}{W^{[l]}}\underset{(n^{[l - 1]}, m)}{A^{[l-1]}} + \underset{(n^{[l]}, 1)}{b^{[l]}}
\end{equation}
$$</p><p>Notice that adding $b^{[l]}$ to the product $W^{[l]}A^{[l-1]}$ is done via broadcasting with NumPy!</p><blockquote><p>A note about taking derivatives is that the derivatives should be the same dimensions as the arrays.</p></blockquote><h2 id=why-deep-representations>Why deep representations?<a hidden class=anchor aria-hidden=true href=#why-deep-representations>#</a></h2><p>We have been mentioning how deep neural networks amount to automatic feature generation, and how this is what really set deep neural networks apart from other contemporaneous models. Therefore, a key thing is that they do not just need to be &ldquo;big&rdquo;, but they need to be deep, i.e. have hidden layers. How many? Keep reading.</p><p>The basic idea is that as you run the features through your network, they are combined into more abstract features. The example used in the course is going from raw audio to phonemes, from phonemes to words, from words to sentences; and of course the magic is that you, the programmer, don&rsquo;t to know what these are, but the machinery figures them out by itself by optimization.</p><p>An important idea mentioned in the course is that of the relationship between depth and height, that is the number of hidden units. First think of neural networks as trying to learn a function $f(x)$ that maps a vector $x \mapsto y$. Now, there are some functions that you can estimate using &ldquo;small&rdquo; $L$-layer neural networks. However, if you want to use a shallower network and keep the same level of performance in the estimation, you will need to use <em>exponentially</em> more hidden units.</p><p>More precisely, if you have $n$ features, and you want to compute the <a href=https://en.wikipedia.org/wiki/Exclusive_or>exclusive or (XOR)</a> of all the features, you will need a network which has a <em>depth</em> in the order of $O(\log n)$, where $n$ is also the number of nodes in your network. On the other hand, if you&rsquo;re forced to use a shallow network, i.e. logistic regression, you will need $O(2^n)$ nodes. In summary, adding layers to your network is much more computationally efficient than growing the size of a hidden layer. Authors Mhaskar, Liao and Poggio show this at much more detail in their <a href=https://arxiv.org/pdf/1603.00988.pdf>paper</a>.</p><h2 id=parameters-and-hyperparameters>Parameters and Hyperparameters<a hidden class=anchor aria-hidden=true href=#parameters-and-hyperparameters>#</a></h2><p>The parameters $W^{[l]}, b^{[l]}$ are the things you derive via training. On the other hand hyperparameters are fixed during training and must be set before training.</p><p>A good way to think about the difference between parameters and hyperparameters comes from <a href=https://tamarabroderick.com/files/ml_6036_2020_lectures/broderick_lecture_06.pdf>Tamara Broderick</a>&rsquo;s slides. In machine learning we usually want to evaluate a hypothesis $h(x)$ that maps $x \mapsto y$. A hypothesis can belong to a hypothesis class. A hypothesis class $H$ used by a learning algorithm is the set of all classifiers considered by it. <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> For example a linear classifier considers all classifiers whose decision boundary is linear. During training the algorithm will search <em>within</em> the hypothesis class for a <em>particular</em> hypothesis that minimizes the cost. Hyperparameters are related to the hypothesis class, while parameters are related to a particular hypothesis.</p><p>Hyperparameters are usually selected via hyperparameter tuning, which is the topic of the next course.</p><p>Some hyperparameters in neural networks are:</p><ul><li>Learning rate $\alpha$: the step-size of gradient descent.</li><li>Number of iterations: how many times will the model observe the entire training set.</li><li>Number of hidden layers.</li><li>Size of hidden layers.</li><li>Which activation functions and on which layers.</li></ul><p>As you can probably see, there are a lot of hyperparameters to search over, especially considering the size of the set of all combinations.</p><p>The first week of the next course&rsquo;s post can be found <a href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/>here</a>.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://www.cs.cmu.edu/~epxing/Class/10701-11f/recitation/recitation_4_Bin.pdf>Hypothesis Class | Carnegie Mellon University</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://manmartgarc.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=https://manmartgarc.github.io/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=prev href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/><span class=title>« Prev</span><br><span>Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks</span>
</a><a class=next href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/><span class=title>Next »</span><br><span>Improving Deep Learning Networks: Week 1 | Practical Aspects of Deep Learning</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://manmartgarc.github.io/>Manuel Martinez</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>