<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Neural Networks and Deep Learning: Week 2 | Neural Network Basics | Manuel Martinez</title><meta name=keywords content="machine learning,deep learning"><meta name=description content="Here we kick off the second week of the first course in the specialization. This week is very technical, and many of the details shown in the course will be lost in the summarization. Also, a lot of the content is based on you doing programming assignments. There is simply no substitute for getting your hands dirty.
This week&rsquo;s topics are:

Binary Classification
Logistic Regression
Logistic Function
Gradient Descent
Computation Graph
Python and Vectorization
Broadcasting


Binary Classification
Binary classification is a supervised learning approach where you train what&rsquo;s called a classifier. The binary classifier is a model that learns how to discriminate between two classes from the features, think about cats and dogs. A key concept is that of linearly separability:"><meta name=author content="Manuel Martinez"><link rel=canonical href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week2/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://manmartgarc.github.io/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://manmartgarc.github.io/images/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://manmartgarc.github.io/images/favicon.ico><link rel=apple-touch-icon href=https://manmartgarc.github.io/apple-touch-icon.png><link rel=mask-icon href=https://manmartgarc.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week2/"><meta property="og:site_name" content="Manuel Martinez"><meta property="og:title" content="Neural Networks and Deep Learning: Week 2 | Neural Network Basics"><meta property="og:description" content="Here we kick off the second week of the first course in the specialization. This week is very technical, and many of the details shown in the course will be lost in the summarization. Also, a lot of the content is based on you doing programming assignments. There is simply no substitute for getting your hands dirty.
This week’s topics are:
Binary Classification Logistic Regression Logistic Function Gradient Descent Computation Graph Python and Vectorization Broadcasting Binary Classification Binary classification is a supervised learning approach where you train what’s called a classifier. The binary classifier is a model that learns how to discriminate between two classes from the features, think about cats and dogs. A key concept is that of linearly separability:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-14T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-14T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Neural Networks and Deep Learning: Week 2 | Neural Network Basics"><meta name=twitter:description content="Here we kick off the second week of the first course in the specialization. This week is very technical, and many of the details shown in the course will be lost in the summarization. Also, a lot of the content is based on you doing programming assignments. There is simply no substitute for getting your hands dirty.
This week&rsquo;s topics are:

Binary Classification
Logistic Regression
Logistic Function
Gradient Descent
Computation Graph
Python and Vectorization
Broadcasting


Binary Classification
Binary classification is a supervised learning approach where you train what&rsquo;s called a classifier. The binary classifier is a model that learns how to discriminate between two classes from the features, think about cats and dogs. A key concept is that of linearly separability:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://manmartgarc.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Neural Networks and Deep Learning: Week 2 | Neural Network Basics","item":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Neural Networks and Deep Learning: Week 2 | Neural Network Basics","name":"Neural Networks and Deep Learning: Week 2 | Neural Network Basics","description":"Here we kick off the second week of the first course in the specialization. This week is very technical, and many of the details shown in the course will be lost in the summarization. Also, a lot of the content is based on you doing programming assignments. There is simply no substitute for getting your hands dirty.\nThis week\u0026rsquo;s topics are:\nBinary Classification Logistic Regression Logistic Function Gradient Descent Computation Graph Python and Vectorization Broadcasting Binary Classification Binary classification is a supervised learning approach where you train what\u0026rsquo;s called a classifier. The binary classifier is a model that learns how to discriminate between two classes from the features, think about cats and dogs. A key concept is that of linearly separability:\n","keywords":["machine learning","deep learning"],"articleBody":"Here we kick off the second week of the first course in the specialization. This week is very technical, and many of the details shown in the course will be lost in the summarization. Also, a lot of the content is based on you doing programming assignments. There is simply no substitute for getting your hands dirty.\nThis week’s topics are:\nBinary Classification Logistic Regression Logistic Function Gradient Descent Computation Graph Python and Vectorization Broadcasting Binary Classification Binary classification is a supervised learning approach where you train what’s called a classifier. The binary classifier is a model that learns how to discriminate between two classes from the features, think about cats and dogs. A key concept is that of linearly separability:\nThe existence of a line separating the two types of points means that the data is linearly separable\nThis is a property of your training data, some datasets are linearly separable and others are not. You can a have a linear binary classifier that works perfectly if the data is linearly separable, and this is what a perceptron can do, the building blocks of neural nets. However, if you imagine a dataset where there is no line that separates the classes then you need a non-linear classifier, which is extremely common when you work with data in the wild. There are two main ways of adding non-linearities to a linear classifier: use a kernel classifier, or you can add a single hidden layer to a perceptron, making it into a neural network. One of the best demonstrations of this principle is in creating a classifier that works on the XOR problem. It turns out that the latter is much more versatile.\nLogistic Regression A basic perceptron can be a linear classifier, but by itself it cannot tell you how uncertain it is about a particular prediction. If dogs are labeled as $0$ and cats as $1$, then you don’t want ones and zeros, but actually a probability that a training sample is a cat (or a dog depending on the positive class). We can define what we want our prediction to be:\n$$ \\begin{equation} \\hat{y} = P(y = 1 \\mid x) \\end{equation} $$\nWhere $\\hat{y}$ is the probability that a picture of a dog or cat $x$ is actually a cat, $y=1$, given a particular picture $x$. We will never know the exact probability $y$ (a parameter), so we denote our estimate with a hat $\\hat{y}$ (a statistic). Logistic regression achieves this by composing two things:\nLinear regression Logistic function Linear regression is a way of fitting a line (or plane, or something as you go up higher dimensions) to some data. The line (or plane, etc.) will be a line that minimizes some measurement of error, usually the euclidean distance between a point, and it’s prediction. You can describe a line with two parameters, it’s intercept $b$, and it’s slope $w$. Linear regression finds $\\hat{b}$ and $\\hat{w}$ such that they minimize some loss or error. You can describe linear regression as a linear combination of your features and the parameters:\n$$ \\begin{equation} y_i = \\mathbf{x}_i^Tw + b \\end{equation} $$\nIt turns out that under some assumptions, linear regression has a closed form solution, which simply means that you can take out a pen and do algebra and solve for a set of linear equations using least squares. The next step will undo our ability to have a closed-form solution. If you’re into economics you might now this and much more by heart, but to the extent of this course, it’s enough to know that we have a way of fitting a line (linear classifier) to our data, and that it has a closed-form solution.\nLogistic Function We now have a way of fitting a line to our data. However, that line’s domain, it’s range of values, ranges over all the real numbers $\\mathbb{R}$. Our linear regression could output any number, but we want probabilities. It turns out that there is a nice way to map the real numbers $\\mathbb{R}$, or the interval $[-\\infty, \\infty]$ into the interval $(0, 1)$. You can do this with a type of sigmoid function, called the standard logistic function:\n$$ \\begin{equation} \\sigma(x) = f(x) = \\frac{1}{1+e^{-x}} \\end{equation} $$\nStandard Logistic Function\nIf $x$ is large, then $\\sigma(x) \\approx 1$, if $x$ is small, then $\\sigma(x) \\approx 0$, and everything in between in a continuous fashion. This is how we get our probabilities. Remember that any number $x^0 = 1$ so when $x = 0$ then $\\sigma(0) = 0.5$\nLogistic regression combines the two in a very literal way:\n$$ \\begin{equation} P(y = 1 \\mid x) = \\hat{y} = \\sigma(\\mathbf{x}_i^Tw + b) \\end{equation} $$\nWe literally just pass our linear regression estimate through a sigmoid function. It turns out that we no longer have access to a nice closed-form solution. This means that we will need to find the parameters $w, b$ via numerical optimization. Now, how do we know this amounts to estimating the probabilities? There is a lot of work done on this, under the name of maximum-likelihood estimation (MLE). For now assume that what you get out are estimates of the probabilities.\nLet’s pack our model’s parameters $w, b$ into a single vector $\\theta$, so that $\\theta_0 = b, \\theta_1 = w_0$, etc. Now we can have many parameters, one for each feature in our input, so that feature $i$ maps to $\\theta_{i+1}$ since $b$ is an additive constant. Now our logistic regression looks like this:\n$$ \\begin{equation} P(y = 1 \\mid x) = \\hat{y} = \\sigma(\\theta^TX) \\end{equation} $$\nWhere $X$ is a matrix representing our data, each sample in a row, and each feature as a column.\nGradient Descent To recap, we have combined a linear regression with a sigmoid function with the purpose of getting an estimate of the probability that an image is a cat. We know that there is no closed-solution to the equation, and that we need to find the parameters via optimization. Gradient descent does this for us. There is a ton of amazing gradient descent content online, so I will skip a lot of details.\nIn a nutshell, gradient descent is what fuels the “learning”. This is how the model is able to estimate the parameters from the training data, in our case $\\hat{\\theta}$. It starts with $\\hat{\\theta}$ chosen at random (more on this later), and then it runs a training sample through our logistic regression function. Then it compares the predicted value with the actual value, $(\\hat{y_i}, y_i)$ using a loss function, $\\mathcal{L}(\\hat{y_i}, y_i)$. The loss function tells us how bad a single prediction is. We get to be very creative with our loss functions, although many loss functions are well established for certain applications. On top of this, we have a cost function $\\mathcal{J}(\\theta)$, which computes the loss for our entire training set using a specific set of parameters.\nFinally, gradient descent does its magic. Gradient descent will differentiate $J(\\theta) = \\sum_{i = 1}^m\\frac{1}{m}\\mathcal{L}(\\hat{y_i}, y_i)$, our cost function averaged across our training set, with respect to $\\hat{\\theta}$, our current estimate of $\\theta$ (some random numbers currently). Remember that differentiation describes the rate of change between some function parameter and its output. In this case, we want to know how perturbing or changing $\\hat{\\theta}$, our parameters, changes $J(\\theta)$, how bad our prediction is. If $\\theta$ is a knob in our machine, we want to know which way to turn the knob to make our predictions better. This is what gradient descent does for us. It figures out in which direction our predictions become better with respect to $\\theta$, and then it takes a step (moves the knob) in the direction that minimizes our loss (how we want the machine to work). Then it repeats this steps many times, called iterations, until we converge to a minimum. The basic version of gradient descent uses the entire dataset for each iteration, and it calculates a cost function $J(\\theta)$ which is the loss of our predictions averaged across the entire dataset.\nThe name gradient descent comes from the fact that a gradient is simply the vector version of a derivative. Whereas the derivative is usually a scalar (single number), the gradient $\\nabla f(\\mathbf{x})$ is a vector describing how $f$ changes with respect to every element $x_i$ of $\\mathbf{x}$. Since at every iteration we are “taking a step” towards a lower (better) cost, it’s a descent.\nAll of this is done via basic calculus, using derivative rules. In the case of logistic regression, this is relatively easy to do. But in the case of neural networks, where we stack these logistic regression on top of each other, the calculus gets very messy.\nComputation Graph Since the calculus gets very messy, we might as well get a machine to do it for us. We describe this process to a machine via a computational graph; this is simply an ordering of the mathematical operations in our neural network. This particular ordering can be differentiated step by step using the basic rules of differentiation. This is nice because as our neural networks grow deeper, the computational graph also grows bigger and so more computation is required. The process described just now is called automatic differentiation, and it’s the core of modern deep learning software like PyTorch or TensorFlow.\nWith a computation graph and a way to calculate the derivatives, we can unleash gradient descent on our data. The basic steps are:\nForward propagation: Calculate the output of our model with the current $\\hat{\\theta}$ Backward propagation: Calculate the gradient of our cost function $J(\\theta)$ with respect to our current estimates $\\hat{\\theta}$ and then update $\\hat{\\theta}$ (via calculus) towards the minimum cost by some amount (the learning rate). Rinse and repeat Python and Vectorization This section shows you how to actually code these things from scratch in a relatively efficient manner. I say relatively, because in practice (or production to borrow a fancy term), you will almost never code these things up from scratch. Don’t reinvent the wheel (unless you think the wheel is obsolete). You will most likely implement a neural network using a mature software package such as PyTorch or Tensorflow. There are still key ideas that are important to discuss.\nVectorization or array programming, simply means applying a single operation to many pieces of data, it’s a $1:M$ operation where $M$ is the size of an array. For example, this is a $1 : 1$ operation:\n\u003e\u003e\u003e \"w\".upper() \"W\" A $1:M$ operation:\n\u003e\u003e\u003e \"\".join(map(str.upper, \"Hello, World!\")) \"HELLO, WORLD!\" In Python, strings are sequences of characters, therefore they are iterable. What the code above does is it applies the str.upper method to all the elements of a string.\nThis is technically vectorization, but in general, the term vectorization usually refers to math and linear algebra operations. If we have two vectors $x, y$ both of size $3$, and we want to calculate the dot product between the two, we would have to write a loop:\nx = [1, 2, 3] y = [1, 2, 3] total = 0 for left, right in zip(x, y): total += left * right print(total) 14 Compare this with the NumPy version:\nx = np.array([1, 2, 3]) y = np.array([1, 2, 3]) total = x @ y # @ is the dot operator print(total) 14 You might think, oh well, it’s just syntactic sugar hiding the loop from us. But it’s much more than that. I suggest you time your code using much larger vectors and see how they measure up. Without getting bogged down into the details of how NumPy actually works, this is related to how NumPy uses pre-compiled C libraries to perform arithmetic and linear algebra operations. There’s a conversion cost to go from Python objects to NumPy (which uses C under the hood), but once you converted them, the speed-ups are tremendous. Here is more information about why is NumPy fast.\nThis concept even goes down to the hardware level and is present generally in computer architecture and design. Older machines usually subscribed to a single-instruction single-data (SISD) architecture. Where one instruction is applied to one piece of data in a CPU cycle. However, with single-instruction multiple-data (SIMD), you can apply a single instruction to multiple pieces of data in parallel. In a very general sense, this is why GPUs are so powerful in performing the same task (dot product) to arrays of things (images).\nBroadcasting The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations, so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. There are, however, cases where broadcasting is a bad idea because it leads to inefficient use of memory that slows computation. 1\nI copied and pasted the definition above from the NumPy documentation because I’ve seen the term broadcasting be misunderstood by people. The idea is very basic: if you have a vectorized operation with arrays of different sizes in NumPy, it will “pad” the smaller one to match the bigger one.\nFor example:\nx = np.array([1, 2, 3, 4, 5]) y = np.array([1]) print(x + y) [2, 3, 4, 5, 6] Here we have that $x$ is a $5$ dimensional vector, while $y$ is a $1$ dimensional vector. We used the vectorized sum operator, which should perform an element-wise addition. That is, adding $y_0$ to every element $x_i$. But $y$ is smaller than $x$! So actually the $y$ vector is broadcasted (or padded) over the $x$ vector so that under the hood this is actually happening:\nx = np.array([1, 2, 3, 4, 5]) y = np.array([1, 1, 1, 1, 1]) print(x + y) [2 3 4 5 6] Which brings us back to vectorization. This is the most basic example of broadcasting, and there are some times when the dimensions of the arrays are incompatible, such as matrix multiplication. You can rely on NumPy complaining when this occurs.\nNext week’s post is here.\nBroadcasting ↩︎\n","wordCount":"2356","inLanguage":"en","datePublished":"2023-06-14T00:00:00Z","dateModified":"2023-06-14T00:00:00Z","author":{"@type":"Person","name":"Manuel Martinez"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week2/"},"publisher":{"@type":"Organization","name":"Manuel Martinez","logo":{"@type":"ImageObject","url":"https://manmartgarc.github.io/images/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://manmartgarc.github.io/ accesskey=h title="Manuel Martinez (Alt + H)">Manuel Martinez</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://manmartgarc.github.io/about/ title=About><span>About</span></a></li><li><a href=https://manmartgarc.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://manmartgarc.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://manmartgarc.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://manmartgarc.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://manmartgarc.github.io/cv/ title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://manmartgarc.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://manmartgarc.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Neural Networks and Deep Learning: Week 2 | Neural Network Basics</h1><div class=post-meta><span title='2023-06-14 00:00:00 +0000 UTC'>June 14, 2023</span>&nbsp;·&nbsp;<span>12 min</span>&nbsp;·&nbsp;<span>Manuel Martinez</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#binary-classification>Binary Classification</a></li><li><a href=#logistic-regression>Logistic Regression</a></li><li><a href=#logistic-function>Logistic Function</a></li><li><a href=#gradient-descent>Gradient Descent</a></li><li><a href=#computation-graph>Computation Graph</a></li><li><a href=#python-and-vectorization>Python and Vectorization</a></li><li><a href=#broadcasting>Broadcasting</a></li></ul></nav></div></details></div><div class=post-content><p>Here we kick off the second week of the first course in the specialization. This week is very technical, and many of the details shown in the course will be lost in the summarization. Also, a lot of the content is based on you doing programming assignments. There is simply no substitute for getting your hands dirty.</p><p>This week&rsquo;s topics are:</p><ul><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week2/#binary-classification>Binary Classification</a></li><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week2/#logistic-regression>Logistic Regression</a></li><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week2/#logistic-function>Logistic Function</a></li><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week2/#gradient-descent>Gradient Descent</a></li><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week2/#computation-graph>Computation Graph</a></li><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week2/#python-and-vectorization>Python and Vectorization</a></li><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week2/#broadcasting>Broadcasting</a></li></ul><hr><h2 id=binary-classification>Binary Classification<a hidden class=anchor aria-hidden=true href=#binary-classification>#</a></h2><p>Binary classification is a supervised learning approach where you train what&rsquo;s called a <em>classifier</em>. The binary classifier is a model that learns how to discriminate between two classes from the features, think about cats and dogs. A key concept is that of <a href=https://en.wikipedia.org/wiki/Linear_separability>linearly separability</a>:</p><figure><img loading=lazy src=/images/linear-separable.png alt="The existence of a line separating the two types of points means that the data is linearly separable"><figcaption><p><a href=https://en.wikipedia.org/wiki/Linear_separability>The existence of a line separating the two types of points means that the data is linearly separable</a></p></figcaption></figure><p>This is a property of your training data, some datasets are linearly separable and others are not. You can a have a <em>linear</em> binary classifier that works perfectly if the data is linearly separable, and this is what a perceptron can do, the building blocks of neural nets. However, if you imagine a dataset where there is no line that separates the classes then you need a non-linear classifier, which is extremely common when you work with data in the wild. There are two main ways of adding non-linearities to a linear classifier: use a <a href=https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick>kernel</a> classifier, or you can add a <strong>single</strong> hidden layer to a perceptron, making it into a neural network. One of the best demonstrations of this principle is in creating a classifier that works on the <a href="https://www.youtube.com/watch?v=s7nRWh_3BtA">XOR</a> problem. It turns out that the latter is much more versatile.</p><h2 id=logistic-regression>Logistic Regression<a hidden class=anchor aria-hidden=true href=#logistic-regression>#</a></h2><p>A basic perceptron can be a linear classifier, but by itself it cannot tell you how uncertain it is about a particular prediction. If dogs are labeled as $0$ and cats as $1$, then you don&rsquo;t want ones and zeros, but actually a probability that a training sample is a cat (or a dog depending on the positive class). We can define what we want our prediction to be:</p><p>$$
\begin{equation}
\hat{y} = P(y = 1 \mid x)
\end{equation}
$$</p><p>Where $\hat{y}$ is the probability that a picture of a dog or cat $x$ is <em>actually</em> a cat, $y=1$, <em>given</em> a particular picture $x$. We will never know the exact probability $y$ (a parameter), so we denote our estimate with a hat $\hat{y}$ (a statistic). Logistic regression achieves this by composing two things:</p><ol><li>Linear regression</li><li>Logistic function</li></ol><p>Linear regression is a way of fitting a line (or plane, or something as you go up higher dimensions) to some data. The line (or plane, etc.) will be a line that minimizes some measurement of error, usually the euclidean distance between a point, and it&rsquo;s prediction. You can describe a line with two parameters, it&rsquo;s intercept $b$, and it&rsquo;s slope $w$. Linear regression finds $\hat{b}$ and $\hat{w}$ such that they minimize some loss or error. You can describe linear regression as a <a href=https://en.wikipedia.org/wiki/Linear_combination>linear combination</a> of your features and the parameters:</p><p>$$
\begin{equation}
y_i = \mathbf{x}_i^Tw + b
\end{equation}
$$</p><p>It turns out that under some assumptions, linear regression has a closed form solution, which simply means that you can take out a pen and do algebra and solve for a set of linear equations using <a href=https://en.wikipedia.org/wiki/Least_squares>least squares</a>. The next step will undo our ability to have a closed-form solution. If you&rsquo;re into economics you might now this and much more by heart, but to the extent of this course, it&rsquo;s enough to know that we have a way of fitting a line (linear classifier) to our data, and that it has a closed-form solution.</p><h2 id=logistic-function>Logistic Function<a hidden class=anchor aria-hidden=true href=#logistic-function>#</a></h2><p>We now have a way of fitting a line to our data. However, that line&rsquo;s domain, it&rsquo;s range of values, ranges over all the real numbers $\mathbb{R}$. Our linear regression could output any number, but we want probabilities. It turns out that there is a nice way to map the real numbers $\mathbb{R}$, or the interval $[-\infty, \infty]$ into the interval $(0, 1)$. You can do this with a type of <a href=https://en.wikipedia.org/wiki/Sigmoid_function>sigmoid function</a>, called the standard <a href=https://en.wikipedia.org/wiki/Logistic_function>logistic function</a>:</p><p>$$
\begin{equation}
\sigma(x) = f(x) = \frac{1}{1+e^{-x}}
\end{equation}
$$</p><figure><img loading=lazy src=/images/logistic.png alt="Standard Logistic Function"><figcaption><p><a href=https://en.wikipedia.org/wiki/Logistic_function>Standard Logistic Function</a></p></figcaption></figure><p>If $x$ is large, then $\sigma(x) \approx 1$, if $x$ is small, then $\sigma(x) \approx 0$, and everything in between in a <em>continuous</em> fashion. This is how we get our probabilities. Remember that any number $x^0 = 1$ so when $x = 0$ then $\sigma(0) = 0.5$</p><p>Logistic regression combines the two in a very literal way:</p><p>$$
\begin{equation}
P(y = 1 \mid x) = \hat{y} = \sigma(\mathbf{x}_i^Tw + b)
\end{equation}
$$</p><p>We literally just pass our linear regression estimate through a sigmoid function. It turns out that we no longer have access to a nice closed-form solution. This means that we will need to find the parameters $w, b$ via numerical optimization. Now, how do we know this amounts to estimating the probabilities? There is a lot of work done on this, under the name of <a href=https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf>maximum-likelihood estimation</a> (MLE). For now assume that what you get out are estimates of the probabilities.</p><p>Let&rsquo;s pack our model&rsquo;s parameters $w, b$ into a single vector $\theta$, so that $\theta_0 = b, \theta_1 = w_0$, etc. Now we can have many parameters, one for each feature in our input, so that feature $i$ maps to $\theta_{i+1}$ since $b$ is an additive constant. Now our logistic regression looks like this:</p><p>$$
\begin{equation}
P(y = 1 \mid x) = \hat{y} = \sigma(\theta^TX)
\end{equation}
$$</p><p>Where $X$ is a matrix representing our data, each sample in a row, and each feature as a column.</p><h2 id=gradient-descent>Gradient Descent<a hidden class=anchor aria-hidden=true href=#gradient-descent>#</a></h2><p>To recap, we have combined a linear regression with a sigmoid function with the purpose of getting an estimate of the probability that an image is a cat. We know that there is no closed-solution to the equation, and that we need to find the parameters via optimization. Gradient descent does this for us. There is a ton of amazing gradient descent content online, so I will skip a lot of details.</p><p>In a nutshell, gradient descent is what fuels the &ldquo;learning&rdquo;. This is how the model is able to estimate the parameters from the training data, in our case $\hat{\theta}$. It starts with $\hat{\theta}$ chosen at random (more on this later), and then it runs a training sample through our logistic regression function. Then it compares the predicted value with the actual value, $(\hat{y_i}, y_i)$ using a loss function, $\mathcal{L}(\hat{y_i}, y_i)$. The loss function tells us how bad a <em>single</em> prediction is. We get to be very creative with our loss functions, although many loss functions are well established for certain applications. On top of this, we have a cost function $\mathcal{J}(\theta)$, which computes the loss for our entire training set using a specific set of parameters.</p><p>Finally, gradient descent does its magic. Gradient descent will differentiate $J(\theta) = \sum_{i = 1}^m\frac{1}{m}\mathcal{L}(\hat{y_i}, y_i)$, our cost function averaged across our training set, with respect to $\hat{\theta}$, our current estimate of $\theta$ (some random numbers currently). Remember that differentiation describes the rate of change between some function parameter and its output. In this case, we want to know how perturbing or changing $\hat{\theta}$, our parameters, changes $J(\theta)$, how bad our prediction is. If $\theta$ is a knob in our machine, we want to know which way to turn the knob to make our predictions better. This is what gradient descent does for us. It figures out in which direction our predictions become better with respect to $\theta$, and then it takes a <em>step</em> (moves the knob) in the direction that minimizes our loss (how we want the machine to work). Then it repeats this steps many times, called iterations, until we converge to a minimum. The basic version of gradient descent uses the entire dataset for each iteration, and it calculates a <em>cost</em> function $J(\theta)$ which is the loss of our predictions averaged across the entire dataset.</p><p>The name gradient descent comes from the fact that a gradient is simply the vector version of a derivative. Whereas the derivative is usually a scalar (single number), the gradient $\nabla f(\mathbf{x})$ is a vector describing how $f$ changes with respect to every element $x_i$ of $\mathbf{x}$. Since at every iteration we are &ldquo;taking a step&rdquo; towards a lower (better) cost, it&rsquo;s a descent.</p><p>All of this is done via basic calculus, using derivative rules. In the case of logistic regression, this is relatively easy to do. But in the case of neural networks, where we stack these logistic regression on top of each other, the calculus gets very messy.</p><h2 id=computation-graph>Computation Graph<a hidden class=anchor aria-hidden=true href=#computation-graph>#</a></h2><p>Since the calculus gets very messy, we might as well get a machine to do it for us. We describe this process to a machine via a <a href=https://www.cs.cornell.edu/courses/cs5740/2017sp/lectures/04-nn-compgraph.pdf>computational graph</a>; this is simply an ordering of the mathematical operations in our neural network. This particular ordering can be differentiated step by step using the basic rules of differentiation. This is nice because as our neural networks grow deeper, the computational graph also grows bigger and so more computation is required. The process described just now is called <a href=https://en.wikipedia.org/wiki/Automatic_differentiation>automatic differentiation</a>, and it&rsquo;s the core of modern deep learning software like PyTorch or TensorFlow.</p><p>With a computation graph and a way to calculate the derivatives, we can unleash gradient descent on our data. The basic steps are:</p><ol><li>Forward propagation: Calculate the output of our model with the current $\hat{\theta}$</li><li>Backward propagation: Calculate the gradient of our cost function $J(\theta)$ with respect to our current estimates $\hat{\theta}$ and then update $\hat{\theta}$ (via calculus) towards the minimum cost by some amount (the learning rate).</li><li>Rinse and repeat</li></ol><h2 id=python-and-vectorization>Python and Vectorization<a hidden class=anchor aria-hidden=true href=#python-and-vectorization>#</a></h2><p>This section shows you how to actually code these things from scratch in a relatively efficient manner. I say relatively, because in practice (or production to borrow a fancy term), you will almost never code these things up from scratch. Don&rsquo;t reinvent the wheel (unless you think the wheel is obsolete). You will most likely implement a neural network using a mature software package such as PyTorch or Tensorflow. There are still key ideas that are important to discuss.</p><p><a href=https://en.wikipedia.org/wiki/Array_programming>Vectorization</a> or array programming, simply means applying a <em>single</em> operation to <em>many</em> pieces of data, it&rsquo;s a $1:M$ operation where $M$ is the size of an array. For example, this is a $1 : 1$ operation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=s2>&#34;w&#34;</span><span class=o>.</span><span class=n>upper</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;W&#34;</span>
</span></span></code></pre></div><p>A $1:M$ operation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=s2>&#34;&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=nb>map</span><span class=p>(</span><span class=nb>str</span><span class=o>.</span><span class=n>upper</span><span class=p>,</span> <span class=s2>&#34;Hello, World!&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;HELLO, WORLD!&#34;</span>
</span></span></code></pre></div><p>In Python, strings are <em>sequences</em> of characters, therefore they are iterable. What the code above does is it applies the <code>str.upper</code> method to all the elements of a string.</p><p>This is <em>technically</em> vectorization, but in general, the term vectorization usually refers to math and linear algebra operations. If we have two vectors $x, y$ both of size $3$, and we want to calculate the dot product between the two, we would have to write a loop:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>total</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>left</span><span class=p>,</span> <span class=n>right</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>total</span> <span class=o>+=</span> <span class=n>left</span> <span class=o>*</span> <span class=n>right</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>total</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=mi>14</span>
</span></span></code></pre></div><p>Compare this with the NumPy version:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>total</span> <span class=o>=</span> <span class=n>x</span> <span class=o>@</span> <span class=n>y</span>  <span class=c1># @ is the dot operator</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>total</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=mi>14</span>
</span></span></code></pre></div><p>You might think, oh well, it&rsquo;s just syntactic sugar hiding the loop from us. But it&rsquo;s much more than that. I suggest you time your code using much larger vectors and see how they measure up. Without getting bogged down into the details of how NumPy actually works, this is related to how NumPy uses pre-compiled C libraries to perform arithmetic and linear algebra operations. There&rsquo;s a conversion cost to go from Python objects to NumPy (which uses C under the hood), but once you converted them, the speed-ups are tremendous. Here is more information about <a href=https://numpy.org/doc/stable/user/whatisnumpy.html#why-is-numpy-fast>why is NumPy fast</a>.</p><p>This concept even goes down to the hardware level and is present generally in computer architecture and design. Older machines usually subscribed to a single-instruction single-data (SISD) architecture. Where one instruction is applied to one piece of data in a CPU cycle. However, with <a href=https://en.wikipedia.org/wiki/Single_instruction,_multiple_data>single-instruction multiple-data (SIMD)</a>, you can apply a single instruction to multiple pieces of data in parallel. In a very general sense, this is why GPUs are so powerful in performing the same task (dot product) to arrays of things (images).</p><h2 id=broadcasting>Broadcasting<a hidden class=anchor aria-hidden=true href=#broadcasting>#</a></h2><p>The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations, so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. There are, however, cases where broadcasting is a bad idea because it leads to inefficient use of memory that slows computation. <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p>I copied and pasted the definition above from the NumPy documentation because I&rsquo;ve seen the term broadcasting be misunderstood by people. The idea is very basic: if you have a vectorized operation with arrays of different sizes in NumPy, it will &ldquo;pad&rdquo; the smaller one to match the bigger one.</p><p>For example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>6</span><span class=p>]</span>
</span></span></code></pre></div><p>Here we have that $x$ is a $5$ dimensional vector, while $y$ is a $1$ dimensional vector. We used the vectorized sum operator, which should perform an element-wise addition. That is, adding $y_0$ to every element $x_i$. But $y$ is smaller than $x$! So actually the $y$ vector is <em>broadcasted</em> (or padded) over the $x$ vector so that under the hood this is actually happening:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mi>2</span> <span class=mi>3</span> <span class=mi>4</span> <span class=mi>5</span> <span class=mi>6</span><span class=p>]</span>
</span></span></code></pre></div><p>Which brings us back to vectorization. This is the most basic example of broadcasting, and there are some times when the dimensions of the arrays are incompatible, such as matrix multiplication. You can rely on NumPy complaining when this occurs.</p><p>Next week&rsquo;s post is <a href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/>here</a>.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://numpy.org/doc/stable/user/basics.broadcasting.html#broadcasting>Broadcasting</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://manmartgarc.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=https://manmartgarc.github.io/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=prev href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week1/><span class=title>« Prev</span><br><span>Neural Networks and Deep Learning: Week 1 | Introduction to Deep Learning</span>
</a><a class=next href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/><span class=title>Next »</span><br><span>Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://manmartgarc.github.io/>Manuel Martinez</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>