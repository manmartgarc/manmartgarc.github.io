<!DOCTYPE html>
<html lang="en">
  <head>
    <title>
  Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks · Manuel Martinez
</title>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Manuel Martinez">
<meta name="description" content="This week&rsquo;s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.
This week&rsquo;s topics are:
Overview Neural Network Representation Computing a Neural Network&rsquo;s Output Vectorizing across multiple examples Activation functions Random Initialization Summary OverviewLink to headingIt&rsquo;s time to refine our notation and to disambiguate some concepts introduced in week 2.">
<meta name="keywords" content="blog,developer,personal">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks"/>
<meta name="twitter:description" content="This week&rsquo;s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.
This week&rsquo;s topics are:
Overview Neural Network Representation Computing a Neural Network&rsquo;s Output Vectorizing across multiple examples Activation functions Random Initialization Summary OverviewLink to headingIt&rsquo;s time to refine our notation and to disambiguate some concepts introduced in week 2."/>

<meta property="og:title" content="Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks" />
<meta property="og:description" content="This week&rsquo;s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.
This week&rsquo;s topics are:
Overview Neural Network Representation Computing a Neural Network&rsquo;s Output Vectorizing across multiple examples Activation functions Random Initialization Summary OverviewLink to headingIt&rsquo;s time to refine our notation and to disambiguate some concepts introduced in week 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-15T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-06-15T00:00:00+00:00" />




<link rel="canonical" href="https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/">


<link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.36f76aaf39a14ecf5c3a3c6250dcaf06c238b3d8365d17d646f95cb1874e852b.css" integrity="sha256-NvdqrzmhTs9cOjxiUNyvBsI4s9g2XRfWRvlcsYdOhSs=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.216e36d3eaf6f4cdfd67dc1200c49a8169e6478102977b3e9ac51a064c57054c.css" integrity="sha256-IW420&#43;r29M39Z9wSAMSagWnmR4ECl3s&#43;msUaBkxXBUw=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/png" href="/images/favicon.ico" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon.ico" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">




<meta name="generator" content="Hugo 0.115.4">





  </head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Manuel Martinez
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Posts</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/projects/">Projects</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/cv/">CV</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/">
              Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime="2023-06-15T00:00:00Z">
                June 15, 2023
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              10-minute read
            </span>
          </div>
          
          <div class="categories">
  <i class="fa fa-folder" aria-hidden="true"></i>
    <a href="/categories/coursera/">Coursera</a>
      <span class="separator">•</span>
    <a href="/categories/deep-learning/">Deep Learning</a>
      <span class="separator">•</span>
    <a href="/categories/neural-networks-and-deep-learning/">Neural Networks and Deep Learning</a></div>

          <div class="tags">
  <i class="fa fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/machine-learning/">machine learning</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/deep-learning/">deep learning</a>
    </span></div>

        </div>
      </header>

      <div class="post-content">
        
        <p>This week&rsquo;s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.</p>
<p>This week&rsquo;s topics are:</p>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#neural-network-representation">Neural Network Representation</a></li>
<li><a href="#computing-a-neural-networks-output">Computing a Neural Network&rsquo;s Output</a></li>
<li><a href="#vectorizing-across-multiple-examples">Vectorizing across multiple examples</a></li>
<li><a href="#activation-functions">Activation functions</a></li>
<li><a href="#random-initialization">Random Initialization</a></li>
<li><a href="#summary">Summary</a></li>
</ul>
<hr>
<h2 id="overview">
  Overview
  <a class="heading-link" href="#overview">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>It&rsquo;s time to refine our notation and to disambiguate some concepts introduced in week 2. Let&rsquo;s start with the notation used in the course.</p>
<ul>
<li>Our inputs are represented by a vector $x^{(i)}$ where $i$ represents the $i$th training sample.</li>
<li>The weights for layer $l$ are represented by a matrix $W^{[l]}$</li>
<li>The bias term for layer $l$ is represented by a vector $b^{[l]}$</li>
<li>The linear combination of layers $l$&rsquo;s inputs is $z^{[l]} = W^{[l]}x + b^{[l]}$</li>
<li>Layer $l$&rsquo;s output, after using an activation function (in this case the sigmoid) is $a^{[l]} = \sigma(z^{[l]})$</li>
<li>$a^{[l]}$ is the input to layer $l + 1$ so that $z^{[l + 1]} = W^{[l + 1]}a^{[l]} + b^{[l + 1]}$</li>
<li>At the end, your loss is $\mathcal{L}(a^{[L]}, y)$</li>
</ul>
<h2 id="neural-network-representation">
  Neural Network Representation
  <a class="heading-link" href="#neural-network-representation">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>The input layer corresponds to your training samples $x^{(1)}, x^{(2)}, \dots, x^{(m)}$. You can also think of the input layer as $a^{[0]}$. This means that the first hidden layer is $a^{[1]}$, and in a neural network with a <em>single</em> hidden layer, the output layer would be $a^{[2]}$.</p>
<p>Graduating from logistic regression to neural networks requires us to differentiate which $a$ we are taking about. This is because any hidden layer&rsquo;s inputs, including the output layer, is the output of a previous layer.</p>
<blockquote>
<p>An important comment in the course is that a network with a single hidden layer is usually referred to as a two layer network.</p>
</blockquote>
<h2 id="computing-a-neural-networks-output">
  Computing a Neural Network&rsquo;s Output
  <a class="heading-link" href="#computing-a-neural-networks-output">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>This section goes over how to graduate from vectors into matrices to improve our notation. It might look intimidating, but it&rsquo;s just notation, and it&rsquo;s very important to become comfortable with the dimensions of the layers.</p>
<p>Imagine that you have a <em>single</em> training example $x$ with three features: $x_1, x_2, x_3$. Imagine also that you have a two layer neural network, that is a neural network with a single hidden unit. Finally also imagine that the hidden layer has $4$ hidden units $a^{[1]}_1, a^{[1]}_2, a^{[1]}_3, a^{[1]}_4$. <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<figure><img src="/images/2nn-example.png"
         alt="A two layer neural network"/><figcaption>
            <p><a href="https://www.coursera.support/s/question/0D51U00003BlX8hSAF/proper-citation-of-coursera-lecture?language=en_US">A two layer neural network</a></p>
        </figcaption>
</figure>

<p>Now let&rsquo;s focus on calculating $z^{[1]} = W^{[1]}x + b^{[1]}$. Notice that $W^{[1]}$ is a matrix and this is how it is built: remember that we have four hidden units in our hidden layer. This means that $z^{[1]}$ will have four elements, $z_1^{[1]}, z_2^{[1]}, z_3^{[1]}, z_4^{[1]}$, and this is how each of them is calculated:</p>
<ul>
<li>$z_1^{[1]} = w_1^{[1]T}x + b_1^{[1]}$</li>
<li>$z_2^{[1]} = w_2^{[1]T}x + b_2^{[1]}$</li>
<li>$z_3^{[1]} = w_3^{[1]T}x + b_3^{[1]}$</li>
<li>$z_4^{[1]} = w_4^{[1]T}x + b_4^{[1]}$</li>
</ul>
<p>Notice that $w_i^{[1]T}$ is actually a vector! Its size is the size of the previous layer, your features, so each $w_i^{[1]T}$ has three elements, each of which is multiplied by each of your input features. The main idea is to stack $w_i^{[1]T}$ into a matrix $W^{[1]}$, like this:</p>
<p>$$
\begin{equation}
W^{[1]} = \begin{bmatrix}
&mdash; &amp; w_1^{[1]T} &amp; &mdash; \\
&mdash; &amp; w_2^{[1]T} &amp; &mdash; \\
&mdash; &amp; w_3^{[1]T} &amp; &mdash; \\
&mdash; &amp; w_4^{[1]T} &amp; &mdash; \\
\end{bmatrix}
\end{equation}
$$</p>
<p>Remember that each $w_i^{[1]T}$ was of size 3, so our matrix $W^{[1]}$ is of dimensions $(4, 3)$. Because of how <a href="https://mathinsight.org/matrix_vector_multiplication">matrix-vector multiplication</a> works when you define a vector as a column matrix, now we can do the whole thing in a single step. So that:</p>
<p>$$
\begin{equation}
z^{[1]} = \begin{bmatrix}
&mdash; &amp; w_1^{[1]T} &amp; &mdash; \\
&mdash; &amp; w_2^{[1]T} &amp; &mdash; \\
&mdash; &amp; w_3^{[1]T} &amp; &mdash; \\
&mdash; &amp; w_4^{[1]T} &amp; &mdash; \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}
+
\begin{bmatrix}
b_1^{[1]} \\
b_2^{[1]} \\
b_3^{[1]} \\
b_4^{[1]}
\end{bmatrix} =
\begin{bmatrix}
w_1^{[1]T}x + b_1^{[1]} \\
w_2^{[1]T}x + b_2^{[1]} \\
w_3^{[1]T}x + b_3^{[1]} \\
w_4^{[1]T}x + b_4^{[1]}
\end{bmatrix} =
\begin{bmatrix}
z_1^{[1]} \\
z_2^{[1]} \\
z_3^{[1]} \\
z_4^{[1]}
\end{bmatrix}
\end{equation}
$$</p>
<p>Which you can simply rewrite as:</p>
<p>$$
\begin{equation}
z^{[1]} = W^{[1]}x + b^{[1]}
\end{equation}
$$</p>
<p>Which is a lot better!</p>
<p>Now let&rsquo;s not forget about $a^{[1]} = \sigma(z^{[1]})$. This means that the sigmoid function $\sigma(x)$ is applied <em>element-wise</em> to $z^{[1]}$. So that:</p>
<p>$$
\begin{equation}
a^{[1]} =
\begin{bmatrix}
\sigma(z_1^{[1]}) \\
\sigma(z_2^{[1]}) \\
\sigma(z_3^{[1]}) \\
\sigma(z_4^{[1]}) \\
\end{bmatrix}
\end{equation}
$$</p>
<p>Now let&rsquo;s keep track of the dimensions. Remember that we have $1$ training example with $3$ features and our single hidden layer has $4$ nodes:</p>
<ul>
<li>$\underset{(4, 1)}{z^{[1]}} = \underset{(4, 3)}{W^{[1]}}\underset{(3, 1)}{x} + \underset{(4, 1)}{b^{[1]}}$</li>
<li>$\underset{(4, 1)}{a^{[1]}} = \underset{(4, 1)}{\sigma(z^{[1]})}$</li>
<li>$\underset{(1, 1)}{z^{[2]}} = \underset{(1, 4)}{W^{[2]}}\underset{(4, 1)}{a^{[1]}} + \underset{(1, 1)}{b^{[2]}}$</li>
<li>$\underset{(1, 1)}{a^{[2]}} = \underset{(1, 1)}{z^{[2]}}$</li>
</ul>
<p>Notice that the dimensions of the arrays are below them. Remember that the product $AB$ of two matrices $A, B$ is only defined if the number of <em>columns</em> in $A$ equals the number of <em>rows</em> in $B$. So that you can multiply a $m \times n$ matrix $A$ by a $n \times p$ matrix $B$, and the result $AB$ will be a $m \times p$ matrix. This is exactly why the dimensions have to line up. <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<h2 id="vectorizing-across-multiple-examples">
  Vectorizing across multiple examples
  <a class="heading-link" href="#vectorizing-across-multiple-examples">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Previously, $x$ was a <em>single</em> training sample. It had three features so that $x \in \mathbb{R}^3$ (this means that $x$ belongs to the set of all vectors that live in three-dimensional space).</p>
<p>We could run a for loop for each of our training samples and do the calculation in the previous section for each of the elements, but it turns out that linear algebra is the gift that keeps on givin'.</p>
<p>If we stack our training samples $x_1, \dots x_m$ as <strong>columns</strong> of a matrix $X$ we can make our life infinitely easier. If our data has $n$ feature and $m$ training examples, then:</p>
<p>$$
\begin{equation}
\underset{(n, m)}{X} =
\begin{bmatrix}
\mid &amp; \mid &amp; &amp; \mid \\
x^{(1)} &amp; x^{(2)} &amp; \dots &amp; x^{(m)} \\
\mid &amp; \mid &amp; &amp; \mid \\
\end{bmatrix}
\end{equation}
$$</p>
<blockquote>
<p>Notice that $x_i$ refers to the $i$th feature of a training example, while $x^{(i)}$ refers to the $i$th training example.</p>
</blockquote>
<p>Now we can rewrite our neural net with matrices for $Z^{[l]}, A^{[l]}$:</p>
<ul>
<li>$Z^{[1]} = W^{[1]}X + b^{[1]}$</li>
<li>$A^{[1]} = \sigma(Z^{[1]})$</li>
<li>$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$</li>
<li>$A^{[2]} = \sigma(Z^{[2]})$</li>
</ul>
<p>Similar to how the training examples are stacked in the columns of $X$, each training example is a column of $A^{[l]}, Z^{[l]}$. In the case of $A^{[k]}$, the entry $i, j$ corresponds to the $i$th hidden-unit&rsquo;s activation of the $k$th hidden layer on the $j$th training example.</p>
<p>On a personal note, I think that Andrew&rsquo;s explanation of this is exceptional, and highlights the importance of being able to abstract away from one-training-example scale to neural-network scale.</p>
<h2 id="activation-functions">
  Activation functions
  <a class="heading-link" href="#activation-functions">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>So far we&rsquo;ve picked the sigmoid function, $\sigma(x)$, and slapped it at the end of our linear regression. But why did we pick it? We picked it to add non-linearities, which are important to deal with datasets that are non-linearly separable. Also think that the composition of two linear functions is itself a linear function. Without any non-linearities your network would not be able to learn more &ldquo;interesting&rdquo; (non-linear) features as you go deeper in the layers. It turns out however, that this is not our only choice, and in fact the choice matters in many ways.</p>
<blockquote>
<p><a href="https://tamarabroderick.com/">Tamara Broderick</a> is an Associate Professor at MIT, where she teaches machine learning and statistics. Being an all-around amazing person, she has published an entire course of machine learning on <a href="https://www.youtube.com/watch?v=0xaLT4Svzgo&amp;list=PLxC_ffO4q_rW0bqQB80_vcQB09HOA3ClV">YouTube</a>, and the slides made public on her website. The reason I mention this, besides the fact that she is an amazing teacher, is that her slides are beautiful; geometrically so. In her slides, you can see why how the choice of activation function changes the decision plane in graphical detail. I 100% suggest you check out these <a href="https://tamarabroderick.com/files/ml_6036_2020_lectures/broderick_lecture_06.pdf">slides</a> and marvel at the geometric beauty of neural nets!</p>
</blockquote>
<p>It turns out that there are three widely used (some more than others) activation functions. We denote the activation function of layer $l$ as $g^{[l]}$. The functions are:</p>
<ol>
<li>Hyperbolic Tangent Function: $g^{[l]}(z^{[l]}) = \text{tanh}(z^{[l]}) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$</li>
<li>Standard Logistic Function: $g^{[l]}(z^{[l]}) = \sigma(z^{[l]}) = \frac{1}{1 + e^{-z}}$</li>
<li>Rectified Linear Unit (ReLU): $g^{[l]}(z^{[l]}) = \text{ReLu}(z^{[l]}) = \max(0, z)$</li>
</ol>
<p>Now when do you use which? Only use the $\sigma(z)$ as your output function. If your output is strictly greater than $0$, you could also use a $\text{ReLU}(z)$ here as well, even for regression. Sometimes you don&rsquo;t need a non-linear activation function in your output layer for regression. The $\text{tanh}(z)$ function is strictly superior to $\sigma(z)$ in hidden layers. Most commonly use the $\text{ReLU}(z)$ in hidden layers. Why? Because of differentiation! The derivatives of these functions have to be well-behaved across the domain in order for our fancy gradient descent to not get bogged by numerical precision issues. This is later covered in the course under the name of exploding or vanishing gradients.</p>
<h2 id="random-initialization">
  Random Initialization
  <a class="heading-link" href="#random-initialization">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>How do we initialize our parameters? The choice of how we initialize our $W^{[l]}$ actually matters. There is a property known as <a href="https://cedar.buffalo.edu/~srihari/CSE676/8.4%20ParInitializn.pdf">symmetry</a>. If you choose all your parameters as $0$, then you are not &ldquo;breaking&rdquo; symmetry. What this means is that every iteration of gradient descent will result in the same weights, therefore it will never improve your cost. You might be asking, hey what about $b^{[l]}$? It turns out that $b^{[l]}$ doesn&rsquo;t have the symmetry problem, so you don&rsquo;t need to worry too much about it, and can in fact initialize it to $0$.</p>
<p>Fine, but what do we initialize it to? You can use random numbers (actually not just any random numbers as we will see in the next course), with the only caveat that you need to scale your randomly chosen parameters by a constant, usually $0.01$ or $10^{-2}$. This is done to keep the numbers close to $0$, where the derivatives of your activation functions are better defined than at the extremes. Remember that $\sigma(z^{[l]})$ is basically flat for values $|x| \approx 6$. There are more sophisticated approaches to parameter initialization that will are covered in the next course.</p>
<h2 id="summary">
  Summary
  <a class="heading-link" href="#summary">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>This week also included a lot of technical instruction about how to calculate the derivatives of all the layers and their respective activation functions. There is a lot of amazing content online that you can get the level of detail shown in the course. However, in practice, these details are encapsulated from a software point of view. This means that you, the programmer, don&rsquo;t have to know all the details to use the software.</p>
<ul>
<li>
<p>Linear algebra can help our notation be much better:</p>
<ul>
<li>Writing our layer operations with vectors is much cleaner.</li>
<li>We can go one step further and define our layer operations with matrices, allowing us to vectorize over some set of training samples.</li>
</ul>
</li>
<li>
<p>Activation functions matter:</p>
<ul>
<li>Different activation functions affect the decision boundary plane. Some can make them into step-wise, while others can make them smooth.</li>
<li>Activation functions matter for optimization since we are taking their derivatives when doing gradient descent.</li>
</ul>
</li>
<li>
<p>Parameter initialization:</p>
<ul>
<li>If you initialize your parameters as $0$ you will have a bad day.</li>
<li>Make sure you use randomly initialized parameters, and scale them by some small constant.</li>
<li>This is also related to optimization via vanishing and/or exploding gradients.</li>
</ul>
</li>
</ul>
<p>Next week&rsquo;s post is <a href="https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/">here</a>.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Deep Learning Specialization | Coursera, Andrew Ng&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://mathinsight.org/matrix_vector_multiplication">Multiplying matrices and vectors | Math Insight</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

      </div>


      <footer>
        


        <div id="disqus_thread"></div>
<script>
  window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "https-manmartgarc-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
    
    document.addEventListener('themeChanged', function (e) { 
        if (document.readyState == 'complete') {
          DISQUS.reset({ reload: true, config: disqus_config });
        }
    });
</script>
        
        
      </footer>
    </article>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
    integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
    integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body,
      {
        delimiters: [
          {left: '$$', right: '$$', display:true},
          {left: '$', right: '$', display:false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ]
      }
    );"></script>
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
    2023
     Manuel Martinez 
    ·
    
      Licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA-4.0</a>
    ·
    
    Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.27afce394fb6284f521b3fbc9f6a8326342333c3092267f3944d770489876fed.js" integrity="sha256-J6/OOU&#43;2KE9SGz&#43;8n2qDJjQjM8MJImfzlE13BImHb&#43;0="></script>
  

  

  

  

  

  

  

  

  

  
</body>

</html>
