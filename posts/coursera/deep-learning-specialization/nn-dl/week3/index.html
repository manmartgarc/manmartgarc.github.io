<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks | Manuel Martinez</title><meta name=keywords content="machine learning,deep learning"><meta name=description content="This week&rsquo;s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from a single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.
This week&rsquo;s topics are:

Overview
Neural Network Representation
Computing a Neural Network&rsquo;s Output
Vectorizing across multiple examples
Activation functions
Random Initialization


Overview
It&rsquo;s time to refine our notation and to disambiguate some concepts introduced in week 2. Let&rsquo;s start with the notation used in the course."><meta name=author content="Manuel Martinez"><link rel=canonical href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://manmartgarc.github.io/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://manmartgarc.github.io/images/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://manmartgarc.github.io/images/favicon.ico><link rel=apple-touch-icon href=https://manmartgarc.github.io/apple-touch-icon.png><link rel=mask-icon href=https://manmartgarc.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/"><meta property="og:site_name" content="Manuel Martinez"><meta property="og:title" content="Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks"><meta property="og:description" content="This week’s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from a single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.
This week’s topics are:
Overview Neural Network Representation Computing a Neural Network’s Output Vectorizing across multiple examples Activation functions Random Initialization Overview It’s time to refine our notation and to disambiguate some concepts introduced in week 2. Let’s start with the notation used in the course."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-15T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-15T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks"><meta name=twitter:description content="This week&rsquo;s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from a single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.
This week&rsquo;s topics are:

Overview
Neural Network Representation
Computing a Neural Network&rsquo;s Output
Vectorizing across multiple examples
Activation functions
Random Initialization


Overview
It&rsquo;s time to refine our notation and to disambiguate some concepts introduced in week 2. Let&rsquo;s start with the notation used in the course."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://manmartgarc.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks","item":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks","name":"Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks","description":"This week\u0026rsquo;s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from a single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.\nThis week\u0026rsquo;s topics are:\nOverview Neural Network Representation Computing a Neural Network\u0026rsquo;s Output Vectorizing across multiple examples Activation functions Random Initialization Overview It\u0026rsquo;s time to refine our notation and to disambiguate some concepts introduced in week 2. Let\u0026rsquo;s start with the notation used in the course.\n","keywords":["machine learning","deep learning"],"articleBody":"This week’s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from a single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.\nThis week’s topics are:\nOverview Neural Network Representation Computing a Neural Network’s Output Vectorizing across multiple examples Activation functions Random Initialization Overview It’s time to refine our notation and to disambiguate some concepts introduced in week 2. Let’s start with the notation used in the course.\nOur inputs are represented by a vector $x^{(i)}$ where $i$ represents the $i$th training sample. The weights for layer $l$ are represented by a matrix $W^{[l]}$ The bias term for layer $l$ is represented by a vector $b^{[l]}$ The linear combination of layers $l$’s inputs is $z^{[l]} = W^{[l]}x + b^{[l]}$ Layer $l$’s output, after using an activation function (in this case the sigmoid) is $a^{[l]} = \\sigma(z^{[l]})$ $a^{[l]}$ is the input to layer $l + 1$ so that $z^{[l + 1]} = W^{[l + 1]}a^{[l]} + b^{[l + 1]}$ At the end, your loss is $\\mathcal{L}(a^{[L]}, y)$ Neural Network Representation The input layer corresponds to your training samples $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. You can also think of the input layer as $a^{[0]}$. This means that the first hidden layer is $a^{[1]}$, and in a neural network with a single hidden layer, the output layer would be $a^{[2]}$.\nGraduating from logistic regression to neural networks requires us to differentiate which $a$ we are taking about. This is because any hidden layer’s inputs, including the output layer, is the output of a previous layer.\nAn important comment in the course is that a network with a single hidden layer is usually referred to as a two layer network.\nComputing a Neural Network’s Output This section goes over how to graduate from vectors into matrices to improve our notation. It might look intimidating, but it’s just notation, and it’s very important to become comfortable with the dimensions of the layers.\nImagine that you have a single training example $x$ with three features: $x_1, x_2, x_3$. Imagine also that you have a two layer neural network, that is a neural network with a single hidden unit. Finally also imagine that the hidden layer has $4$ hidden units $a^{[1]}_1, a^{[1]}_2, a^{[1]}_3, a^{[1]}_4$. 1\nA two layer neural network\nNow let’s focus on calculating $z^{[1]} = W^{[1]}x + b^{[1]}$. Notice that $W^{[1]}$ is a matrix and this is how it is built: remember that we have four hidden units in our hidden layer. This means that $z^{[1]}$ will have four elements, $z_1^{[1]}, z_2^{[1]}, z_3^{[1]}, z_4^{[1]}$, and this is how each of them is calculated:\n$z_1^{[1]} = w_1^{[1]T}x + b_1^{[1]}$ $z_2^{[1]} = w_2^{[1]T}x + b_2^{[1]}$ $z_3^{[1]} = w_3^{[1]T}x + b_3^{[1]}$ $z_4^{[1]} = w_4^{[1]T}x + b_4^{[1]}$ Notice that $w_i^{[1]T}$ is actually a vector! Its size is the size of the previous layer, your features, so each $w_i^{[1]T}$ has three elements, each of which is multiplied by each of your input features. The main idea is to stack $w_i^{[1]T}$ into a matrix $W^{[1]}$, like this:\n$$ \\begin{equation} W^{[1]} = \\begin{bmatrix} — \u0026 w_1^{[1]T} \u0026 — \\\\ — \u0026 w_2^{[1]T} \u0026 — \\\\ — \u0026 w_3^{[1]T} \u0026 — \\\\ — \u0026 w_4^{[1]T} \u0026 — \\\\ \\end{bmatrix} \\end{equation} $$\nRemember that each $w_i^{[1]T}$ was of size 3, so our matrix $W^{[1]}$ is of dimensions $(4, 3)$. Because of how matrix-vector multiplication works when you define a vector as a column matrix, now we can do the whole thing in a single step. So that:\n$$ \\begin{equation} z^{[1]} = \\begin{bmatrix} — \u0026 w_1^{[1]T} \u0026 — \\\\ — \u0026 w_2^{[1]T} \u0026 — \\\\ — \u0026 w_3^{[1]T} \u0026 — \\\\ — \u0026 w_4^{[1]T} \u0026 — \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} + \\begin{bmatrix} b_1^{[1]} \\\\ b_2^{[1]} \\\\ b_3^{[1]} \\\\ b_4^{[1]} \\end{bmatrix} = \\begin{bmatrix} w_1^{[1]T}x + b_1^{[1]} \\\\ w_2^{[1]T}x + b_2^{[1]} \\\\ w_3^{[1]T}x + b_3^{[1]} \\\\ w_4^{[1]T}x + b_4^{[1]} \\end{bmatrix} = \\begin{bmatrix} z_1^{[1]} \\\\ z_2^{[1]} \\\\ z_3^{[1]} \\\\ z_4^{[1]} \\end{bmatrix} \\end{equation} $$\nWhich you can simply rewrite as:\n$$ \\begin{equation} z^{[1]} = W^{[1]}x + b^{[1]} \\end{equation} $$\nWhich is a lot better!\nNow let’s not forget about $a^{[1]} = \\sigma(z^{[1]})$. This means that the sigmoid function $\\sigma(x)$ is applied element-wise to $z^{[1]}$. So that:\n$$ \\begin{equation} a^{[1]} = \\begin{bmatrix} \\sigma(z_1^{[1]}) \\\\ \\sigma(z_2^{[1]}) \\\\ \\sigma(z_3^{[1]}) \\\\ \\sigma(z_4^{[1]}) \\\\ \\end{bmatrix} \\end{equation} $$\nNow let’s keep track of the dimensions. Remember that we have $1$ training example with $3$ features and our single hidden layer has $4$ nodes:\n$\\underset{(4, 1)}{z^{[1]}} = \\underset{(4, 3)}{W^{[1]}}\\underset{(3, 1)}{x} + \\underset{(4, 1)}{b^{[1]}}$ $\\underset{(4, 1)}{a^{[1]}} = \\underset{(4, 1)}{\\sigma(z^{[1]})}$ $\\underset{(1, 1)}{z^{[2]}} = \\underset{(1, 4)}{W^{[2]}}\\underset{(4, 1)}{a^{[1]}} + \\underset{(1, 1)}{b^{[2]}}$ $\\underset{(1, 1)}{a^{[2]}} = \\underset{(1, 1)}{z^{[2]}}$ Notice that the dimensions of the arrays are below them. Remember that the product $AB$ of two matrices $A, B$ is only defined if the number of columns in $A$ equals the number of rows in $B$. So that you can multiply a $m \\times n$ matrix $A$ by a $n \\times p$ matrix $B$, and the result $AB$ will be a $m \\times p$ matrix. This is exactly why the dimensions have to line up. 2\nVectorizing across multiple examples Previously, $x$ was a single training sample. It had three features so that $x \\in \\mathbb{R}^3$ (this means that $x$ belongs to the set of all vectors that live in three-dimensional space).\nWe could run a for loop for each of our training samples and do the calculation in the previous section for each of the elements, but it turns out that linear algebra is the gift that keeps on givin'.\nIf we stack our training samples $x_1, \\dots x_m$ as columns of a matrix $X$ we can make our life infinitely easier. If our data has $n$ feature and $m$ training examples, then:\n$$ \\begin{equation} \\underset{(n, m)}{X} = \\begin{bmatrix} \\mid \u0026 \\mid \u0026 \u0026 \\mid \\\\ x^{(1)} \u0026 x^{(2)} \u0026 \\dots \u0026 x^{(m)} \\\\ \\mid \u0026 \\mid \u0026 \u0026 \\mid \\\\ \\end{bmatrix} \\end{equation} $$\nNotice that $x_i$ refers to the $i$th feature of a training example, while $x^{(i)}$ refers to the $i$th training example.\nNow we can rewrite our neural net with matrices for $Z^{[l]}, A^{[l]}$:\n$Z^{[1]} = W^{[1]}X + b^{[1]}$ $A^{[1]} = \\sigma(Z^{[1]})$ $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$ $A^{[2]} = \\sigma(Z^{[2]})$ Similar to how the training examples are stacked in the columns of $X$, each training example is a column of $A^{[l]}, Z^{[l]}$. In the case of $A^{[k]}$, the entry $i, j$ corresponds to the $i$th hidden-unit’s activation of the $k$th hidden layer on the $j$th training example.\nOn a personal note, I think that Andrew’s explanation of this is exceptional, and highlights the importance of being able to abstract away from one-training-example scale to neural-network scale.\nActivation functions So far we’ve picked the sigmoid function, $\\sigma(x)$, and slapped it at the end of our linear regression. But why did we pick it? We picked it to add non-linearities, which are important to deal with datasets that are non-linearly separable. Also think that the composition of two linear functions is itself a linear function. Without any non-linearities your network would not be able to learn more “interesting” (non-linear) features as you go deeper in the layers. It turns out however, that this is not our only choice, and in fact the choice matters in many ways.\nTamara Broderick is an Associate Professor at MIT, where she teaches machine learning and statistics. Being an all-around amazing person, she has published an entire course of machine learning on YouTube, and the slides made public on her website. The reason I mention this, besides the fact that she is an amazing teacher, is that her slides are beautiful; geometrically so. In her slides, you can see why how the choice of activation function changes the decision plane in graphical detail. I 100% suggest you check out these slides and marvel at the geometric beauty of neural nets!\nIt turns out that there are three widely used (some more than others) activation functions. We denote the activation function of layer $l$ as $g^{[l]}$. The functions are:\nHyperbolic Tangent Function: $g^{[l]}(z^{[l]}) = \\text{tanh}(z^{[l]}) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ Standard Logistic Function: $g^{[l]}(z^{[l]}) = \\sigma(z^{[l]}) = \\frac{1}{1 + e^{-z}}$ Rectified Linear Unit (ReLU): $g^{[l]}(z^{[l]}) = \\text{ReLu}(z^{[l]}) = \\max(0, z)$ Now when do you use which? Only use the $\\sigma(z)$ as your output function. If your output is strictly greater than $0$, you could also use a $\\text{ReLU}(z)$ here as well, even for regression. Sometimes you don’t need a non-linear activation function in your output layer for regression. The $\\text{tanh}(z)$ function is strictly superior to $\\sigma(z)$ in hidden layers. Most commonly use the $\\text{ReLU}(z)$ in hidden layers. Why? Because of differentiation! The derivatives of these functions have to be well-behaved across the domain in order for our fancy gradient descent to not get bogged by numerical precision issues. This is later covered in the course under the name of exploding or vanishing gradients.\nRandom Initialization How do we initialize our parameters? The choice of how we initialize our $W^{[l]}$ actually matters. There is a property known as symmetry. If you choose all your parameters as $0$, then you are not “breaking” symmetry. What this means is that every iteration of gradient descent will result in the same weights, therefore it will never improve your cost. You might be asking, hey what about $b^{[l]}$? It turns out that $b^{[l]}$ doesn’t have the symmetry problem, so you don’t need to worry too much about it, and can in fact initialize it to $0$.\nFine, but what do we initialize it to? You can use random numbers (actually not just any random numbers as we will see in the next course), with the only caveat that you need to scale your randomly chosen parameters by a constant, usually $0.01$ or $10^{-2}$. This is done to keep the numbers close to $0$, where the derivatives of your activation functions are better defined than at the extremes. Remember that $\\sigma(z^{[l]})$ is basically flat for values $|x| \\approx 6$. There are more sophisticated approaches to parameter initialization that will are covered in the next course.\nNext week’s post is here.\nDeep Learning Specialization | Coursera, Andrew Ng ↩︎\nMultiplying matrices and vectors | Math Insight ↩︎\n","wordCount":"1712","inLanguage":"en","datePublished":"2023-06-15T00:00:00Z","dateModified":"2023-06-15T00:00:00Z","author":{"@type":"Person","name":"Manuel Martinez"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/"},"publisher":{"@type":"Organization","name":"Manuel Martinez","logo":{"@type":"ImageObject","url":"https://manmartgarc.github.io/images/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://manmartgarc.github.io/ accesskey=h title="Manuel Martinez (Alt + H)">Manuel Martinez</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://manmartgarc.github.io/about/ title=About><span>About</span></a></li><li><a href=https://manmartgarc.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://manmartgarc.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://manmartgarc.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://manmartgarc.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://manmartgarc.github.io/cv/ title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://manmartgarc.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://manmartgarc.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks</h1><div class=post-meta><span title='2023-06-15 00:00:00 +0000 UTC'>June 15, 2023</span>&nbsp;·&nbsp;<span>9 min</span>&nbsp;·&nbsp;<span>Manuel Martinez</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#overview>Overview</a></li><li><a href=#neural-network-representation>Neural Network Representation</a></li><li><a href=#computing-a-neural-networks-output>Computing a Neural Network&rsquo;s Output</a></li><li><a href=#vectorizing-across-multiple-examples>Vectorizing across multiple examples</a></li><li><a href=#activation-functions>Activation functions</a></li><li><a href=#random-initialization>Random Initialization</a></li></ul></nav></div></details></div><div class=post-content><p>This week&rsquo;s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from a single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.</p><p>This week&rsquo;s topics are:</p><ul><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week3/#overview>Overview</a></li><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week3/#neural-network-representation>Neural Network Representation</a></li><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week3/#computing-a-neural-networks-output>Computing a Neural Network&rsquo;s Output</a></li><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week3/#vectorizing-across-multiple-examples>Vectorizing across multiple examples</a></li><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week3/#activation-functions>Activation functions</a></li><li><a href=/posts/coursera/deep-learning-specialization/nn-dl/week3/#random-initialization>Random Initialization</a></li></ul><hr><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p>It&rsquo;s time to refine our notation and to disambiguate some concepts introduced in week 2. Let&rsquo;s start with the notation used in the course.</p><ul><li>Our inputs are represented by a vector $x^{(i)}$ where $i$ represents the $i$th training sample.</li><li>The weights for layer $l$ are represented by a matrix $W^{[l]}$</li><li>The bias term for layer $l$ is represented by a vector $b^{[l]}$</li><li>The linear combination of layers $l$&rsquo;s inputs is $z^{[l]} = W^{[l]}x + b^{[l]}$</li><li>Layer $l$&rsquo;s output, after using an activation function (in this case the sigmoid) is $a^{[l]} = \sigma(z^{[l]})$</li><li>$a^{[l]}$ is the input to layer $l + 1$ so that $z^{[l + 1]} = W^{[l + 1]}a^{[l]} + b^{[l + 1]}$</li><li>At the end, your loss is $\mathcal{L}(a^{[L]}, y)$</li></ul><h2 id=neural-network-representation>Neural Network Representation<a hidden class=anchor aria-hidden=true href=#neural-network-representation>#</a></h2><p>The input layer corresponds to your training samples $x^{(1)}, x^{(2)}, \dots, x^{(m)}$. You can also think of the input layer as $a^{[0]}$. This means that the first hidden layer is $a^{[1]}$, and in a neural network with a <em>single</em> hidden layer, the output layer would be $a^{[2]}$.</p><p>Graduating from logistic regression to neural networks requires us to differentiate which $a$ we are taking about. This is because any hidden layer&rsquo;s inputs, including the output layer, is the output of a previous layer.</p><blockquote><p>An important comment in the course is that a network with a single hidden layer is usually referred to as a two layer network.</p></blockquote><h2 id=computing-a-neural-networks-output>Computing a Neural Network&rsquo;s Output<a hidden class=anchor aria-hidden=true href=#computing-a-neural-networks-output>#</a></h2><p>This section goes over how to graduate from vectors into matrices to improve our notation. It might look intimidating, but it&rsquo;s just notation, and it&rsquo;s very important to become comfortable with the dimensions of the layers.</p><p>Imagine that you have a <em>single</em> training example $x$ with three features: $x_1, x_2, x_3$. Imagine also that you have a two layer neural network, that is a neural network with a single hidden unit. Finally also imagine that the hidden layer has $4$ hidden units $a^{[1]}_1, a^{[1]}_2, a^{[1]}_3, a^{[1]}_4$. <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><figure><img loading=lazy src=/images/2nn-example.png alt="A two layer neural network"><figcaption><p><a href="https://www.coursera.support/s/question/0D51U00003BlX8hSAF/proper-citation-of-coursera-lecture?language=en_US">A two layer neural network</a></p></figcaption></figure><p>Now let&rsquo;s focus on calculating $z^{[1]} = W^{[1]}x + b^{[1]}$. Notice that $W^{[1]}$ is a matrix and this is how it is built: remember that we have four hidden units in our hidden layer. This means that $z^{[1]}$ will have four elements, $z_1^{[1]}, z_2^{[1]}, z_3^{[1]}, z_4^{[1]}$, and this is how each of them is calculated:</p><ul><li>$z_1^{[1]} = w_1^{[1]T}x + b_1^{[1]}$</li><li>$z_2^{[1]} = w_2^{[1]T}x + b_2^{[1]}$</li><li>$z_3^{[1]} = w_3^{[1]T}x + b_3^{[1]}$</li><li>$z_4^{[1]} = w_4^{[1]T}x + b_4^{[1]}$</li></ul><p>Notice that $w_i^{[1]T}$ is actually a vector! Its size is the size of the previous layer, your features, so each $w_i^{[1]T}$ has three elements, each of which is multiplied by each of your input features. The main idea is to stack $w_i^{[1]T}$ into a matrix $W^{[1]}$, like this:</p><p>$$
\begin{equation}
W^{[1]} = \begin{bmatrix}
&mdash; & w_1^{[1]T} & &mdash; \\
&mdash; & w_2^{[1]T} & &mdash; \\
&mdash; & w_3^{[1]T} & &mdash; \\
&mdash; & w_4^{[1]T} & &mdash; \\
\end{bmatrix}
\end{equation}
$$</p><p>Remember that each $w_i^{[1]T}$ was of size 3, so our matrix $W^{[1]}$ is of dimensions $(4, 3)$. Because of how <a href=https://mathinsight.org/matrix_vector_multiplication>matrix-vector multiplication</a> works when you define a vector as a column matrix, now we can do the whole thing in a single step. So that:</p><p>$$
\begin{equation}
z^{[1]} = \begin{bmatrix}
&mdash; & w_1^{[1]T} & &mdash; \\
&mdash; & w_2^{[1]T} & &mdash; \\
&mdash; & w_3^{[1]T} & &mdash; \\
&mdash; & w_4^{[1]T} & &mdash; \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}
+
\begin{bmatrix}
b_1^{[1]} \\
b_2^{[1]} \\
b_3^{[1]} \\
b_4^{[1]}
\end{bmatrix} =
\begin{bmatrix}
w_1^{[1]T}x + b_1^{[1]} \\
w_2^{[1]T}x + b_2^{[1]} \\
w_3^{[1]T}x + b_3^{[1]} \\
w_4^{[1]T}x + b_4^{[1]}
\end{bmatrix} =
\begin{bmatrix}
z_1^{[1]} \\
z_2^{[1]} \\
z_3^{[1]} \\
z_4^{[1]}
\end{bmatrix}
\end{equation}
$$</p><p>Which you can simply rewrite as:</p><p>$$
\begin{equation}
z^{[1]} = W^{[1]}x + b^{[1]}
\end{equation}
$$</p><p>Which is a lot better!</p><p>Now let&rsquo;s not forget about $a^{[1]} = \sigma(z^{[1]})$. This means that the sigmoid function $\sigma(x)$ is applied <em>element-wise</em> to $z^{[1]}$. So that:</p><p>$$
\begin{equation}
a^{[1]} =
\begin{bmatrix}
\sigma(z_1^{[1]}) \\
\sigma(z_2^{[1]}) \\
\sigma(z_3^{[1]}) \\
\sigma(z_4^{[1]}) \\
\end{bmatrix}
\end{equation}
$$</p><p>Now let&rsquo;s keep track of the dimensions. Remember that we have $1$ training example with $3$ features and our single hidden layer has $4$ nodes:</p><ul><li>$\underset{(4, 1)}{z^{[1]}} = \underset{(4, 3)}{W^{[1]}}\underset{(3, 1)}{x} + \underset{(4, 1)}{b^{[1]}}$</li><li>$\underset{(4, 1)}{a^{[1]}} = \underset{(4, 1)}{\sigma(z^{[1]})}$</li><li>$\underset{(1, 1)}{z^{[2]}} = \underset{(1, 4)}{W^{[2]}}\underset{(4, 1)}{a^{[1]}} + \underset{(1, 1)}{b^{[2]}}$</li><li>$\underset{(1, 1)}{a^{[2]}} = \underset{(1, 1)}{z^{[2]}}$</li></ul><p>Notice that the dimensions of the arrays are below them. Remember that the product $AB$ of two matrices $A, B$ is only defined if the number of <em>columns</em> in $A$ equals the number of <em>rows</em> in $B$. So that you can multiply a $m \times n$ matrix $A$ by a $n \times p$ matrix $B$, and the result $AB$ will be a $m \times p$ matrix. This is exactly why the dimensions have to line up. <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><h2 id=vectorizing-across-multiple-examples>Vectorizing across multiple examples<a hidden class=anchor aria-hidden=true href=#vectorizing-across-multiple-examples>#</a></h2><p>Previously, $x$ was a <em>single</em> training sample. It had three features so that $x \in \mathbb{R}^3$ (this means that $x$ belongs to the set of all vectors that live in three-dimensional space).</p><p>We could run a for loop for each of our training samples and do the calculation in the previous section for each of the elements, but it turns out that linear algebra is the gift that keeps on givin'.</p><p>If we stack our training samples $x_1, \dots x_m$ as <strong>columns</strong> of a matrix $X$ we can make our life infinitely easier. If our data has $n$ feature and $m$ training examples, then:</p><p>$$
\begin{equation}
\underset{(n, m)}{X} =
\begin{bmatrix}
\mid & \mid & & \mid \\
x^{(1)} & x^{(2)} & \dots & x^{(m)} \\
\mid & \mid & & \mid \\
\end{bmatrix}
\end{equation}
$$</p><blockquote><p>Notice that $x_i$ refers to the $i$th feature of a training example, while $x^{(i)}$ refers to the $i$th training example.</p></blockquote><p>Now we can rewrite our neural net with matrices for $Z^{[l]}, A^{[l]}$:</p><ul><li>$Z^{[1]} = W^{[1]}X + b^{[1]}$</li><li>$A^{[1]} = \sigma(Z^{[1]})$</li><li>$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$</li><li>$A^{[2]} = \sigma(Z^{[2]})$</li></ul><p>Similar to how the training examples are stacked in the columns of $X$, each training example is a column of $A^{[l]}, Z^{[l]}$. In the case of $A^{[k]}$, the entry $i, j$ corresponds to the $i$th hidden-unit&rsquo;s activation of the $k$th hidden layer on the $j$th training example.</p><p>On a personal note, I think that Andrew&rsquo;s explanation of this is exceptional, and highlights the importance of being able to abstract away from one-training-example scale to neural-network scale.</p><h2 id=activation-functions>Activation functions<a hidden class=anchor aria-hidden=true href=#activation-functions>#</a></h2><p>So far we&rsquo;ve picked the sigmoid function, $\sigma(x)$, and slapped it at the end of our linear regression. But why did we pick it? We picked it to add non-linearities, which are important to deal with datasets that are non-linearly separable. Also think that the composition of two linear functions is itself a linear function. Without any non-linearities your network would not be able to learn more &ldquo;interesting&rdquo; (non-linear) features as you go deeper in the layers. It turns out however, that this is not our only choice, and in fact the choice matters in many ways.</p><blockquote><p><a href=https://tamarabroderick.com/>Tamara Broderick</a> is an Associate Professor at MIT, where she teaches machine learning and statistics. Being an all-around amazing person, she has published an entire course of machine learning on <a href="https://www.youtube.com/watch?v=0xaLT4Svzgo&amp;list=PLxC_ffO4q_rW0bqQB80_vcQB09HOA3ClV">YouTube</a>, and the slides made public on her website. The reason I mention this, besides the fact that she is an amazing teacher, is that her slides are beautiful; geometrically so. In her slides, you can see why how the choice of activation function changes the decision plane in graphical detail. I 100% suggest you check out these <a href=https://tamarabroderick.com/files/ml_6036_2020_lectures/broderick_lecture_06.pdf>slides</a> and marvel at the geometric beauty of neural nets!</p></blockquote><p>It turns out that there are three widely used (some more than others) activation functions. We denote the activation function of layer $l$ as $g^{[l]}$. The functions are:</p><ol><li>Hyperbolic Tangent Function: $g^{[l]}(z^{[l]}) = \text{tanh}(z^{[l]}) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$</li><li>Standard Logistic Function: $g^{[l]}(z^{[l]}) = \sigma(z^{[l]}) = \frac{1}{1 + e^{-z}}$</li><li>Rectified Linear Unit (ReLU): $g^{[l]}(z^{[l]}) = \text{ReLu}(z^{[l]}) = \max(0, z)$</li></ol><p>Now when do you use which? Only use the $\sigma(z)$ as your output function. If your output is strictly greater than $0$, you could also use a $\text{ReLU}(z)$ here as well, even for regression. Sometimes you don&rsquo;t need a non-linear activation function in your output layer for regression. The $\text{tanh}(z)$ function is strictly superior to $\sigma(z)$ in hidden layers. Most commonly use the $\text{ReLU}(z)$ in hidden layers. Why? Because of differentiation! The derivatives of these functions have to be well-behaved across the domain in order for our fancy gradient descent to not get bogged by numerical precision issues. This is later covered in the course under the name of exploding or vanishing gradients.</p><h2 id=random-initialization>Random Initialization<a hidden class=anchor aria-hidden=true href=#random-initialization>#</a></h2><p>How do we initialize our parameters? The choice of how we initialize our $W^{[l]}$ actually matters. There is a property known as <a href=https://cedar.buffalo.edu/~srihari/CSE676/8.4%20ParInitializn.pdf>symmetry</a>. If you choose all your parameters as $0$, then you are not &ldquo;breaking&rdquo; symmetry. What this means is that every iteration of gradient descent will result in the same weights, therefore it will never improve your cost. You might be asking, hey what about $b^{[l]}$? It turns out that $b^{[l]}$ doesn&rsquo;t have the symmetry problem, so you don&rsquo;t need to worry too much about it, and can in fact initialize it to $0$.</p><p>Fine, but what do we initialize it to? You can use random numbers (actually not just any random numbers as we will see in the next course), with the only caveat that you need to scale your randomly chosen parameters by a constant, usually $0.01$ or $10^{-2}$. This is done to keep the numbers close to $0$, where the derivatives of your activation functions are better defined than at the extremes. Remember that $\sigma(z^{[l]})$ is basically flat for values $|x| \approx 6$. There are more sophisticated approaches to parameter initialization that will are covered in the next course.</p><p>Next week&rsquo;s post is <a href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/>here</a>.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Deep Learning Specialization | Coursera, Andrew Ng&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://mathinsight.org/matrix_vector_multiplication>Multiplying matrices and vectors | Math Insight</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://manmartgarc.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=https://manmartgarc.github.io/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=prev href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week2/><span class=title>« Prev</span><br><span>Neural Networks and Deep Learning: Week 2 | Neural Network Basics</span>
</a><a class=next href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/><span class=title>Next »</span><br><span>Neural Networks and Deep Learning: Week 4 | Deep Neural Networks</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://manmartgarc.github.io/>Manuel Martinez</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>