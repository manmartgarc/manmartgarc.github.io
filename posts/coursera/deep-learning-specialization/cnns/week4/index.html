<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Convolutional Neural Networks: Week 4 | Face Recognition & Neural Style Transfer | Manuel Martinez</title><meta name=keywords content="machine learning,deep learning"><meta name=description content="This is the fourth and last week of the fourth course of DeepLearning.AI&rsquo;s Deep Learning Specialization offered on Coursera. In this week, we go over special applications in the field of computer vision with CNNs: face recognition and neural style transfer. This week introduces new important concepts that will be useful even beyond the context of CNNs.
This week&rsquo;s topics are as follows:

Face Recognition

What is Face Recognition?
One Shot Learning
Siamese Network
Triplet Loss
Face Verification and Binary Classification


Neural Style Transfer

What is Neural Style Transfer?
What are deep CNNs learning?
Cost Function

Content Cost Function
Style Cost Function






Face Recognition
What is Face Recognition?
Let&rsquo;s start by going over the important distinction between face verification and face recognition."><meta name=author content="Manuel Martinez"><link rel=canonical href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week4/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://manmartgarc.github.io/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://manmartgarc.github.io/images/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://manmartgarc.github.io/images/favicon.ico><link rel=apple-touch-icon href=https://manmartgarc.github.io/apple-touch-icon.png><link rel=mask-icon href=https://manmartgarc.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week4/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week4/"><meta property="og:site_name" content="Manuel Martinez"><meta property="og:title" content="Convolutional Neural Networks: Week 4 | Face Recognition & Neural Style Transfer"><meta property="og:description" content="This is the fourth and last week of the fourth course of DeepLearning.AI’s Deep Learning Specialization offered on Coursera. In this week, we go over special applications in the field of computer vision with CNNs: face recognition and neural style transfer. This week introduces new important concepts that will be useful even beyond the context of CNNs.
This week’s topics are as follows:
Face Recognition What is Face Recognition? One Shot Learning Siamese Network Triplet Loss Face Verification and Binary Classification Neural Style Transfer What is Neural Style Transfer? What are deep CNNs learning? Cost Function Content Cost Function Style Cost Function Face Recognition What is Face Recognition? Let’s start by going over the important distinction between face verification and face recognition."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-02T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-02T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Convolutional Neural Networks: Week 4 | Face Recognition & Neural Style Transfer"><meta name=twitter:description content="This is the fourth and last week of the fourth course of DeepLearning.AI&rsquo;s Deep Learning Specialization offered on Coursera. In this week, we go over special applications in the field of computer vision with CNNs: face recognition and neural style transfer. This week introduces new important concepts that will be useful even beyond the context of CNNs.
This week&rsquo;s topics are as follows:

Face Recognition

What is Face Recognition?
One Shot Learning
Siamese Network
Triplet Loss
Face Verification and Binary Classification


Neural Style Transfer

What is Neural Style Transfer?
What are deep CNNs learning?
Cost Function

Content Cost Function
Style Cost Function






Face Recognition
What is Face Recognition?
Let&rsquo;s start by going over the important distinction between face verification and face recognition."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://manmartgarc.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Convolutional Neural Networks: Week 4 | Face Recognition \u0026 Neural Style Transfer","item":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week4/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Convolutional Neural Networks: Week 4 | Face Recognition \u0026 Neural Style Transfer","name":"Convolutional Neural Networks: Week 4 | Face Recognition \u0026 Neural Style Transfer","description":"This is the fourth and last week of the fourth course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. In this week, we go over special applications in the field of computer vision with CNNs: face recognition and neural style transfer. This week introduces new important concepts that will be useful even beyond the context of CNNs.\nThis week\u0026rsquo;s topics are as follows:\nFace Recognition What is Face Recognition? One Shot Learning Siamese Network Triplet Loss Face Verification and Binary Classification Neural Style Transfer What is Neural Style Transfer? What are deep CNNs learning? Cost Function Content Cost Function Style Cost Function Face Recognition What is Face Recognition? Let\u0026rsquo;s start by going over the important distinction between face verification and face recognition.\n","keywords":["machine learning","deep learning"],"articleBody":"This is the fourth and last week of the fourth course of DeepLearning.AI’s Deep Learning Specialization offered on Coursera. In this week, we go over special applications in the field of computer vision with CNNs: face recognition and neural style transfer. This week introduces new important concepts that will be useful even beyond the context of CNNs.\nThis week’s topics are as follows:\nFace Recognition What is Face Recognition? One Shot Learning Siamese Network Triplet Loss Face Verification and Binary Classification Neural Style Transfer What is Neural Style Transfer? What are deep CNNs learning? Cost Function Content Cost Function Style Cost Function Face Recognition What is Face Recognition? Let’s start by going over the important distinction between face verification and face recognition.\nIn the case of face verification, we get a pair of things: an input image, and also a name or ID. The output of such a system is a binary choice that says whether the image corresponds to the name or ID, or whether it does not correspond. On the other hand, in the case of face recognition, we only get a single thing: an image. The output of such a system is whether the input image matches to any $K$ identities already stored in our system’s database.\nThis means that face verification is a $1 \\to 1$ procedure, while face recognition is a $1 \\to K$ procedure. The latter is whether we find a match in or not, and this usually means doing $K$ comparisons in the worst case.\nSo far it doesn’t sound too bad, but here’s the real issue. Imagine that you’re implementing a face recognition system at your company’s building. How many images can you get of each person? Ideally you’d like to get as many as possible, but that’s unfeasible; even worse, in practice, we usually only have access to one or at most two pictures of each person. Having only one training example for each “class” is what defines one-shot learning; having a few is usually called few-shot learning.\nOne Shot Learning So imagine that we go around our company and finally get a face picture of everyone that should be allowed to get into the building. We want to train a model so that when someone walks into the door, and is one of the people allowed, that the model recognizes that this person is allowed from only one picture. Obviously, we also want to keep away unwanted people!\nThe most immediate idea is to train a classifier with a CNN architecture. If we have $K$ employees, then the output layer of our CNN will have $K + 1$ hidden units in the softmax, the extra one for when the input doesn’t match anyone. The issue is that this CNN will have terrible performance because of the size of our training data—remember, we only have $K$ pictures. Even worse, what happens when we hire someone else? We would have to retrain the whole network every time. There has to be a better way.\nThe better way is to move away from classification and to think of a way to learn a similarity function. A similarity function is a function that takes in a pair of elements and outputs their similarity. You might have heard of Jaccard similarity or cosine similarity. In this case, we want to learn a function that takes two images:\n$$ \\text{d}(\\text{img}_1, \\text{img}_2) = \\text{degree of difference between the images} $$\nThen we can set some threshold $\\tau$ and binarize our similarity function:\n$$ \\text{Verification}(\\text{img}_1, \\text{img}_2) = \\begin{cases} \\text{Same} \u0026 \\text{if} \\ \\text{d}(\\text{img}_1, \\text{img}_2) \\leq \\tau \\\\ \\text{Different} \u0026 \\text{otherwise} \\end{cases} $$\nHopefully, when our colleague Alice walks in the door of the building, the picture taken from the device at the entrance will have the lowest difference when compared to Alice’s picture in our database, amongst all other employees. Also, someone who is not our colleague, should not have a similarity less than $\\tau$ amongst anybody else in our employee set. We will see how these kinks are ironed out when we get to triplet loss. Let’s first see how we can learn such a function as $\\text{d}(\\text{img}_1, \\text{img}_2)$.\nSiamese Network The idea behind Siamese networks is pretty straightforward: we run a pair of images, $(\\text{img}_1, \\text{img}_2)$, through the same CNN with a fully connected layer at the end, and then use these outputs to compare the images. Let’s dig deeper into what this means.\nRemember that CNNs usually reduce the spatial dimensions of the input volume while increasing the channel dimension. Imagine that we have a CNN that takes an image of a face with dimensions $100 \\times 100 \\times 3$ and after reducing this volume through some number of convolutional and max pooling layers, we get a $128$ dimensional vector. This is the same as in a vanilla CNN, but right before we run the fully connected layer through a softmax layer for classification.\nLet’s call our input image $x^{(1)}$. Let’s call running $x^{(1)}$ through our CNN, and transforming the $100 \\times 100 \\times 3$ input into a $128$ element vector $f(x^{(1)})$, so that $f(x^{(1)}) \\in \\mathbb{R}^{128}$. This $128$ dimensional vector is an encoding of the input; it simply some particular representation of the original image containing a face.\nIf we let $x^{(1)}$ be the image of our employee in the database, and $x^{(2)}$ be the image of the face of the person who just walked in, you might imagine what we want to do. We’ll run $x^{(2)}$ through our CNN and encode it in the same way, so that we get $f(x^{(2)})$. Notice that $f(x^{(2)}) \\in \\mathbb{R}^{128}$ as well, and that the encoding was generated with the same CNN as $f(x^{(1)})$.\nNow we can redefine our similarity function: $\\text{d}(\\text{img}_1, \\text{img}_2)$ in terms of these numerical encodings, so that our new similarity function is:\n$$ d(x^{(1)}, x^{(2)}) = ||f(x^{(1)}) - f(x^{(2)})||_2^2 $$\nThis is our old friend, the L2-norm of the difference between the two encodings.\nYou might be thinking, why not compare the two images with the L2-norm directly? Think about what happens when you compare two images of the same person but with different lighting, hairstyle, makeup, etc. It won’t work well.\nBut how do we train such a network? Remember we want a CNN that takes as input an image, and generates a $128$ dimensional encoding $f(x^{(i)})$. But of course, not just any encoding! We want that $128$ dimensional encoding to have certain properties:\nIf $x^{(i)}, x^{(j)}$ are the same person, we want $||f(x^{(1)}) - f(x^{(2)})||_2^2$ to be small. If $x^{(i)}, x^{(j)}$ are different persons, we want $||f(x^{(1)}) - f(x^{(2)})||_2^2$ to be large. It turns out that we can use back-propagation to learn such parameters, as long as we can define a loss function that has these properties, i.e. it generates good encodings for comparing images of faces. This is how triplet loss was defined.\nTriplet Loss Triplet loss is called so because it uses three elements: an anchor, a positive and a negative example. The anchor is a picture of one of our employees. The positive example is another picture of the same employee. The negative example is simply a picture of someone else. That is, we distance between the anchor and the positive example should be low, while the distance between the anchor and the negative example should be high. We will use the letters $A, P, N$ to refer to the anchor, positive and negative examples respectively.\nRemember the two properties we wanted out of our encodings, we wanted that the L2-norm of the difference between two encodings to be small if they are of the same person and large if they are not. At least we want the distance to be larger between the anchor and the negative than the distance between the anchor and the positive example. In math, we want:\n$$ \\begin{aligned} ||f(A) - f(P)||_2^2 \u0026\\leq ||f(A) - f(N)||_2^2 \\\\ ||f(A) - f(P)||_2^2 - ||f(A) - f(N)||_2^2 \u0026\\leq 0 \\end{aligned} $$\nThere’s an issue with this approach. There is a trivial solution where we set everything to $0$. To prevent our network from learning this solution, we add a margin; similar to the one used in support vector machines. The margin, called $\\alpha$ can be a hyperparameter, so that we have the following:\n$$ ||f(A) - f(P)||_2^2 - ||f(A) - f(N)||_2^2 + \\alpha \\leq 0 $$\nSetting $\\alpha$ allows us to specify how much bigger the difference between the anchor and the negative compared to the anchor and the positive examples should be.\nWe are ready to define our loss function, given three images $A, P, N$:\n$$ \\mathcal{L}(A, P, N) = \\max(||f(A) - f(P)||_2^2 - ||f(A) - f(N)||_2^2 + \\alpha, 0) $$\nWe use the $\\max$ operator here because as long as we have made the difference between the L2-norm of the differences less than $0$ (plus the margin), we have done “good”. Otherwise, we have done well, and the difference is above zero.\nWe can also now define our cost function, over some $m$ training samples:\n$$ J = \\sum_{i=1}^m \\mathcal{L}(A^{(i)}, P^{(i)}, N^{(i)}) $$\nNotice that to form the triplets, we need at least two pictures of the same person! For example, we could have a training set of $10,000$ images of $1,000$ people, some of them repeated more than once. You might be wondering, sure, but how do we make the triplets? It turns out that this is very important.\nIf we choose $A, P, N$ triplets at random, then the constraints are easily satisfied; but our network will not perform well. We need to choose triplets that are “hard” to train on, i.e. $\\text{d}(A, P) \\approx \\text{d}(A, N)$. By doing this, we are forcing the network to deal with the harder cases, where people look similar but are not the same person. The details on how to build triplets are described in the FaceNet paper by Schroff, et al.\nFace Verification and Binary Classification It turns out that the triplet loss approach is not the only way to build a face recognition system. We could also use the result of the two CNNs, i.e., the $128$-dimensional embeddings of the pictures, and then feed these to a logistic regression unit and perform binary classification on them; estimating whether the two embeddings are the same or not. The final logistic layer would look like this:\n$$ \\hat{y} = \\sigma \\left( \\sum_{k=1}^{128} w_k ||f(x^{(i)})_{k} - f(x^{(j)})_k||_2 + b \\right) $$\nWhere we are still using the embeddings, but we are calculating an element-wise squared difference, each multiplied by its own weight.\nIn practice, and in both approaches, we can speed up the latency of the system by precomputing the encodings of our employees, and only computing the encoding of the person we are trying to recognize on the fly.\nNeural Style Transfer What is Neural Style Transfer? You might still remember back when neural style transfer was the latest, hottest thing in machine learning. Perhaps even more so, the pointed questions it raised about intellectual property rights. Today, many companies use stable diffusion, a text-to-image system, as an interface for performing neural style transfer.\nNeural style transfer, in a nutshell, is being able to imbue an image with another style. For example, we might have a picture of our cats, but we’d like to make that picture look like it was painted by Rembrandt. Neural style transfer allows us to “transfer” the style of a Rembrandt painting into a picture of our cat, so that it looks like Rembrandt himself painted our furry friend.\nWe will be using the notation $C$ to refer to the content image, in our case our cat. The letter $S$ will represent the style image, in our case a Rembrandt painting. Finally, we will use $G$ to refer to the generated image, that is our cat in the style of Rembrandt.\nWhat are deep CNNs learning? Before getting into neural style transfer, we must understand, at a high level, how the input changes through the layers in a CNN. There is an amazing paper by Zeiler and Fergus, 2013 in which they come up with novel ways to visualize what visual features the filters are learning on each of the layers.\nThe gist of it is that the filters in the shallower (earlier) layers of the network learn to pick apart basic features in our image—think vertical lines, diagonal lines, etc. As we progress down the layers, deeper into the CNN, the filters start to learn more abstract features, such as concentric circles, colors, and lines, etc. Even later on, we see that some filters specialize in certain regions of the face: noses, eyes, etc. So on and so forth.\nThis is important to keep in mind, especially in the context of neural style transfer, because we can choose to give a unique weight to each layer in the combination of content and style.\nCost Function Similar to all other applications of supervised learning, we need to establish a cost function that will guide the optimization process. Let’s get started.\nWe have our content image $C$ and our style image $S$, and we’d like to generate an image $G$ which is some mixture of both $C$ and $S$. We will define our loss function in terms of these elements:\n$$ J(G) = \\alpha J_{content}(C, G) + \\beta J_{style}(S, G) $$\nLet’s unpack the formula. Our total cost is a function of two separate costs. The first one is the content cost, that is, how bad our generated image is relative to the original content image, which in our example is a picture of a cat. The second one is the style cost, how bad our generated image is relative to the original style image, which in our example is a Rembrandt painting. We have two hyperparameters $\\alpha$ and $\\beta$ which allow us to adjust the mixture between the two costs.\nBut how do we get $G$? We start by initializing it randomly. In practice, we add some random noise to the original content image $C$, but imagine that we start $G$ completely at random; that is, a random noise picture of dimensions $100 \\times 100 \\times 3$. Then we can use gradient descent to update our random picture $G$:\n$$ G := G - \\frac{\\partial}{\\partial G} J(G) $$\nWe are not updating any parameters! We are directly updating the pixel values of our generated image $G$ at each step of gradient descent, or whatever garden variety optimization algorithm we choose to use. Let’s now break down each of the components of the cost function.\nContent Cost Function Remember, our cost function was:\n$$ J(G) = \\alpha J_{content}(C, G) + \\beta J_{style}(S, G) $$\nWe will focus on the $J_{content}(C, G)$ component.\nSay that we are using a pre-trained CNN, such as VGG-19, and that we focus on some layer $l$. Keeping in mind what happens at each layer of a CNN, we will hedge our bets and pick some $l$ that’s in the middle—not too deep and not too shallow. We will focus on the activations for this layer, $a^{[l]}$.\nWe will run both images, $C$ and $G$, through $a^{[l]}$ and generate $a^{l}, a^{l}$. The idea is that if we compare $a^{l}, a^{l}$, and they are similar, then the images have similar content. This means that the filters are picking up similar activations on the features that they specialize in for that particular layer.\nThis comparison is done by our old friend, the L2-norm of the difference:\n$$ J^{[l]}_{content}(C, G) = \\frac{1}{4 n_H^{[l]} n_W^{[l]} n_C^{[l]}}\\sum_{\\text{all entries}}||a^{[l](C)} - a^{[l](G)}||_2^2 $$\nThe normalization factor was chosen by the authors and takes into account the dimensions of $a^{[l]}$.\nThis will make our optimization algorithm set pixel values on $G$ that minimize this difference, so that the generated image has similar content to the content image.\nStyle Cost Function Calculating the content cost is so far not that crazy; doing element-wise squared differences is a reasonable approach for the content. But how can we quantify “style” and compare our generated image $G$ to our style image $S$ to see how far off they are from each other? Here is where things get fascinating.\nThe authors define “style” as the pair-wise correlation between the activations across channels. Since each channel learns a different feature, style is defined as the interaction between the features. More specifically, their covariance. If one feature picks up stark lines, and another one picks up colors, then if both of these features have positive or negative covariance, then we can say that this is one of the elements of style. When we do a pair-wise comparison between all channels, we get a pair-wise matrix, which we will call $G^{[l]}$, the style matrix. This matrix will tell us how each of the channels covary with each other, and this, in a sense, is the essence of style.\nWe want to compare the style matrix for both $S$, our style image, and $G$, our generated image, and repeat the comparison we did for the content image, but this time on the style matrix or Gram matrix of both $G$ and $S$. Both style matrices $G^{l}$ and $G^{l}$ will be of the same dimensions: $n_C^{[l]} \\times n_C^{[l]}$.\nLet’s start with the style matrix of $S$. We define $a^{l}_{i,j,k}$ as one entry in the activations for layer $l$ using $S$ as the input. We will construct $G^{l}$, the Gram matrix of the activations. The entry $kk’$ in the $G^{l}$ is defined as:\n$$ G^{[l](S)}_{kk’} = \\sum_{i=1}^{n_H} \\sum_{j=1}^{n_W} a^{[l](S)}_{i,j,k} a^{[l](S)}_{i,j,k’} $$\nWe will repeat the same, but for $G$, our generated image:\n$$ G^{[l](G)}_{kk’} = \\sum_{i=1}^{n_H} \\sum_{j=1}^{n_W} a^{[l](G)}_{i,j,k} a^{[l](G)}_{i,j,k’} $$\nLet’s recap:\nWe have two images, $S$ and $G$. We defined the “style” of an image as the gram matrix of the volume. That is, the element-wise channel covariance. We calculated the gram or style matrix from the activations of layer $l$ for both $G$ and $S$. That is we ran both $G$ and $S$ through the same layer and got some output. It is from this output that we calculate the style matrix for $G$ and $S$ separately. Now, we compare the two in the same fashion we did for the content cost. The style cost is defined as:\n$$ J^{[l]}_{style}(S, G) = \\frac{1}{\\left(2 n_H^{[l]} n_W^{[l]} n_C^{[l]} \\right)^2} \\sum_{k=1}^{n_C^{[l]}} \\sum_{k’=1}^{n_C^{[l]}} \\left( G_{kk’}^{[l](S)} - G_{kk’}^{[l](G)}\\right)^2 $$\nWhere again, the normalization factor in front was set by the authors.\nA final thing to notice about the style function: notice that it’s indexed by $l$; we calculate the style function at every layer $l$. The authors define the cost function at the network level as:\n$$ J_{style}(S, G) = \\sum_{l=1}^L \\lambda^{[l]} J^{[l]}_{style}(S, G) $$\nUsing the $\\lambda^{[l]}$ parameter for each layer allows us to mix the more basic features in the shallower layers or the more abstract features in the deeper layers. If we want the generated image to softly follow the style, we choose larger weights for the deeper layers and smaller for the shallower layers. On the other hand, if we want our generated image to strongly follow the style image, we do the opposite: select smaller weights for the deeper layers but larger weights for the shallower layers.\nNow we can come back to the cost function we defined earlier:\n$$ J(G) = \\alpha J_{content}(C, G) + \\beta J_{style}(S, G) $$\nThat’s it, this is the last week of the CNN course. The next course is sequential models, which, to me, are much more interesting in terms of applications. The programming exercises for this week were fantastic as well, and I highly suggest that you do them as well.\nNext week’s post is here, and it’s the first week in the sequence models course.\n","wordCount":"3279","inLanguage":"en","datePublished":"2023-08-02T00:00:00Z","dateModified":"2023-08-02T00:00:00Z","author":{"@type":"Person","name":"Manuel Martinez"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week4/"},"publisher":{"@type":"Organization","name":"Manuel Martinez","logo":{"@type":"ImageObject","url":"https://manmartgarc.github.io/images/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://manmartgarc.github.io/ accesskey=h title="Manuel Martinez (Alt + H)">Manuel Martinez</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://manmartgarc.github.io/about/ title=About><span>About</span></a></li><li><a href=https://manmartgarc.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://manmartgarc.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://manmartgarc.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://manmartgarc.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://manmartgarc.github.io/cv/ title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://manmartgarc.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://manmartgarc.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Convolutional Neural Networks: Week 4 | Face Recognition & Neural Style Transfer</h1><div class=post-meta><span title='2023-08-02 00:00:00 +0000 UTC'>August 2, 2023</span>&nbsp;·&nbsp;<span>16 min</span>&nbsp;·&nbsp;<span>Manuel Martinez</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#face-recognition>Face Recognition</a><ul><li><a href=#what-is-face-recognition>What is Face Recognition?</a></li><li><a href=#one-shot-learning>One Shot Learning</a></li><li><a href=#siamese-network>Siamese Network</a></li><li><a href=#triplet-loss>Triplet Loss</a></li><li><a href=#face-verification-and-binary-classification>Face Verification and Binary Classification</a></li></ul></li><li><a href=#neural-style-transfer>Neural Style Transfer</a><ul><li><a href=#what-is-neural-style-transfer>What is Neural Style Transfer?</a></li><li><a href=#what-are-deep-cnns-learning>What are deep CNNs learning?</a></li><li><a href=#cost-function>Cost Function</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>This is the fourth and last week of the <a href=https://www.coursera.org/learn/convolutional-neural-networks/>fourth course</a> of DeepLearning.AI&rsquo;s <a href=https://www.coursera.org/specializations/deep-learning>Deep Learning Specialization</a> offered on Coursera. In this week, we go over special applications in the field of computer vision with CNNs: face recognition and neural style transfer. This week introduces new important concepts that will be useful even beyond the context of CNNs.</p><p>This week&rsquo;s topics are as follows:</p><ul><li><a href=/posts/coursera/deep-learning-specialization/cnns/week4/#face-recognition>Face Recognition</a><ul><li><a href=/posts/coursera/deep-learning-specialization/cnns/week4/#what-is-face-recognition>What is Face Recognition?</a></li><li><a href=/posts/coursera/deep-learning-specialization/cnns/week4/#one-shot-learning>One Shot Learning</a></li><li><a href=/posts/coursera/deep-learning-specialization/cnns/week4/#siamese-network>Siamese Network</a></li><li><a href=/posts/coursera/deep-learning-specialization/cnns/week4/#triplet-loss>Triplet Loss</a></li><li><a href=/posts/coursera/deep-learning-specialization/cnns/week4/#face-verification-and-binary-classification>Face Verification and Binary Classification</a></li></ul></li><li><a href=/posts/coursera/deep-learning-specialization/cnns/week4/#neural-style-transfer>Neural Style Transfer</a><ul><li><a href=/posts/coursera/deep-learning-specialization/cnns/week4/#what-is-neural-style-transfer>What is Neural Style Transfer?</a></li><li><a href=/posts/coursera/deep-learning-specialization/cnns/week4/#what-are-deep-cnns-learning>What are deep CNNs learning?</a></li><li><a href=/posts/coursera/deep-learning-specialization/cnns/week4/#cost-function>Cost Function</a><ul><li><a href=/posts/coursera/deep-learning-specialization/cnns/week4/#content-cost-function>Content Cost Function</a></li><li><a href=/posts/coursera/deep-learning-specialization/cnns/week4/#style-cost-function>Style Cost Function</a></li></ul></li></ul></li></ul><hr><h2 id=face-recognition>Face Recognition<a hidden class=anchor aria-hidden=true href=#face-recognition>#</a></h2><h3 id=what-is-face-recognition>What is Face Recognition?<a hidden class=anchor aria-hidden=true href=#what-is-face-recognition>#</a></h3><p>Let&rsquo;s start by going over the important distinction between face verification and face recognition.</p><p>In the case of face verification, we get a pair of things: an input image, and also a name or ID. The output of such a system is a binary choice that says whether the image corresponds to the name or ID, or whether it does not correspond. On the other hand, in the case of face recognition, we only get a single thing: an image. The output of such a system is whether the input image matches to any $K$ identities already stored in our system&rsquo;s database.</p><p>This means that face verification is a $1 \to 1$ procedure, while face recognition is a $1 \to K$ procedure. The latter is whether we find a match in or not, and this usually means doing $K$ comparisons in the worst case.</p><p>So far it doesn&rsquo;t sound too bad, but here&rsquo;s the real issue. Imagine that you&rsquo;re implementing a face recognition system at your company&rsquo;s building. How many images can you get of each person? Ideally you&rsquo;d like to get as many as possible, but that&rsquo;s unfeasible; even worse, in practice, we usually only have access to one or at most two pictures of each person. Having only one training example for each &ldquo;class&rdquo; is what defines one-shot learning; having a few is usually called few-shot learning.</p><h3 id=one-shot-learning>One Shot Learning<a hidden class=anchor aria-hidden=true href=#one-shot-learning>#</a></h3><p>So imagine that we go around our company and finally get a face picture of everyone that should be allowed to get into the building. We want to train a model so that when someone walks into the door, and is one of the people allowed, that the model recognizes that this person is allowed from only one picture. Obviously, we also want to keep away unwanted people!</p><p>The most immediate idea is to train a classifier with a CNN architecture. If we have $K$ employees, then the output layer of our CNN will have $K + 1$ hidden units in the softmax, the extra one for when the input doesn&rsquo;t match anyone. The issue is that this CNN will have terrible performance because of the size of our training data—remember, we only have $K$ pictures. Even worse, what happens when we hire someone else? We would have to retrain the whole network every time. There has to be a better way.</p><p>The better way is to move away from classification and to think of a way to learn a <em>similarity</em> function. A similarity function is a function that takes in a pair of elements and outputs their similarity. You might have heard of Jaccard similarity or cosine similarity. In this case, we want to learn a function that takes two images:</p><p>$$
\text{d}(\text{img}_1, \text{img}_2) = \text{degree of difference between the images}
$$</p><p>Then we can set some threshold $\tau$ and binarize our similarity function:</p><p>$$
\text{Verification}(\text{img}_1, \text{img}_2) =
\begin{cases}
\text{Same} & \text{if} \ \text{d}(\text{img}_1, \text{img}_2) \leq \tau \\
\text{Different} & \text{otherwise}
\end{cases}
$$</p><p>Hopefully, when our colleague Alice walks in the door of the building, the picture taken from the device at the entrance will have the lowest difference when compared to Alice&rsquo;s picture in our database, amongst all other employees. Also, someone who is not our colleague, should not have a similarity less than $\tau$ amongst anybody else in our employee set. We will see how these kinks are ironed out when we get to <a href=/posts/coursera/deep-learning-specialization/cnns/week4/#triplet-loss>triplet loss</a>. Let&rsquo;s first see how we can learn such a function as $\text{d}(\text{img}_1, \text{img}_2)$.</p><h3 id=siamese-network>Siamese Network<a hidden class=anchor aria-hidden=true href=#siamese-network>#</a></h3><p>The idea behind Siamese networks is pretty straightforward: we run a pair of images, $(\text{img}_1, \text{img}_2)$, through <em>the same</em> CNN with a fully connected layer at the end, and then use these outputs to compare the images. Let&rsquo;s dig deeper into what this means.</p><p>Remember that CNNs usually reduce the spatial dimensions of the input volume while increasing the channel dimension. Imagine that we have a CNN that takes an image of a face with dimensions $100 \times 100 \times 3$ and after reducing this volume through some number of convolutional and max pooling layers, we get a $128$ dimensional vector. This is the same as in a vanilla CNN, but right before we run the fully connected layer through a softmax layer for classification.</p><p>Let&rsquo;s call our input image $x^{(1)}$. Let&rsquo;s call running $x^{(1)}$ through our CNN, and transforming the $100 \times 100 \times 3$ input into a $128$ element vector $f(x^{(1)})$, so that $f(x^{(1)}) \in \mathbb{R}^{128}$. This $128$ dimensional vector is an <strong>encoding</strong> of the input; it simply some particular representation of the original image containing a face.</p><p>If we let $x^{(1)}$ be the image of our employee in the database, and $x^{(2)}$ be the image of the face of the person who just walked in, you might imagine what we want to do. We&rsquo;ll run $x^{(2)}$ through our CNN and encode it in the same way, so that we get $f(x^{(2)})$. Notice that $f(x^{(2)}) \in \mathbb{R}^{128}$ as well, and that the encoding was generated with the <em>same</em> CNN as $f(x^{(1)})$.</p><p>Now we can redefine our similarity function: $\text{d}(\text{img}_1, \text{img}_2)$ in terms of these numerical encodings, so that our new similarity function is:</p><p>$$
d(x^{(1)}, x^{(2)}) = ||f(x^{(1)}) - f(x^{(2)})||_2^2
$$</p><p>This is our old friend, the L2-norm of the difference between the two encodings.</p><blockquote><p>You might be thinking, why not compare the two images with the L2-norm directly? Think about what happens when you compare two images of the same person but with different lighting, hairstyle, makeup, etc. It won&rsquo;t work well.</p></blockquote><p>But how do we train such a network? Remember we want a CNN that takes as input an image, and generates a $128$ dimensional encoding $f(x^{(i)})$. But of course, not just any encoding! We want that $128$ dimensional encoding to have certain properties:</p><ul><li>If $x^{(i)}, x^{(j)}$ are the same person, we want $||f(x^{(1)}) - f(x^{(2)})||_2^2$ to be small.</li><li>If $x^{(i)}, x^{(j)}$ are different persons, we want $||f(x^{(1)}) - f(x^{(2)})||_2^2$ to be large.</li></ul><p>It turns out that we can use back-propagation to learn such parameters, as long as we can define a loss function that has these properties, i.e. it generates good encodings for comparing images of faces. This is how triplet loss was defined.</p><h3 id=triplet-loss>Triplet Loss<a hidden class=anchor aria-hidden=true href=#triplet-loss>#</a></h3><p>Triplet loss is called so because it uses three elements: an anchor, a positive and a negative example. The anchor is a picture of one of our employees. The positive example is <em>another</em> picture of the <em>same</em> employee. The negative example is simply a picture of someone else. That is, we distance between the anchor and the positive example should be low, while the distance between the anchor and the negative example should be high. We will use the letters $A, P, N$ to refer to the anchor, positive and negative examples respectively.</p><p>Remember the two properties we wanted out of our encodings, we wanted that the L2-norm of the difference between two encodings to be small if they are of the same person and large if they are not. At least we want the distance to be larger between the anchor and the negative than the distance between the anchor and the positive example. In math, we want:</p><p>$$
\begin{aligned}
||f(A) - f(P)||_2^2 &\leq ||f(A) - f(N)||_2^2 \\
||f(A) - f(P)||_2^2 - ||f(A) - f(N)||_2^2 &\leq 0
\end{aligned}
$$</p><p>There&rsquo;s an issue with this approach. There is a trivial solution where we set everything to $0$. To prevent our network from learning this solution, we add a margin; similar to the one used in support vector machines. The margin, called $\alpha$ can be a hyperparameter, so that we have the following:</p><p>$$
||f(A) - f(P)||_2^2 - ||f(A) - f(N)||_2^2 + \alpha \leq 0
$$</p><p>Setting $\alpha$ allows us to specify how much bigger the difference between the anchor and the negative compared to the anchor and the positive examples should be.</p><p>We are ready to define our loss function, given three images $A, P, N$:</p><p>$$
\mathcal{L}(A, P, N) = \max(||f(A) - f(P)||_2^2 - ||f(A) - f(N)||_2^2 + \alpha, 0)
$$</p><p>We use the $\max$ operator here because as long as we have made the difference between the L2-norm of the differences less than $0$ (plus the margin), we have done &ldquo;good&rdquo;. Otherwise, we have done well, and the difference is above zero.</p><p>We can also now define our cost function, over some $m$ training samples:</p><p>$$
J = \sum_{i=1}^m \mathcal{L}(A^{(i)}, P^{(i)}, N^{(i)})
$$</p><p>Notice that to form the triplets, we need at least two pictures of the same person! For example, we could have a training set of $10,000$ images of $1,000$ people, some of them repeated more than once. You might be wondering, sure, but how do we make the triplets? It turns out that this is very important.</p><p>If we choose $A, P, N$ triplets at random, then the constraints are easily satisfied; but our network will not perform well. We need to choose triplets that are &ldquo;hard&rdquo; to train on, i.e. $\text{d}(A, P) \approx \text{d}(A, N)$. By doing this, we are forcing the network to deal with the harder cases, where people look similar but are not the same person. The details on how to build triplets are described in the <a href=https://arxiv.org/abs/1503.03832>FaceNet</a> paper by Schroff, et al.</p><h3 id=face-verification-and-binary-classification>Face Verification and Binary Classification<a hidden class=anchor aria-hidden=true href=#face-verification-and-binary-classification>#</a></h3><p>It turns out that the triplet loss approach is not the only way to build a face recognition system. We could also use the result of the two CNNs, i.e., the $128$-dimensional embeddings of the pictures, and then feed these to a logistic regression unit and perform binary classification on them; estimating whether the two embeddings are the same or not. The final logistic layer would look like this:</p><p>$$
\hat{y} = \sigma \left( \sum_{k=1}^{128} w_k ||f(x^{(i)})_{k} - f(x^{(j)})_k||_2 + b \right)
$$</p><p>Where we are still using the embeddings, but we are calculating an element-wise squared difference, each multiplied by its own weight.</p><p>In practice, and in both approaches, we can speed up the latency of the system by precomputing the encodings of our employees, and only computing the encoding of the person we are trying to recognize on the fly.</p><h2 id=neural-style-transfer>Neural Style Transfer<a hidden class=anchor aria-hidden=true href=#neural-style-transfer>#</a></h2><h3 id=what-is-neural-style-transfer>What is Neural Style Transfer?<a hidden class=anchor aria-hidden=true href=#what-is-neural-style-transfer>#</a></h3><p>You might still remember back when neural style transfer was the latest, hottest thing in machine learning. Perhaps even more so, the pointed questions it raised about intellectual property rights. Today, many companies use stable diffusion, a text-to-image system, as an interface for performing neural style transfer.</p><p>Neural style transfer, in a nutshell, is being able to imbue an image with another style. For example, we might have a picture of our cats, but we&rsquo;d like to make that picture look like it was painted by Rembrandt. Neural style transfer allows us to &ldquo;transfer&rdquo; the style of a Rembrandt painting into a picture of our cat, so that it looks like Rembrandt himself painted our furry friend.</p><p>We will be using the notation $C$ to refer to the <em>content</em> image, in our case our cat. The letter $S$ will represent the <em>style</em> image, in our case a Rembrandt painting. Finally, we will use $G$ to refer to the <em>generated</em> image, that is our cat in the style of Rembrandt.</p><h3 id=what-are-deep-cnns-learning>What are deep CNNs learning?<a hidden class=anchor aria-hidden=true href=#what-are-deep-cnns-learning>#</a></h3><p>Before getting into neural style transfer, we must understand, at a high level, how the input changes through the layers in a CNN. There is an amazing paper by <a href=https://arxiv.org/abs/1311.2901>Zeiler and Fergus, 2013</a> in which they come up with novel ways to visualize what visual features the filters are learning on each of the layers.</p><p>The gist of it is that the filters in the shallower (earlier) layers of the network learn to pick apart basic features in our image—think vertical lines, diagonal lines, etc. As we progress down the layers, deeper into the CNN, the filters start to learn more abstract features, such as concentric circles, colors, and lines, etc. Even later on, we see that some filters specialize in certain regions of the face: noses, eyes, etc. So on and so forth.</p><p>This is important to keep in mind, especially in the context of neural style transfer, because we can choose to give a unique weight to each layer in the combination of content and style.</p><h3 id=cost-function>Cost Function<a hidden class=anchor aria-hidden=true href=#cost-function>#</a></h3><p>Similar to all other applications of supervised learning, we need to establish a cost function that will guide the optimization process. Let&rsquo;s get started.</p><p>We have our content image $C$ and our style image $S$, and we&rsquo;d like to generate an image $G$ which is some mixture of both $C$ and $S$. We will define our loss function in terms of these elements:</p><p>$$
J(G) = \alpha J_{content}(C, G) + \beta J_{style}(S, G)
$$</p><p>Let&rsquo;s unpack the formula. Our total cost is a function of two separate costs. The first one is the content cost, that is, how bad our generated image is relative to the original content image, which in our example is a picture of a cat. The second one is the style cost, how bad our generated image is relative to the original style image, which in our example is a Rembrandt painting. We have two hyperparameters $\alpha$ and $\beta$ which allow us to adjust the mixture between the two costs.</p><p>But how do we get $G$? We start by initializing it randomly. In practice, we add some random noise to the original content image $C$, but imagine that we start $G$ completely at random; that is, a random noise picture of dimensions $100 \times 100 \times 3$. Then we can use gradient descent to update our random picture $G$:</p><p>$$
G := G - \frac{\partial}{\partial G} J(G)
$$</p><p>We are not updating any parameters! We are directly updating the pixel values of our generated image $G$ at each step of gradient descent, or whatever garden variety optimization algorithm we choose to use. Let&rsquo;s now break down each of the components of the cost function.</p><h4 id=content-cost-function>Content Cost Function<a hidden class=anchor aria-hidden=true href=#content-cost-function>#</a></h4><p>Remember, our cost function was:</p><p>$$
J(G) = \alpha J_{content}(C, G) + \beta J_{style}(S, G)
$$</p><p>We will focus on the $J_{content}(C, G)$ component.</p><p>Say that we are using a pre-trained CNN, such as VGG-19, and that we focus on some layer $l$. Keeping in mind what happens at <a href=/posts/coursera/deep-learning-specialization/cnns/week4/#what-are-deep-cnns-learning>each layer</a> of a CNN, we will hedge our bets and pick some $l$ that&rsquo;s in the middle—not too deep and not too shallow. We will focus on the activations for this layer, $a^{[l]}$.</p><p>We will run both images, $C$ and $G$, through $a^{[l]}$ and generate $a^{<a href=C>l</a>}, a^{<a href=G>l</a>}$. The idea is that if we compare $a^{<a href=C>l</a>}, a^{<a href=G>l</a>}$, and they are similar, then the images have similar content. This means that the filters are picking up similar activations on the features that they specialize in for that particular layer.</p><p>This comparison is done by our old friend, the L2-norm of the difference:</p><p>$$
J^{[l]}_{content}(C, G) = \frac{1}{4 n_H^{[l]} n_W^{[l]} n_C^{[l]}}\sum_{\text{all entries}}||a^{[l](C)} - a^{[l](G)}||_2^2
$$</p><p>The normalization factor was chosen by the authors and takes into account the dimensions of $a^{[l]}$.</p><p>This will make our optimization algorithm set pixel values on $G$ that minimize this difference, so that the generated image has similar content to the content image.</p><h4 id=style-cost-function>Style Cost Function<a hidden class=anchor aria-hidden=true href=#style-cost-function>#</a></h4><p>Calculating the content cost is so far not that crazy; doing element-wise squared differences is a reasonable approach for the content. But how can we quantify &ldquo;style&rdquo; and compare our generated image $G$ to our style image $S$ to see how far off they are from each other? Here is where things get fascinating.</p><p>The authors define &ldquo;style&rdquo; as the pair-wise correlation between the activations <em>across channels</em>. Since each channel learns a different feature, style is defined as the interaction between the features. More specifically, their covariance. If one feature picks up stark lines, and another one picks up colors, then if both of these features have positive or negative covariance, then we can say that this is one of the elements of style. When we do a pair-wise comparison between all channels, we get a pair-wise matrix, which we will call $G^{[l]}$, the style matrix. This matrix will tell us how each of the channels covary with each other, and this, in a sense, is the essence of style.</p><p>We want to compare the style matrix for both $S$, our style image, and $G$, our generated image, and repeat the comparison we did for the content image, but this time on the style matrix or <a href=https://en.wikipedia.org/wiki/Gram_matrix>Gram matrix</a> of both $G$ and $S$. Both style matrices $G^{<a href=S>l</a>}$ and $G^{<a href=G>l</a>}$ will be of the same dimensions: $n_C^{[l]} \times n_C^{[l]}$.</p><p>Let&rsquo;s start with the style matrix of $S$. We define $a^{<a href=S>l</a>}_{i,j,k}$ as <em>one</em> entry in the activations for layer $l$ using $S$ as the input. We will construct $G^{<a href=S>l</a>}$, the Gram matrix of the activations. The entry $kk&rsquo;$ in the $G^{<a href=S>l</a>}$ is defined as:</p><p>$$
G^{[l](S)}_{kk&rsquo;} = \sum_{i=1}^{n_H} \sum_{j=1}^{n_W} a^{[l](S)}_{i,j,k} a^{[l](S)}_{i,j,k&rsquo;}
$$</p><p>We will repeat the same, but for $G$, our generated image:</p><p>$$
G^{[l](G)}_{kk&rsquo;} = \sum_{i=1}^{n_H} \sum_{j=1}^{n_W} a^{[l](G)}_{i,j,k} a^{[l](G)}_{i,j,k&rsquo;}
$$</p><p>Let&rsquo;s recap:</p><ul><li>We have two images, $S$ and $G$.</li><li>We defined the &ldquo;style&rdquo; of an image as the gram matrix of the volume. That is, the element-wise channel covariance.</li><li>We calculated the gram or style matrix from the activations of layer $l$ for both $G$ and $S$. That is we ran both $G$ and $S$ through the same layer and got some output. It is from this output that we calculate the style matrix for $G$ and $S$ separately.</li></ul><p>Now, we compare the two in the same fashion we did for the content cost. The style cost is defined as:</p><p>$$
J^{[l]}_{style}(S, G) = \frac{1}{\left(2 n_H^{[l]} n_W^{[l]} n_C^{[l]} \right)^2} \sum_{k=1}^{n_C^{[l]}} \sum_{k&rsquo;=1}^{n_C^{[l]}} \left( G_{kk&rsquo;}^{[l](S)} - G_{kk&rsquo;}^{[l](G)}\right)^2
$$</p><p>Where again, the normalization factor in front was set by the authors.</p><p>A final thing to notice about the style function: notice that it&rsquo;s indexed by $l$; we calculate the style function at every layer $l$. The authors define the cost function at the network level as:</p><p>$$
J_{style}(S, G) = \sum_{l=1}^L \lambda^{[l]} J^{[l]}_{style}(S, G)
$$</p><p>Using the $\lambda^{[l]}$ parameter for each layer allows us to mix the more basic features in the shallower layers or the more abstract features in the deeper layers. If we want the generated image to <em>softly</em> follow the style, we choose larger weights for the <em>deeper</em> layers and smaller for the shallower layers. On the other hand, if we want our generated image to <em>strongly</em> follow the style image, we do the opposite: select smaller weights for the deeper layers but larger weights for the shallower layers.</p><p>Now we can come back to the cost function we defined earlier:</p><p>$$
J(G) = \alpha J_{content}(C, G) + \beta J_{style}(S, G)
$$</p><p>That&rsquo;s it, this is the last week of the CNN course. The next course is sequential models, which, to me, are much more interesting in terms of applications. The programming exercises for this week were fantastic as well, and I highly suggest that you do them as well.</p><p>Next week&rsquo;s post is <a href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week1/>here</a>, and it&rsquo;s the first week in the sequence models course.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://manmartgarc.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=https://manmartgarc.github.io/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=prev href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week3/><span class=title>« Prev</span><br><span>Convolutional Neural Networks: Week 3 | Detection Algorithms</span>
</a><a class=next href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week1/><span class=title>Next »</span><br><span>Sequence Models: Week 1 | Recurrent Neural Networks</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://manmartgarc.github.io/>Manuel Martinez</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>