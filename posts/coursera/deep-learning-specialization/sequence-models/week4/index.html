<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Sequence Models: Week 4 | Transformers | Manuel Martinez</title><meta name=keywords content="machine learning,deep learning"><meta name=description content="This is the fourth and last week of the fifth course of DeepLearning.AI&rsquo;s Deep Learning Specialization offered on Coursera. The main topic for this week is transformers, a generalization of the attention model that has taken the deep learning world by storm since its inception in 2017.
This week&rsquo;s topics are:

Transformer Network Intuition
Self-Attention
Multi-Head Attention
Transformer Network Architecture
More Information


Transformer Network Intuition
We started with RNNs (known as part of the prehistoric era now), a simple model that reutilizes the same weights at each time steps; allowing to combine previous step&rsquo;s hidden states with the current one. To solve some issues with vanilla RNNs, we introduced GRUs and LSTMs; both more flexible and more complex than simple RNNs. However, one of the things that they all share in common is that the input must be processed sequentially, i.e. one token at a time. This is a problem with large models, where we want to parallelize computation as much as possible. Amdahl&rsquo;s Law gives us a theoretical speed up limit based on the fraction of parallelizable compute in a computer program. Unfortunately, since the entire model is sequential the speed-ups are miniscule. The transformer architecture allows us to process the entire input at once, and in parallel; allowing us to train much more complex models which in turn generate richer feature representations of our sequences."><meta name=author content="Manuel Martinez"><link rel=canonical href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week4/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://manmartgarc.github.io/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://manmartgarc.github.io/images/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://manmartgarc.github.io/images/favicon.ico><link rel=apple-touch-icon href=https://manmartgarc.github.io/apple-touch-icon.png><link rel=mask-icon href=https://manmartgarc.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week4/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week4/"><meta property="og:site_name" content="Manuel Martinez"><meta property="og:title" content="Sequence Models: Week 4 | Transformers"><meta property="og:description" content="This is the fourth and last week of the fifth course of DeepLearning.AI’s Deep Learning Specialization offered on Coursera. The main topic for this week is transformers, a generalization of the attention model that has taken the deep learning world by storm since its inception in 2017.
This week’s topics are:
Transformer Network Intuition Self-Attention Multi-Head Attention Transformer Network Architecture More Information Transformer Network Intuition We started with RNNs (known as part of the prehistoric era now), a simple model that reutilizes the same weights at each time steps; allowing to combine previous step’s hidden states with the current one. To solve some issues with vanilla RNNs, we introduced GRUs and LSTMs; both more flexible and more complex than simple RNNs. However, one of the things that they all share in common is that the input must be processed sequentially, i.e. one token at a time. This is a problem with large models, where we want to parallelize computation as much as possible. Amdahl’s Law gives us a theoretical speed up limit based on the fraction of parallelizable compute in a computer program. Unfortunately, since the entire model is sequential the speed-ups are miniscule. The transformer architecture allows us to process the entire input at once, and in parallel; allowing us to train much more complex models which in turn generate richer feature representations of our sequences."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-13T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-13T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Sequence Models: Week 4 | Transformers"><meta name=twitter:description content="This is the fourth and last week of the fifth course of DeepLearning.AI&rsquo;s Deep Learning Specialization offered on Coursera. The main topic for this week is transformers, a generalization of the attention model that has taken the deep learning world by storm since its inception in 2017.
This week&rsquo;s topics are:

Transformer Network Intuition
Self-Attention
Multi-Head Attention
Transformer Network Architecture
More Information


Transformer Network Intuition
We started with RNNs (known as part of the prehistoric era now), a simple model that reutilizes the same weights at each time steps; allowing to combine previous step&rsquo;s hidden states with the current one. To solve some issues with vanilla RNNs, we introduced GRUs and LSTMs; both more flexible and more complex than simple RNNs. However, one of the things that they all share in common is that the input must be processed sequentially, i.e. one token at a time. This is a problem with large models, where we want to parallelize computation as much as possible. Amdahl&rsquo;s Law gives us a theoretical speed up limit based on the fraction of parallelizable compute in a computer program. Unfortunately, since the entire model is sequential the speed-ups are miniscule. The transformer architecture allows us to process the entire input at once, and in parallel; allowing us to train much more complex models which in turn generate richer feature representations of our sequences."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://manmartgarc.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Sequence Models: Week 4 | Transformers","item":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week4/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Sequence Models: Week 4 | Transformers","name":"Sequence Models: Week 4 | Transformers","description":"This is the fourth and last week of the fifth course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. The main topic for this week is transformers, a generalization of the attention model that has taken the deep learning world by storm since its inception in 2017.\nThis week\u0026rsquo;s topics are:\nTransformer Network Intuition Self-Attention Multi-Head Attention Transformer Network Architecture More Information Transformer Network Intuition We started with RNNs (known as part of the prehistoric era now), a simple model that reutilizes the same weights at each time steps; allowing to combine previous step\u0026rsquo;s hidden states with the current one. To solve some issues with vanilla RNNs, we introduced GRUs and LSTMs; both more flexible and more complex than simple RNNs. However, one of the things that they all share in common is that the input must be processed sequentially, i.e. one token at a time. This is a problem with large models, where we want to parallelize computation as much as possible. Amdahl\u0026rsquo;s Law gives us a theoretical speed up limit based on the fraction of parallelizable compute in a computer program. Unfortunately, since the entire model is sequential the speed-ups are miniscule. The transformer architecture allows us to process the entire input at once, and in parallel; allowing us to train much more complex models which in turn generate richer feature representations of our sequences.\n","keywords":["machine learning","deep learning"],"articleBody":"This is the fourth and last week of the fifth course of DeepLearning.AI’s Deep Learning Specialization offered on Coursera. The main topic for this week is transformers, a generalization of the attention model that has taken the deep learning world by storm since its inception in 2017.\nThis week’s topics are:\nTransformer Network Intuition Self-Attention Multi-Head Attention Transformer Network Architecture More Information Transformer Network Intuition We started with RNNs (known as part of the prehistoric era now), a simple model that reutilizes the same weights at each time steps; allowing to combine previous step’s hidden states with the current one. To solve some issues with vanilla RNNs, we introduced GRUs and LSTMs; both more flexible and more complex than simple RNNs. However, one of the things that they all share in common is that the input must be processed sequentially, i.e. one token at a time. This is a problem with large models, where we want to parallelize computation as much as possible. Amdahl’s Law gives us a theoretical speed up limit based on the fraction of parallelizable compute in a computer program. Unfortunately, since the entire model is sequential the speed-ups are miniscule. The transformer architecture allows us to process the entire input at once, and in parallel; allowing us to train much more complex models which in turn generate richer feature representations of our sequences.\nThe transformer architecture combines the attention model with a CNN architecture. The idea is to use the attention model’s ability to recognize relevance between pairs of tokens, with the computational efficiency of CNNs; which can be parallelized quite easily. Let’s dive into the two main components of the transformer architecture.\nSelf-Attention Self-attention is the same idea as attention when we used RNNs. However, since our model is not sequential anymore, we need to calculate the attention in one go. Let’s remember that attention is simply some value that describes how relevant a pair of tokens $$ are with respect to generating some output. In the RNN case, we learned some embedding $e^{}$ as a function of the previous step’s post-attention hidden state and each token $t’$ pre-attention hidden state. We no longer have previous hidden states since we are doing it all in one go. Let’s see how this implemented.\nThe first thing to take into account is that since our model is not sequential anymore, we have lost the temporal structure we got from using RNNs. This means that we will have to come up with a way to encode positions, which we will call the positional encodings. For now, just think that we have two things: our word embeddings from the input, and some positional encoding that encodes the position of each word in the sentence.\nMIT Introduction to Deep Learning\nNotice that in the figure, we add the positional encoding to the word embeddings; therefore imbuing the embedding with positional information which was absent before.\nNow we need to come up with a way to define attention. We can think of attention as a way for input nodes to communicate with each other. How can we imbue each node to talk with each other? We will define three things for each node in the input:\nKey $k^{}$: What do I have? Query $q^{}$: What am I looking for? Value $v^{}$: What do I publicly reveal/broadcast to others? 1 Let’s define mathematically attention first, and then we will go over what each of these vectors represent:\n$$ A^{}(q^{}, K, V) = \\sum_{t’=1}^{T_x} \\frac{\\exp(q^{}k^{})}{\\sum_{j=1}^{T_x}\\exp(q^{}k^{})} v^{} $$\nLet’s use as an example, the input sentence Jane visite l’Afrique en septembre., and let’s focus on $x^{\u003c3\u003e} = \\text{l’Afrique}$ and calculate $A^{\u003c3\u003e}$.\nFirst, $q^{\u003c3\u003e}, k^{\u003c3\u003e}, v^{\u003c3\u003e}$ are generated with three weight matrices $W^Q, W^K, W^V$ which are learnable parameters:\n$$ \\begin{aligned} q^{\u003c3\u003e} \u0026= W^Q x^{\u003c3\u003e} \\\\ k^{\u003c3\u003e} \u0026= W^K x^{\u003c3\u003e} \\\\ v^{\u003c3\u003e} \u0026= W^V x^{\u003c3\u003e} \\end{aligned} $$\nTo compute $A^{\u003c3\u003e}$ we will allow $x^{\u003c3\u003e}$ to communicate to all other tokens what it’s looking for: $q^{\u003c3\u003e}$. Each of the tokens will respond with $k^{}$, answering what they have. This is the key part: if the dot product between $q^{\u003c3\u003e}$ and $k^{}$ is high, it means that $k^{}$ has what $q^{\u003c3\u003e}$ is looking for; we are simply looking for a similarity between the query and key vectors. We will allow each token to communicate with all others, and then normalize their contributions with a softmax. We also use $v^{}$ to weight the contribution, allowing token $t’$ to not just say that it has what someone else is looking for, but what it is, regardless of what someone else is looking for. Finally, we sum all of these up into $A^{\u003c3\u003e}$. Let’s revisit these steps in more detail again.\nRemember that we are doing all of this in one go, therefore we need to do this in a vectorized way using matrix multiplication. Let’s redefine attention with matrices:\n$$ \\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_K}}V \\right) $$\nWe get each of these matrices by multiplying the positional embeddings with each of the $W^Q, W^K, W^V$ matrices:\nMIT Introduction to Deep Learning\nLet’s break down the matrix version of the attention formula. Let’s focus on this term:\n$$ \\frac{QK^T}{\\sqrt{d_K}} $$\nThis term is calculating the pair-wise similarity between queries and keys for all the inputs:\nMIT Introduction to Deep Learning\nThis means that we will have a matrix with the dimensions of our maximum input size, where each row and column corresponds to a position of the input. Along the diagonal, we will have the similarity between each token and itself. We would like to normalize the values to sum up to one (across some specific dimension!); we can use our trusty softmax to do that. We therefore get:\n$$ \\text{attention weighting} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_K}} \\right) $$\nMIT Introduction to Deep Learning\nThis matrix encodes which token is relevant for every token in the output. We know to which token to pay attention to, but what about that token do we pay attention to? This is what $V$ encodes. Multiplying the previous with $V$ allows us to extract features with high attention. We finally get to:\n$$ \\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_K}}V \\right) $$\nMIT Introduction to Deep Learning\nIf we do this for every token in our output and get $A^{} \\forall t \\in T_x$ we will get an attention embedding for all the inputs. This constitutes a single head. It turns out that we will use a head similar to how we use a filter in the context of CNNs. This is the part that we can run in parallel, on top of the vectorization of the $A^{}$ calculation. By using different heads, we allow the model to focus on different features when generating the queries, keys and values. That is we can learn to pay attention to different things, as many things as we have heads. This means that if we have $5$ heads, we will have $W_1^Q, W_1^K, W_1^V, \\dots, W_5^Q, W_5^K, W_5^V$. This is called multi-head attention.\nMulti-Head Attention Similar to how we can stack filters in a CNN to learn different features, we will stack multiple heads to learn different attention representations for each token pair. We know that attention a single-head attention is defined as:\n$$ \\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_K}}V \\right) $$\nWe will index each head with the subscript $i$ so that:\n$$ \\text{head}_i = \\text{Attention}(W_i^Q, W_i^K, W_i^V) $$\nThis allows us to define multi-head attention as:\n$$ \\text{MultiHead}(Q, K, V) = \\text{concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W_o $$\nNotice that $W_o$ is another matrix with learnable parameters, which allows us to dial up or down the signal coming from the multi-head component.\nWe said that the transformer architecture allows for parallelization before, and this is exactly the part that runs in parallel. That is, every head runs the communication (attention) scheme in parallel.\nTransformer Network Architecture Alright, let’s do a quick recap:\nWe have dealt with the loss of temporal structure by using positional encodings. We have defined a way which allows nodes to communicate with each other, and learn to which of their friends to pay attention to. We have done this $h$ times, the number of heads, to allow the nodes to ask different combinations of questions and answers: What do you mean? Where are you? Etc. The output of this is some encoding of the input that has all this information clumped together into a super rich embedding of the inputs. This is what we call the encoder part:\nAttention is All You Need\nThe three arrows in the encoder part (left) that go into the Multi-Head Attention component are the three query, keys and values matrices $Q, K, V$ for each of the heads. Remember, we learn $Q, K, V$ via optimization; and we have as many of these representations as we have heads. An additional step shown in the figure is that we add a skip connection with normalization; similar to how we implemented skip-connections in U-Nets.\nWhat about the decoder? The decoder will take the inputs, but shifted to the right for each context length, and learn new $Q, K, V$ representations from the training labels. In machine translation, these are $Q, K, V$ in English instead of French. It will then be able to get its own questions $Q$ in English, and allow it to reach out into the encoder for finding keys and values. This is sometimes called the cross-attention module. After this, we run the embeddings through a feed-forward layer to select the most important features and generate the softmax probabilities for the next token in the prediction.\nMore Information I personally feel like the transformer content was an afterthought in the course. Compared to other content, the transformer content was very shallow and short. There are many amazing communicators that talk about transformers, here are some that I found helpful:\nAndrej Karpathy | Let’s build GPT: from scratch, in code, spelled out. CS25 I Stanford Seminar - Transformers United 2023: Introduction to Transformers w/ Andrej Karpathy MIT 6.S191: Recurrent Neural Networks, Transformers, and Attention Transformer models and BERT model: Overview Attention is All You Need CS25 | Stanford Seminar ↩︎\n","wordCount":"1694","inLanguage":"en","datePublished":"2023-08-13T00:00:00Z","dateModified":"2023-08-13T00:00:00Z","author":{"@type":"Person","name":"Manuel Martinez"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week4/"},"publisher":{"@type":"Organization","name":"Manuel Martinez","logo":{"@type":"ImageObject","url":"https://manmartgarc.github.io/images/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://manmartgarc.github.io/ accesskey=h title="Manuel Martinez (Alt + H)">Manuel Martinez</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://manmartgarc.github.io/about/ title=About><span>About</span></a></li><li><a href=https://manmartgarc.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://manmartgarc.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://manmartgarc.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://manmartgarc.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://manmartgarc.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://manmartgarc.github.io/cv/ title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://manmartgarc.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://manmartgarc.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Sequence Models: Week 4 | Transformers</h1><div class=post-meta><span title='2023-08-13 00:00:00 +0000 UTC'>August 13, 2023</span>&nbsp;·&nbsp;<span>8 min</span>&nbsp;·&nbsp;<span>Manuel Martinez</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#transformer-network-intuition>Transformer Network Intuition</a></li><li><a href=#self-attention>Self-Attention</a></li><li><a href=#multi-head-attention>Multi-Head Attention</a></li><li><a href=#transformer-network-architecture>Transformer Network Architecture</a></li><li><a href=#more-information>More Information</a></li></ul></nav></div></details></div><div class=post-content><p>This is the fourth and last week of the <a href="https://www.coursera.org/learn/nlp-sequence-models?specialization=deep-learning/">fifth course</a> of DeepLearning.AI&rsquo;s <a href=https://www.coursera.org/specializations/deep-learning>Deep Learning Specialization</a> offered on Coursera. The main topic for this week is transformers, a generalization of the attention model that has taken the deep learning world by storm since its inception in 2017.</p><p>This week&rsquo;s topics are:</p><ul><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week4/#transformer-network-intuition>Transformer Network Intuition</a></li><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week4/#self-attention>Self-Attention</a></li><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week4/#multi-head-attention>Multi-Head Attention</a></li><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week4/#transformer-network-architecture>Transformer Network Architecture</a></li><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week4/#more-information>More Information</a></li></ul><hr><h2 id=transformer-network-intuition>Transformer Network Intuition<a hidden class=anchor aria-hidden=true href=#transformer-network-intuition>#</a></h2><p>We started with RNNs (known as part of the <a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&amp;t=1436s">prehistoric era now</a>), a simple model that reutilizes the same weights at each time steps; allowing to combine previous step&rsquo;s hidden states with the current one. To solve some issues with vanilla RNNs, we introduced GRUs and LSTMs; both more flexible and more complex than simple RNNs. However, one of the things that they all share in common is that the <em>input</em> must be processed sequentially, i.e. one token at a time. This is a problem with large models, where we want to parallelize computation as much as possible. <a href=https://en.wikipedia.org/wiki/Amdahl%27s_law>Amdahl&rsquo;s Law</a> gives us a theoretical speed up limit based on the fraction of parallelizable compute in a computer program. Unfortunately, since the entire model is sequential the speed-ups are miniscule. The transformer architecture allows us to process <em>the entire</em> input at once, and in parallel; allowing us to train much more complex models which in turn generate richer feature representations of our sequences.</p><p>The transformer architecture combines the <a href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week3/>attention model</a> with a CNN architecture. The idea is to use the attention model&rsquo;s ability to recognize relevance between pairs of tokens, with the computational efficiency of CNNs; which can be parallelized quite easily. Let&rsquo;s dive into the two main components of the transformer architecture.</p><h2 id=self-attention>Self-Attention<a hidden class=anchor aria-hidden=true href=#self-attention>#</a></h2><p>Self-attention is the same idea as attention when we used RNNs. However, since our model is not sequential anymore, we need to calculate the attention in one go. Let&rsquo;s remember that attention is simply some value that describes how relevant a pair of tokens $&lt;t,t&rsquo;>$ are with respect to generating some output. In the RNN case, we learned some embedding $e^{&lt;t,t&rsquo;>}$ as a function of the previous step&rsquo;s post-attention hidden state and each token $t&rsquo;$ pre-attention hidden state. We no longer have previous hidden states since we are doing it all in one go. Let&rsquo;s see how this implemented.</p><p>The first thing to take into account is that since our model is not sequential anymore, we have lost the temporal structure we got from using RNNs. This means that we will have to come up with a way to encode positions, which we will call the positional encodings. For now, just think that we have two things: our word embeddings from the input, and some positional encoding that encodes the position of each word in the sentence.</p><figure><img loading=lazy src=/images/trans-1.png alt="MIT Introduction to Deep Learning" width=75%><figcaption><p><a href=http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L2.pdf>MIT Introduction to Deep Learning</a></p></figcaption></figure><p>Notice that in the figure, we add the positional encoding to the word embeddings; therefore imbuing the embedding with positional information which was absent before.</p><p>Now we need to come up with a way to define attention. We can think of attention as a way for input nodes to <a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&amp;t=672s">communicate with each other</a>. How can we imbue each node to talk with each other? We will define three things for each node in the input:</p><ol><li>Key $k^{&lt;t>}$: What do I have?</li><li>Query $q^{&lt;t>}$: What am I looking for?</li><li>Value $v^{&lt;t>}$: What do I publicly reveal/broadcast to others? <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></li></ol><p>Let&rsquo;s define mathematically attention first, and then we will go over what each of these vectors represent:</p><p>$$
A^{&lt;t>}(q^{&lt;t>}, K, V) = \sum_{t&rsquo;=1}^{T_x} \frac{\exp(q^{&lt;t>}k^{&lt;t&rsquo;>})}{\sum_{j=1}^{T_x}\exp(q^{&lt;t>}k^{&lt;j>})} v^{&lt;t&rsquo;>}
$$</p><p>Let&rsquo;s use as an example, the input sentence <em>Jane visite l&rsquo;Afrique en septembre.</em>, and let&rsquo;s focus on $x^{&lt;3>} = \text{l&rsquo;Afrique}$ and calculate $A^{&lt;3>}$.</p><p>First, $q^{&lt;3>}, k^{&lt;3>}, v^{&lt;3>}$ are generated with three weight matrices $W^Q, W^K, W^V$ which are learnable parameters:</p><p>$$
\begin{aligned}
q^{&lt;3>} &= W^Q x^{&lt;3>} \\
k^{&lt;3>} &= W^K x^{&lt;3>} \\
v^{&lt;3>} &= W^V x^{&lt;3>}
\end{aligned}
$$</p><p>To compute $A^{&lt;3>}$ we will allow $x^{&lt;3>}$ to communicate to all other tokens what it&rsquo;s looking for: $q^{&lt;3>}$. Each of the tokens will respond with $k^{&lt;t&rsquo;>}$, answering what they have. This is the key part: if the dot product between $q^{&lt;3>}$ and $k^{&lt;t&rsquo;>}$ is high, it means that $k^{&lt;t&rsquo;>}$ has what $q^{&lt;3>}$ is looking for; we are simply looking for a similarity between the query and key vectors. We will allow each token to communicate with all others, and then normalize their contributions with a softmax. We also use $v^{&lt;t&rsquo;>}$ to weight the contribution, allowing token $t&rsquo;$ to not just say that it has what someone else is looking for, but what it is, regardless of what someone else is looking for. Finally, we sum all of these up into $A^{&lt;3>}$. Let&rsquo;s revisit these steps in more detail again.</p><p>Remember that we are doing all of this in one go, therefore we need to do this in a vectorized way using matrix multiplication. Let&rsquo;s redefine attention with matrices:</p><p>$$
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_K}}V \right)
$$</p><p>We get each of these matrices by multiplying the positional embeddings with each of the $W^Q, W^K, W^V$ matrices:</p><figure><img loading=lazy src=/images/trans-2.png alt="MIT Introduction to Deep Learning" width=75%><figcaption><p><a href=http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L2.pdf>MIT Introduction to Deep Learning</a></p></figcaption></figure><p>Let&rsquo;s break down the matrix version of the attention formula. Let&rsquo;s focus on this term:</p><p>$$
\frac{QK^T}{\sqrt{d_K}}
$$</p><p>This term is calculating the <em>pair-wise</em> similarity between queries and keys for all the inputs:</p><figure><img loading=lazy src=/images/trans-3.png alt="MIT Introduction to Deep Learning" width=75%><figcaption><p><a href=http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L2.pdf>MIT Introduction to Deep Learning</a></p></figcaption></figure><p>This means that we will have a matrix with the dimensions of our maximum input size, where each row and column corresponds to a position of the input. Along the diagonal, we will have the similarity between each token and itself. We would like to normalize the values to sum up to one (across some specific dimension!); we can use our trusty softmax to do that. We therefore get:</p><p>$$
\text{attention weighting} = \text{softmax} \left( \frac{QK^T}{\sqrt{d_K}} \right)
$$</p><figure><img loading=lazy src=/images/trans-4.png alt="MIT Introduction to Deep Learning" width=75%><figcaption><p><a href=http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L2.pdf>MIT Introduction to Deep Learning</a></p></figcaption></figure><p>This matrix encodes which token is relevant for every token in the output. We know to which token to pay attention to, but <em>what</em> about that token do we pay attention to? This is what $V$ encodes. Multiplying the previous with $V$ allows us to extract features with high attention. We finally get to:</p><p>$$
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_K}}V \right)
$$</p><figure><img loading=lazy src=/images/trans-5.png alt="MIT Introduction to Deep Learning" width=75%><figcaption><p><a href=http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L2.pdf>MIT Introduction to Deep Learning</a></p></figcaption></figure><p>If we do this for every token in our output and get $A^{&lt;t>} \forall t \in T_x$ we will get an attention embedding for all the inputs. This constitutes a single <em>head</em>. It turns out that we will use a <em>head</em> similar to how we use a <em>filter</em> in the context of CNNs. This is the part that we can run in parallel, on top of the vectorization of the $A^{&lt;t>}$ calculation. By using different heads, we allow the model to focus on different features when generating the queries, keys and values. That is we can learn to pay attention to different things, as many things as we have heads. This means that if we have $5$ heads, we will have $W_1^Q, W_1^K, W_1^V, \dots, W_5^Q, W_5^K, W_5^V$. This is called multi-head attention.</p><h2 id=multi-head-attention>Multi-Head Attention<a hidden class=anchor aria-hidden=true href=#multi-head-attention>#</a></h2><p>Similar to how we can stack filters in a CNN to learn different features, we will stack multiple heads to learn different attention representations for each token pair. We know that attention a single-head attention is defined as:</p><p>$$
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_K}}V \right)
$$</p><p>We will index each head with the subscript $i$ so that:</p><p>$$
\text{head}_i = \text{Attention}(W_i^Q, W_i^K, W_i^V)
$$</p><p>This allows us to define multi-head attention as:</p><p>$$
\text{MultiHead}(Q, K, V) = \text{concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h)W_o
$$</p><p>Notice that $W_o$ is another matrix with learnable parameters, which allows us to dial up or down the signal coming from the multi-head component.</p><p>We said that the transformer architecture allows for parallelization before, and this is exactly the part that runs in parallel. That is, every head runs the communication (attention) scheme in parallel.</p><h2 id=transformer-network-architecture>Transformer Network Architecture<a hidden class=anchor aria-hidden=true href=#transformer-network-architecture>#</a></h2><p>Alright, let&rsquo;s do a quick recap:</p><ul><li>We have dealt with the loss of temporal structure by using positional encodings.</li><li>We have defined a way which allows nodes to communicate with each other, and learn to which of their friends to pay attention to.</li><li>We have done this $h$ times, the number of heads, to allow the nodes to ask different combinations of questions and answers: What do you mean? Where are you? Etc.</li></ul><p>The output of this is some encoding of the input that has all this information clumped together into a super rich embedding of the inputs. This is what we call the encoder part:</p><figure><img loading=lazy src=/images/trans-6.png alt="Attention is All You Need" width=50%><figcaption><p><a href=https://arxiv.org/abs/1706.03762>Attention is All You Need</a></p></figcaption></figure><p>The three arrows in the encoder part (left) that go into the Multi-Head Attention component are the three query, keys and values matrices $Q, K, V$ for each of the heads. Remember, we learn $Q, K, V$ via optimization; and we have as many of these representations as we have heads. An additional step shown in the figure is that we add a skip connection with normalization; similar to how we implemented skip-connections in U-Nets.</p><p>What about the decoder? The decoder will take the inputs, but shifted to the right for each context length, and learn new $Q, K, V$ representations from the training labels. In machine translation, these are $Q, K, V$ in English instead of French. It will then be able to get its own questions $Q$ in English, and allow it to reach out into the encoder for finding keys and values. This is sometimes called the cross-attention module. After this, we run the embeddings through a feed-forward layer to select the most important features and generate the softmax probabilities for the next token in the prediction.</p><h2 id=more-information>More Information<a hidden class=anchor aria-hidden=true href=#more-information>#</a></h2><p>I personally feel like the transformer content was an afterthought in the course. Compared to other content, the transformer content was very shallow and short. There are many amazing communicators that talk about transformers, here are some that I found helpful:</p><ol><li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Andrej Karpathy | Let&rsquo;s build GPT: from scratch, in code, spelled out.</a></li><li><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&amp;t=672s">CS25 I Stanford Seminar - Transformers United 2023: Introduction to Transformers w/ Andrej Karpathy</a></li><li><a href="https://youtu.be/ySEx_Bqxvvo?t=2893">MIT 6.S191: Recurrent Neural Networks, Transformers, and Attention</a></li><li><a href="https://www.youtube.com/watch?v=t45S_MwAcOw">Transformer models and BERT model: Overview</a></li><li><a href=https://arxiv.org/abs/1706.03762>Attention is All You Need</a></li></ol><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&amp;t=672s">CS25 | Stanford Seminar</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://manmartgarc.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=https://manmartgarc.github.io/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=prev href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week3/><span class=title>« Prev</span><br><span>Sequence Models: Week 3 | Sequence Models & Attention Mechanism</span>
</a><a class=next href=https://manmartgarc.github.io/posts/tech-support/go-lambda/><span class=title>Next »</span><br><span>Lambda Functions with Go and CDK</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://manmartgarc.github.io/>Manuel Martinez</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>