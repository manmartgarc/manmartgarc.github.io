<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Sequence Models: Week 1 | Recurrent Neural Networks | Manuel Martinez</title><meta name=keywords content="machine learning,deep learning"><meta name=description content="This is the first week of the fifth course of DeepLearning.AI&rsquo;s Deep Learning Specialization offered on Coursera. This week we go over some motivation for sequence models. These are models that are designed to work with sequential data, otherwise known as time-series. Let&rsquo;s get started.
This week&rsquo;s topics are:

Why Sequence Models?
Notation

Representing Words


Recurrent Neural Network

Forward Propagation
Different Types of RNNs
Language Model and Sequence Generation
Vanishing Gradients with RNNs


Gated Recurrent Unit
Long Short-Term Memory
Bidirectional RNN


Why Sequence Models?
Time-series get to be their own thing, just like in regression analysis. This time, since we are focusing on prediction instead of inference, we are less concerned about the statistical properties of the parameters we estimate, but we&rsquo;d like our models to do very well in their prediction tasks. But how can we exploit temporal information, without using classical methods such as AR methods? The current bag of tricks we have developed so far will only take us some distance. Here are a couple of hiccups:"><meta name=author content="Manuel Martinez"><link rel=canonical href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week1/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://manmartgarc.github.io/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://manmartgarc.github.io/images/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://manmartgarc.github.io/images/favicon.ico><link rel=apple-touch-icon href=https://manmartgarc.github.io/apple-touch-icon.png><link rel=mask-icon href=https://manmartgarc.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week1/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:url" content="https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week1/"><meta property="og:site_name" content="Manuel Martinez"><meta property="og:title" content="Sequence Models: Week 1 | Recurrent Neural Networks"><meta property="og:description" content="This is the first week of the fifth course of DeepLearning.AI’s Deep Learning Specialization offered on Coursera. This week we go over some motivation for sequence models. These are models that are designed to work with sequential data, otherwise known as time-series. Let’s get started.
This week’s topics are:
Why Sequence Models? Notation Representing Words Recurrent Neural Network Forward Propagation Different Types of RNNs Language Model and Sequence Generation Vanishing Gradients with RNNs Gated Recurrent Unit Long Short-Term Memory Bidirectional RNN Why Sequence Models? Time-series get to be their own thing, just like in regression analysis. This time, since we are focusing on prediction instead of inference, we are less concerned about the statistical properties of the parameters we estimate, but we’d like our models to do very well in their prediction tasks. But how can we exploit temporal information, without using classical methods such as AR methods? The current bag of tricks we have developed so far will only take us some distance. Here are a couple of hiccups:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-08T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-08T00:00:00+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Sequence Models: Week 1 | Recurrent Neural Networks"><meta name=twitter:description content="This is the first week of the fifth course of DeepLearning.AI&rsquo;s Deep Learning Specialization offered on Coursera. This week we go over some motivation for sequence models. These are models that are designed to work with sequential data, otherwise known as time-series. Let&rsquo;s get started.
This week&rsquo;s topics are:

Why Sequence Models?
Notation

Representing Words


Recurrent Neural Network

Forward Propagation
Different Types of RNNs
Language Model and Sequence Generation
Vanishing Gradients with RNNs


Gated Recurrent Unit
Long Short-Term Memory
Bidirectional RNN


Why Sequence Models?
Time-series get to be their own thing, just like in regression analysis. This time, since we are focusing on prediction instead of inference, we are less concerned about the statistical properties of the parameters we estimate, but we&rsquo;d like our models to do very well in their prediction tasks. But how can we exploit temporal information, without using classical methods such as AR methods? The current bag of tricks we have developed so far will only take us some distance. Here are a couple of hiccups:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://manmartgarc.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Sequence Models: Week 1 | Recurrent Neural Networks","item":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Sequence Models: Week 1 | Recurrent Neural Networks","name":"Sequence Models: Week 1 | Recurrent Neural Networks","description":"This is the first week of the fifth course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. This week we go over some motivation for sequence models. These are models that are designed to work with sequential data, otherwise known as time-series. Let\u0026rsquo;s get started.\nThis week\u0026rsquo;s topics are:\nWhy Sequence Models? Notation Representing Words Recurrent Neural Network Forward Propagation Different Types of RNNs Language Model and Sequence Generation Vanishing Gradients with RNNs Gated Recurrent Unit Long Short-Term Memory Bidirectional RNN Why Sequence Models? Time-series get to be their own thing, just like in regression analysis. This time, since we are focusing on prediction instead of inference, we are less concerned about the statistical properties of the parameters we estimate, but we\u0026rsquo;d like our models to do very well in their prediction tasks. But how can we exploit temporal information, without using classical methods such as AR methods? The current bag of tricks we have developed so far will only take us some distance. Here are a couple of hiccups:\n","keywords":["machine learning","deep learning"],"articleBody":"This is the first week of the fifth course of DeepLearning.AI’s Deep Learning Specialization offered on Coursera. This week we go over some motivation for sequence models. These are models that are designed to work with sequential data, otherwise known as time-series. Let’s get started.\nThis week’s topics are:\nWhy Sequence Models? Notation Representing Words Recurrent Neural Network Forward Propagation Different Types of RNNs Language Model and Sequence Generation Vanishing Gradients with RNNs Gated Recurrent Unit Long Short-Term Memory Bidirectional RNN Why Sequence Models? Time-series get to be their own thing, just like in regression analysis. This time, since we are focusing on prediction instead of inference, we are less concerned about the statistical properties of the parameters we estimate, but we’d like our models to do very well in their prediction tasks. But how can we exploit temporal information, without using classical methods such as AR methods? The current bag of tricks we have developed so far will only take us some distance. Here are a couple of hiccups:\nVanilla neural networks require all training samples $X$ to be of the same length. What happens when we want to use text as input and each sample can have different length? Sure we can pad, but I hope you’re convinced that’s not a great idea from the get go. Vanilla neural networks treat every feature $x^{(i)}_i$ as independent. The whole point of time-series is that they have two components: time-invariant and time-variant components. We definitely want to exploit the temporal structure of our data, which the vanilla models won’t allow us to. Vanilla neural networks with dense layers explode very quickly in their size. We need some approach, similar to that of CNNs, where parameters are shared to have a feasible approach in terms of computation. Before we get to the thick of it, let’s define the notation used in the course and disambiguate some terminology as well.\nNotation Our input will be, as usual, some vector of features. We denote each time step with $t$, so that $x^{}$ is the $t_{th}$ element in our input vector. We also denote the $i_{th}$ training example as $x^{(i)}$, so that $x^{(i)}$ denotes the $i_{th}$ training example’s $t_{th}$ time step. Similarly, we will also denote the size of our input with the variable $T_x$. All of this applies also to our labels $y$. Sometimes $T_x = T_y$, and sometimes it’s not. We will discuss these variations under the different types of RNNs.\nRepresenting Words Since a lot of sequence models deal with words, we need a handy way of representing words and even sentences. We need to start by defining a vocabulary. A vocabulary is simply the set of all words in our corpus. A corpus is natural-language processing equivalent to a dataset. If we are training a model on data scraped from news sites, then all the news articles we scraped will make our corpus.\nSince we have a set of all words in our corpus, more precisely tokens in our corpus, we can assign each word a number. This number is the position of the word in our vocabulary. For example, the word aaron might be in position $2$, while the word zulu might be in position $10,000$. This is assuming that our vocabulary is sorted and that it’s size, or cardinality is $10,000$.\nWe can now transform every word in a sentence into a one-hot vector. This simply means creating a vector of size equal to our vocabulary, that is $10,000$ and fill it with zeros. Next, we get the index of a word from our vocabulary, and set that index’s element in our vector to $1$. So that every word is represented by a $10,000$ dimensional vector where all the entries are $0$ except for the position that maps to a word in our vocabulary. More precisely, we can represent each element in our input $x^{}$ as a one-hot vector. Thus, a sentence can be represented by a matrix, where we concatenate each word. If we have a sentence with $20$ words, then our representation will be of dimensions $(10000, 20)$.\nSince we can also represent some output sequence $y$ in the same fashion, we will use supervised learning to learn a mapping that goes from $x \\to y$.\nAs we mentioned before, there are two main issues with using a standard neural network approach with sequence data.\nInput and outputs can be of different lengths between them and also across training examples. Does not share features across different positions of the sequence. Every element is processed independently of another, which is a pretty bad approach since we are dealing with data that is intrinsically linked in time. We can solve both these issues using a surprisingly simple model, a recurrent neural network (RNN).\nRecurrent Neural Network RNNs is an architectural approach that allows us to express our desire to share parameters, and also to generalize input and output lengths. Let’s define that.\nAt each step an RNN will take some input $x^{}$, combine it with the activations (hidden state) from the previous time step $a^{}$, and generate $a^{}$. It will use $a^{}$ to generate some output $\\hat{y}^{}$ and also pass $a^{}$ to the next layer $l^{}$ to repeat the process again. This means that at each time step we are putting in an input, combining it with the activations from the previous layer and generating an output; so on and so forth until we reach the end of the time steps. Andrew mentions that a lot of the diagrams from RNNs, and later GRUs and LSTM networks, are hard to understand. In that spirit I will focus more on the equations because they are clearer to me.\nA key thing to take note of is that we are sharing parameters across all time steps. There are three important parameter matrices: $W_{ax}$, $W_{aa}$ and $W_{ay}$. In the notation of the course, the subscripts $W_{ax}$ denote that this particular $W$ is being used to generate $a$, and it’s using $x$. This means that $W_{ax}$ is used to combine the inputs $x^{}$ and generate $a^{}$. Similarly, $W_{aa}$ is also used to generate $a^{}$, but it uses $a^{}$, the activations from the previous time step. This will become clearer once we go over the equations. However, just keep in mind that we have three knobs (parameter matrices):\n$W_{ax}$: regulates how each $x^{}$ affects the activations $a^{}$. $W_{aa}$: regulates how the activations from the previous layer $a^{}$ affect our current activations $a^{}$. $W_{ay}$: regulates how the activations $a^{}$ are combined into the current output $\\hat{y}^{}$. Notice that these three matrices are reused at every time step. This means that our parameters will not grow or shrink with respect to the length of our output or input sequences.\nAlso notice the main point of RNNs: as long as $t \u003e 1$ then the output $y^{}$ will be computed using all time steps $t_1, \\dots, t_k$. That is, the future is affected by the past (but not the other way around). If you’re interested doing away with the arrow of time, you can use bidirectional RNNs, but that comes later.\nForward Propagation Let’s finally define the equations that govern the forward pass in an RNN and hopefully do away with ambiguities in the writing.\nWe said that at every time step, an RNN combines the hidden state from the previous time step with the current inputs to generate an output. How do we get started? That is, what do at $t = 0$? We initialize the hidden state, $a^{}$ to be the zero vector, therefore $a^{\u003c0\u003e} = \\vec{0}$. Thus, we can define the general form of the forward propagation equation:\n$$ \\begin{aligned} a^{} \u0026= g_a (W_{aa} a^{} + W_{ax} x^{} + b_a) \\\\ \\hat{y}^{} \u0026= g_y (W_{ya} a^{} + b_y) \\end{aligned} $$\nNotice the activation functions $g_a, g_y$ might be different. In practice, we use the $\\tanh$ for $g_a$ and the sigmoid for $g_y$ in the case of classification.\nThese two operations are all that’s going in an RNN, repeated for every time step of the output. As mentioned before, keep in mind that $T_x = T_y$ for now.\nAnother thing done in the course is to simplify the notation of the equation above, by concatenating $W_{aa}$ and $W_{ax}$ together. For example, if $W_{aa}$’s dimensions are $(100, 100)$ and $W_{ax}$’s dimensions are $(100, 10000)$, then we can concatenate these into $W_a$. So that:\n$$ \\underset{(100, 10100)}{W_a} = \\left[\\underset{(100, 100)}{W_{aa}} | \\underset{(100, 10000)}{W_{ax}}\\right] $$\nSimilarly, we can stack $a^{}$ and $x^{}$ to match the dimensions of $W_a$. So that:\n$$ \\underset{(10100, 1)}{\\left[ a^{}, x^{}\\right]} = \\begin{bmatrix} \\underset{(100, 1)}{a^{}} \\\\ \\hline \\\\ \\underset{(10000, 1)}{x^{}} \\end{bmatrix} $$\nThis way we can simplify the activation function to:\n$$ a^{} = g_a (W_a \\left[ a^{}, x^{}\\right] + b_a) $$\nWhich is what we’ll use through the rest of the week. Notice that this is just a notational trick and also to keep all hidden state parameters together into a single matrix. We are not adding or removing any parameters by doing this notational refinement.\nDifferent Types of RNNs It turns out that setting $T_x = T_y$ is pretty restrictive. We can have different types of RNNs that have different sizes for their input and output. We have the following:\nMany-to-one approach: In sentiment classification we take a sequence and want a single output. We do this by editing the usual RNN approach. Instead of having an output at every time step, $\\hat{y}^{}$, we only have one at the end. A key thing is that we still keep the hidden state going through each time step like a regular RNN. One-to-many approach: In a case like music generation, we might input a single thing, such as a chord, and would like to generate a sequence based on this. This is the mirror opposite of the many-to-one approach. We still have one output at each time step $\\hat{y}^{}$, but we only have one input at the first time step. Many-to-many approach: In this approach, we have two cases: $T_x = T_y$: This is the usual approach. Each RNN unit takes in an input and produces an output. $T_x \\neq T_y$: This approach is usually used in machine translation. The idea is that you first have an encoder, and then a decoder. The encoder takes the inputs and learns an encoding of the inputs, still passing the hidden-state forward in time. While the decoder generates the outputs. Here is an image from the course:\nTypes of RNNs\nLanguage Model and Sequence Generation Language modeling is a probabilistic model of natural language that can generate probabilities of a series of words, based on text corpora in one or multiple languages it was trained on. 1 This means that for a given sentence, our a language model should be able to estimate the probability of that sentence occurring in our corpus. Notice that we can estimate probabilities for sentences that were not seen during training.\nIf we have a sentence such as “Cats average 15 hours of sleep a day.”, then we can model each token as a one-hot vector. Our tokens will be [cats, average, 15, hours, of, sleep, a, day, ], where is a special token that denotes the end of sentence. This is useful, because we want our model to be able to learn when it’s likely for a sentence to end. We can see how the RNN approach can be used almost directly to estimate the probabilities of this sentence.\nImagine that we train an RNN with the same settings as before, and that we trained it on a large English corpus. Also keep in mind that we want to set $x^{} = y^{}$. What happens in the first time step? We just set $x^{\u003c1\u003e}$ to be $\\vec{0}$. If we run this through the first RNN unit, we will get a softmax output over the entire vocabulary. That is, $P(x) \\forall x \\in V$ where $V$ is our vocabulary. We can grab the most likely word, or sample the vocabulary used the softmax probabilities, and generate $x^{\u003c2\u003e}$ from what we just estimated from $y^{\u003c1\u003e}$, so on and so forth.\nThe important idea behind this is that, using an RNN like this amounts to calculating the conditional probabilities for each of the words in the sentence. For example, $y^{\u003c1\u003e} = P(y^{\u003c1\u003e})$, $y^{\u003c2\u003e} = P(y^{\u003c2\u003e} | y^{\u003c1\u003e})$ and so on. When training the neural network, we use the following loss function:\n$$ J\\left(\\hat{y}^{}, y^{}\\right) = - \\sum_{i=1}^m y_i^{} \\log \\hat{y}_i^{} $$\nWhich is the cross-entropy loss we are already familiar with. Therefore, our cost function is just the sum over all time steps:\n$$ \\mathcal{L} = \\sum_{t=1}^{T_y} J^{}\\left(\\hat{y}^{}, y^{}\\right) $$\nThis means that the model learns how to assign probabilities to sentences using conditional probability. For example the second output $\\hat{y}^{\u003c2\u003e} = P(y^{\u003c1\u003e}) P(y^{\u003c2\u003e} | y^{\u003c1\u003e})$, the third output gives us $\\hat{y}^{\u003c3\u003e} = P(y^{\u003c3\u003e} | y^{\u003c1\u003e}, y^{\u003c2\u003e})$. Since we are computing the loss with the sum of cross entropy, we are multiplying all these probabilities (summing in log space) to get the conditional probability of all these words together, given the ones that came before. This is why we can use an RNN as a language model. Even more so, we can generate novel sequences by using the output from one time step into the next step. Notice that we didn’t do this just now, we passed the target label from the previous time step to the next input time step.\nVanishing Gradients with RNNs It turns out that vanilla RNNs suffer greatly from vanishing gradient problems. This might not be surprising since each RNN unit is like a layer, and therefore part of the chain rule when we do back propagation. In English, this means that the network is not very good at capturing long-term dependencies in a sequence. For example, in the sentence “The cat, which already ate an apple, was full”, when compared to “The cats, which already ate an apple, were full” has a long-term dependency. If it’s not clear, the dependency is between the pair $(\\text{cat}, \\text{was})$, and $(\\text{cats}, \\text{were})$. We still have not gotten to bidirectional RNNs, so the network is only scanning from left to right. This means that our optimization algorithm will have a harder and harder time updating the weights of the elements in the sequence that are further from the output towards the beginning of the sequence. This is what we mean by vanishing gradients in this problem.\nIn practice, a vanilla RNN implementation has high local dependencies. This means that some output at time step $t$ will be greatly influenced by the activations from the neighboring time steps before, but not much, if at all, by the ones further behind. This is of course, a problem. We want our language model to realize that seeing either cat or cats should affect the probabilities in the rest of the prediction, and also to memorize this fact as it keeps going down the sequence.\nWe can also have exploding gradients in some cases, which we can ameliorate with gradient clipping. That is, we restrict the gradient values to be within some arbitrary range chosen by us.\nGated Recurrent Unit The Gated Recurrent Unit (GRU) is an improvement over vanilla RNNs that deals with the problem we just discussed, that of vanishing gradients, and therefore the lack of long-term dependency recognition. Before we define the GRU approach, let’s go back to how the regular RNN calculated the activations for some time step $t$:\n$$ a^{} = \\tanh (W_a \\left[ a^{}, x^{}\\right] + b_a) $$\nWhere we simply replaced $g_a$ to use the $\\tanh$ activation function. Let’s talk about what $W_a$ is doing. The part that’s multiplied with $a^{}$ are the parameters that regulate how much the hidden state influences the activation, while the part that’s multiplied with $x^{}$ are the parameters that regulate how much each new input sequence influences the activation. That is, in a vanilla RNN each activation is a combination of the hidden state from the previous time step, and the current time step’s input.\nWhat the GRU does, is that it allows us to parametrize the mixture between hidden states and the activation; this is on top of $W_a$. The mechanism that performs the mixture is the gate, which is where the name comes from. The gate can be full, closed or anything in between, think sigmoid. When it’s open, the mixture from the past is heavy, when it’s closed, the mixture from the past is light. Let’s clarify this with math.\nLet’s say that $c$ is a memory cell, and that in this case $c^{} = a^{}$. We will also define a candidate memory cell $\\tilde{c}^{}$:\n$$ \\tilde{c}^{} = \\tanh (W_c \\left[ c^{}, x^{}\\right] + b_c) $$\nSo far it’s the same as before but with other variable names. Now, we introduce the update gate:\n$$ \\Gamma_u = \\sigma (W_u \\left[ c^{}, x^{}\\right] + b_u) $$\nLet’s keep in mind some properties of $\\Gamma_u$:\n$W_u$ are the parameters that we learn by optimization. The sigmoid guarantees values between $0$ and $1$. The gate has the same dimensions as the hidden state $c^{}$. This means that element-wise multiplication can be used. This is similar to applying a mask, but instead of a binary one, a continuous one because of the sigmoid activation. If the value of the gate at some position is close to $0$, then the product is close to $0$ which dampens the effect of the previous hidden state into the next time stamp. On the other hand, we can heighten the hidden state’s effect into the next time stamp. We’d like to add this gate to the $c^{}$ equation. We can do this by:\n$$ c^{} = \\Gamma_u \\tilde{c}^{} + (1 - \\Gamma_u) c^{} $$\nSo that at every activation, we are generating the candidate, and applying the gate over it; thus generating the final activations for the current unit. This means that a GRU not only learns how to mix the past and the present, but also how some tokens or inputs can be “context switches”; and also how some are not context switches. This means that the model can learn longer-term structural information over a sequence. If there’s a token that conditions the rest of the sequence, then the model can learn to update or open the gate. Otherwise, it can learn to keep the gate shut, thus enlarging or diminishing the effect from the past.\nA key thing to note is the element-wise application of the gate. This means that the gate operates at the dimension level, instead of the input level. This means that the network can selectively update some dimensions more aggressively than other dimensions.\nIt turns out that the actual GRU unit has two gates, instead of one; but it’s literally the same principle. The full GRU equations are:\n$$ \\begin{aligned} \\Gamma_r \u0026= \\sigma (W_r \\left[ c^{}, x^{}\\right] + b_r) \\\\ \\tilde{c}^{} \u0026= \\tanh (W_c \\left[\\Gamma_r c^{}, x^{}\\right] + b_c) \\\\ \\Gamma_u \u0026= \\sigma (W_u \\left[ c^{}, x^{}\\right] + b_u) \\\\ c^{} \u0026= \\Gamma_u \\tilde{c}^{} + (1 - \\Gamma_u) c^{} \\end{aligned} $$\nThe gate $\\Gamma_r$ is the “relevance” gate. It’s a parameter that regulates how relevant the previous memory cell $c^{}$ is in the next time step. While $\\Gamma_u$ is the “update” gate. A parameter that regulates how relevant our candidate memory cell is relative to the previous memory cell.\nNote that in the literature $c$ and $\\tilde{c}$ are usually referred as $h$ and $\\tilde{h}$. Similarly, $\\Gamma_u$ and $\\Gamma_r$ are referred to as $u$ and $r$ respectively.\nLong Short-Term Memory Long Short-Term Memory (LSTM) units are the extension of the GRU idea, even though LSTMs were published earlier. Let’s start by reviewing the equations for GRUs since the LSTM is a small extension of them.\nFor the GRU we have:\nCandidate for replacing the current memory cell: $\\tilde{c}^{} = \\tanh (W_c \\left[\\Gamma_r c^{}, x^{}\\right] + b_c)$ The relevance gate: $\\Gamma_r = \\sigma (W_r \\left[ c^{}, x^{}\\right] + b_r)$ The update gate: $\\Gamma_u = \\sigma (W_u \\left[ c^{}, x^{}\\right] + b_u)$ The current memory cell: $c^{} = \\Gamma_u \\tilde{c}^{} + (1 - \\Gamma_u) c^{}$ Which is a mixture of the current memory cell and the candidate memory cell mixed by the update gate. $a^{} = c^{}$ This is something that will change in the LSTM. In the LSTM we get rid of $\\Gamma_r$, but we introduce two other gates: $\\Gamma_f$ and $\\Gamma_o$, the forget gate and output gate respectively.\nFor the LSTM we have:\n$$ \\tilde{c}^{} = \\tanh (W_c \\left[a^{}, x^{}\\right] + b_c) $$\nNotice how we are back to using $a^{}$ instead of $c^{}$, and also that we dropped $\\Gamma_r$ which we will redefine in a second.\nThe update gate remains the same, albeit with the input change:\n$$ \\Gamma_u = \\sigma \\left(W_u[a^{}, x^{}] + b_u\\right) $$\nWe dropped $\\Gamma_r$ but only to redefine it more explicitly as $\\Gamma_f$, the forget gate:\n$$ \\Gamma_f = \\sigma \\left(W_f[a^{}, x^{}] + b_f\\right) $$\nWe will add yet another gate, same as the others, $\\Gamma_o$ the output gate:\n$$ \\Gamma_o = \\sigma \\left(W_o[a^{}, x^{}] + b_o\\right) $$\nThe current memory cell will now use both $\\Gamma_u$ and $\\Gamma_f$ instead of $\\Gamma_u$ and $(1 - \\Gamma_u)$:\n$$ c^{} = \\Gamma_u \\tilde{c}^{} + \\Gamma_f c^{} $$\nFinally, we use the output gate $\\Gamma_o$ to generate the final activations:\n$$ a^{} = \\Gamma_o c^{} $$\nWhat does the LSTM get us for the extra size? It gets us more complexity and adaptability. By using more gates, and by keeping the hidden state and the output separate, we are subdividing the tasks more finely than in the GRU; therefore being able to achieve greater specialization.\nBidirectional RNN Bidirectional RNNs (BRRNs) are simply running time forward and backward before doing a prediction. That is, before generating the first output, we’ve seen the entire sequence. By keeping two hidden states, one for forward in time, and the other for backwards in time, and then combining both these states with the input, we can use information from the future to make a prediction now. Computationally it’s almost two times as expensive as a single RNN.\nIn practice many people use bidirectional GRUs or LSTMs to also allow the model to parametrize the mixture over longer periods of time.\nNext week’s post is here.\nLanguage Model | Wikipedia ↩︎\n","wordCount":"3718","inLanguage":"en","datePublished":"2023-08-08T00:00:00Z","dateModified":"2023-08-08T00:00:00Z","author":{"@type":"Person","name":"Manuel Martinez"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week1/"},"publisher":{"@type":"Organization","name":"Manuel Martinez","logo":{"@type":"ImageObject","url":"https://manmartgarc.github.io/images/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://manmartgarc.github.io/ accesskey=h title="Manuel Martinez (Alt + H)">Manuel Martinez</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://manmartgarc.github.io/about/ title=About><span>About</span></a></li><li><a href=https://manmartgarc.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://manmartgarc.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://manmartgarc.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://manmartgarc.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://manmartgarc.github.io/cv/ title=CV><span>CV</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://manmartgarc.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://manmartgarc.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Sequence Models: Week 1 | Recurrent Neural Networks</h1><div class=post-meta><span title='2023-08-08 00:00:00 +0000 UTC'>August 8, 2023</span>&nbsp;·&nbsp;<span>18 min</span>&nbsp;·&nbsp;<span>Manuel Martinez</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#why-sequence-models>Why Sequence Models?</a></li><li><a href=#notation>Notation</a><ul><li><a href=#representing-words>Representing Words</a></li></ul></li><li><a href=#recurrent-neural-network>Recurrent Neural Network</a><ul><li><a href=#forward-propagation>Forward Propagation</a></li><li><a href=#different-types-of-rnns>Different Types of RNNs</a></li><li><a href=#language-model-and-sequence-generation>Language Model and Sequence Generation</a></li><li><a href=#vanishing-gradients-with-rnns>Vanishing Gradients with RNNs</a></li></ul></li><li><a href=#gated-recurrent-unit>Gated Recurrent Unit</a></li><li><a href=#long-short-term-memory>Long Short-Term Memory</a></li><li><a href=#bidirectional-rnn>Bidirectional RNN</a></li></ul></nav></div></details></div><div class=post-content><p>This is the first week of the <a href="https://www.coursera.org/learn/nlp-sequence-models?specialization=deep-learning/">fifth course</a> of DeepLearning.AI&rsquo;s <a href=https://www.coursera.org/specializations/deep-learning>Deep Learning Specialization</a> offered on Coursera. This week we go over some motivation for sequence models. These are models that are designed to work with sequential data, otherwise known as time-series. Let&rsquo;s get started.</p><p>This week&rsquo;s topics are:</p><ul><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week1/#why-sequence-models>Why Sequence Models?</a></li><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week1/#notation>Notation</a><ul><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week1/#representing-words>Representing Words</a></li></ul></li><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week1/#recurrent-neural-network>Recurrent Neural Network</a><ul><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week1/#forward-propagation>Forward Propagation</a></li><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week1/#different-types-of-rnns>Different Types of RNNs</a></li><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week1/#language-model-and-sequence-generation>Language Model and Sequence Generation</a></li><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week1/#vanishing-gradients-with-rnns>Vanishing Gradients with RNNs</a></li></ul></li><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week1/#gated-recurrent-unit>Gated Recurrent Unit</a></li><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week1/#long-short-term-memory>Long Short-Term Memory</a></li><li><a href=/posts/coursera/deep-learning-specialization/sequence-models/week1/#bidirectional-rnn>Bidirectional RNN</a></li></ul><hr><h2 id=why-sequence-models>Why Sequence Models?<a hidden class=anchor aria-hidden=true href=#why-sequence-models>#</a></h2><p>Time-series get to be their own thing, just like in regression analysis. This time, since we are focusing on prediction instead of inference, we are less concerned about the statistical properties of the parameters we estimate, but we&rsquo;d like our models to do very well in their prediction tasks. But how can we exploit temporal information, without using classical methods such as AR methods? The current bag of tricks we have developed so far will only take us some distance. Here are a couple of hiccups:</p><ul><li>Vanilla neural networks require all training samples $X$ to be of the same length. What happens when we want to use text as input and each sample can have different length? Sure we can pad, but I hope you&rsquo;re convinced that&rsquo;s not a great idea from the get go.</li><li>Vanilla neural networks treat every feature $x^{(i)}_i$ as independent. The whole point of time-series is that they have two components: time-invariant and time-variant components. We definitely want to exploit the temporal structure of our data, which the vanilla models won&rsquo;t allow us to.</li><li>Vanilla neural networks with dense layers explode very quickly in their size. We need some approach, similar to that of CNNs, where parameters are shared to have a feasible approach in terms of computation.</li></ul><p>Before we get to the thick of it, let&rsquo;s define the notation used in the course and disambiguate some terminology as well.</p><h2 id=notation>Notation<a hidden class=anchor aria-hidden=true href=#notation>#</a></h2><p>Our input will be, as usual, some vector of features. We denote each time step with $t$, so that $x^{&lt;t>}$ is the $t_{th}$ element in our input vector. We also denote the $i_{th}$ training example as $x^{(i)}$, so that $x^{(i)&lt;t>}$ denotes the $i_{th}$ training example&rsquo;s $t_{th}$ time step. Similarly, we will also denote the <em>size</em> of our input with the variable $T_x$. All of this applies also to our labels $y$. Sometimes $T_x = T_y$, and sometimes it&rsquo;s not. We will discuss these variations under the <a href=/posts/coursera/deep-learning-specialization/sequence-models/week1/#different-types-of-rnns>different types of RNNs</a>.</p><h3 id=representing-words>Representing Words<a hidden class=anchor aria-hidden=true href=#representing-words>#</a></h3><p>Since a lot of sequence models deal with words, we need a handy way of representing words and even sentences. We need to start by defining a <em>vocabulary</em>. A vocabulary is simply the <em>set</em> of all words in our <a href=https://en.wikipedia.org/wiki/Text_corpus>corpus</a>. A corpus is natural-language processing equivalent to a dataset. If we are training a model on data scraped from news sites, then all the news articles we scraped will make our corpus.</p><p>Since we have a set of all words in our corpus, more precisely <a href=https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html>tokens</a> in our corpus, we can assign each word a number. This number is the position of the word in our vocabulary. For example, the word <code>aaron</code> might be in position $2$, while the word <code>zulu</code> might be in position $10,000$. This is assuming that our vocabulary is sorted and that it&rsquo;s size, or cardinality is $10,000$.</p><p>We can now transform every word in a sentence into a <a href=https://en.wikipedia.org/wiki/One-hot>one-hot vector</a>. This simply means creating a vector of size equal to our vocabulary, that is $10,000$ and fill it with zeros. Next, we get the <em>index</em> of a word from our vocabulary, and set that index&rsquo;s element in our vector to $1$. So that every word is represented by a $10,000$ dimensional vector where all the entries are $0$ except for the position that maps to a word in our vocabulary. More precisely, we can represent each element in our input $x^{&lt;t>}$ as a one-hot vector. Thus, a sentence can be represented by a matrix, where we concatenate each word. If we have a sentence with $20$ words, then our representation will be of dimensions $(10000, 20)$.</p><p>Since we can also represent some output sequence $y$ in the same fashion, we will use supervised learning to learn a mapping that goes from $x \to y$.</p><p>As we mentioned before, there are two main issues with using a standard neural network approach with sequence data.</p><ol><li>Input and outputs can be of different lengths between them and also across training examples.</li><li>Does not share features across different positions of the sequence. Every element is processed independently of another, which is a pretty bad approach since we are dealing with data that is intrinsically linked in time.</li></ol><p>We can solve both these issues using a <a href=http://karpathy.github.io/2015/05/21/rnn-effectiveness/>surprisingly simple model</a>, a recurrent neural network (RNN).</p><h2 id=recurrent-neural-network>Recurrent Neural Network<a hidden class=anchor aria-hidden=true href=#recurrent-neural-network>#</a></h2><p>RNNs is an architectural approach that allows us to express our desire to share parameters, and also to generalize input and output lengths. Let&rsquo;s define that.</p><p>At each <em>step</em> an RNN will take some input $x^{&lt;t>}$, combine it with the activations (hidden state) from the <em>previous</em> time step $a^{&lt;t-1>}$, and generate $a^{&lt;t>}$. It will use $a^{&lt;t>}$ to generate some output $\hat{y}^{&lt;t>}$ and also pass $a^{&lt;t>}$ to the next layer $l^{&lt;t+1>}$ to repeat the process again. This means that at each time step we are putting in an input, combining it with the activations from the previous layer and generating an output; so on and so forth until we reach the end of the time steps. Andrew mentions that a lot of the diagrams from RNNs, and later GRUs and LSTM networks, are hard to understand. In that spirit I will focus more on the equations because they are clearer to me.</p><p>A key thing to take note of is that we are <em>sharing</em> parameters across all time steps. There are three important parameter matrices: $W_{ax}$, $W_{aa}$ and $W_{ay}$. In the notation of the course, the subscripts $W_{ax}$ denote that this particular $W$ is being used to generate $a$, and it&rsquo;s using $x$. This means that $W_{ax}$ is used to combine the inputs $x^{&lt;t>}$ and generate $a^{&lt;t>}$. Similarly, $W_{aa}$ is also used to generate $a^{&lt;t>}$, but it uses $a^{&lt;t-1>}$, the activations from the previous time step. This will become clearer once we go over the equations. However, just keep in mind that we have three knobs (parameter matrices):</p><ol><li>$W_{ax}$: regulates how each $x^{&lt;t>}$ affects the activations $a^{&lt;t>}$.</li><li>$W_{aa}$: regulates how the activations from the <em>previous</em> layer $a^{&lt;t-1>}$ affect our current activations $a^{&lt;t>}$.</li><li>$W_{ay}$: regulates how the activations $a^{&lt;t>}$ are combined into the current output $\hat{y}^{&lt;t>}$.</li></ol><p>Notice that these three matrices are <em>reused</em> at every time step. This means that our parameters will not grow or shrink with respect to the length of our output or input sequences.</p><p>Also notice the main point of RNNs: as long as $t > 1$ then the output $y^{&lt;k>}$ will be computed using all time steps $t_1, \dots, t_k$. That is, the future is affected by the past (but not the other way around). If you&rsquo;re interested doing away with the arrow of time, you can use <a href=/posts/coursera/deep-learning-specialization/sequence-models/week1/#bidirectional-rnn>bidirectional RNNs</a>, but that comes later.</p><h3 id=forward-propagation>Forward Propagation<a hidden class=anchor aria-hidden=true href=#forward-propagation>#</a></h3><p>Let&rsquo;s finally define the equations that govern the forward pass in an RNN and hopefully do away with ambiguities in the writing.</p><p>We said that at every time step, an RNN combines the hidden state from the previous time step with the current inputs to generate an output. How do we get started? That is, what do at $t = 0$? We initialize the hidden state, $a^{&lt;t>}$ to be the zero vector, therefore $a^{&lt;0>} = \vec{0}$. Thus, we can define the general form of the forward propagation equation:</p><p>$$
\begin{aligned}
a^{&lt;t>} &= g_a (W_{aa} a^{&lt;t - 1>} + W_{ax} x^{&lt;t>} + b_a) \\
\hat{y}^{&lt;t>} &= g_y (W_{ya} a^{&lt;t>} + b_y)
\end{aligned}
$$</p><p>Notice the activation functions $g_a, g_y$ might be different. In practice, we use the $\tanh$ for $g_a$ and the sigmoid for $g_y$ in the case of classification.</p><p>These two operations are all that&rsquo;s going in an RNN, repeated for every time step of the output. As mentioned before, keep in mind that $T_x = T_y$ for now.</p><p>Another thing done in the course is to simplify the notation of the equation above, by concatenating $W_{aa}$ and $W_{ax}$ together. For example, if $W_{aa}$&rsquo;s dimensions are $(100, 100)$ and $W_{ax}$&rsquo;s dimensions are $(100, 10000)$, then we can concatenate these into $W_a$. So that:</p><p>$$
\underset{(100, 10100)}{W_a} = \left[\underset{(100, 100)}{W_{aa}} | \underset{(100, 10000)}{W_{ax}}\right]
$$</p><p>Similarly, we can stack $a^{&lt;t-1>}$ and $x^{&lt;t>}$ to match the dimensions of $W_a$. So that:</p><p>$$
\underset{(10100, 1)}{\left[ a^{&lt;t-1>}, x^{&lt;t>}\right]} = \begin{bmatrix}
\underset{(100, 1)}{a^{&lt;t-1>}} \\
\hline \\
\underset{(10000, 1)}{x^{&lt;t>}}
\end{bmatrix}
$$</p><p>This way we can simplify the activation function to:</p><p>$$
a^{&lt;t>} = g_a (W_a \left[ a^{&lt;t-1>}, x^{&lt;t>}\right] + b_a)
$$</p><p>Which is what we&rsquo;ll use through the rest of the week. Notice that this is just a notational trick and also to keep all hidden state parameters together into a single matrix. We are not adding or removing any parameters by doing this notational refinement.</p><h3 id=different-types-of-rnns>Different Types of RNNs<a hidden class=anchor aria-hidden=true href=#different-types-of-rnns>#</a></h3><p>It turns out that setting $T_x = T_y$ is pretty restrictive. We can have different types of RNNs that have different sizes for their input and output. We have the following:</p><ul><li><strong>Many-to-one approach:</strong> In sentiment classification we take a sequence and want a single output. We do this by editing the usual RNN approach. Instead of having an output at every time step, $\hat{y}^{&lt;t>}$, we only have one at the end. A key thing is that we still keep the hidden state going through each time step like a regular RNN.</li><li><strong>One-to-many approach:</strong> In a case like music generation, we might input a single thing, such as a chord, and would like to generate a sequence based on this. This is the mirror opposite of the many-to-one approach. We still have one output at each time step $\hat{y}^{&lt;t>}$, but we only have one input at the first time step.</li><li><strong>Many-to-many approach:</strong> In this approach, we have two cases:<ul><li>$T_x = T_y$: This is the usual approach. Each RNN unit takes in an input and produces an output.</li><li>$T_x \neq T_y$: This approach is usually used in machine translation. The idea is that you first have an encoder, and then a decoder. The encoder takes the inputs and learns an encoding of the inputs, still passing the hidden-state forward in time. While the decoder generates the outputs.</li></ul></li></ul><p>Here is an image from the course:</p><figure><img loading=lazy src=/images/rnn-1.png alt="Types of RNNs" width=75%><figcaption><p><a href="https://www.coursera.org/learn/nlp-sequence-models?specialization=deep-learning">Types of RNNs</a></p></figcaption></figure><h3 id=language-model-and-sequence-generation>Language Model and Sequence Generation<a hidden class=anchor aria-hidden=true href=#language-model-and-sequence-generation>#</a></h3><p>Language modeling is a probabilistic model of natural language that can generate probabilities of a series of words, based on text corpora in one or multiple languages it was trained on. <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> This means that for a given sentence, our a language model should be able to estimate the probability of that sentence occurring in our corpus. Notice that we can estimate probabilities for sentences that were not seen during training.</p><p>If we have a sentence such as &ldquo;Cats average 15 hours of sleep a day.&rdquo;, then we can model each token as a one-hot vector. Our tokens will be <code>[cats, average, 15, hours, of, sleep, a, day, &lt;eos>]</code>, where <code>&lt;eos></code> is a special token that denotes the end of sentence. This is useful, because we want our model to be able to learn when it&rsquo;s likely for a sentence to end. We can see how the RNN approach can be used almost directly to estimate the probabilities of this sentence.</p><p>Imagine that we train an RNN with the same settings as <a href=/posts/coursera/deep-learning-specialization/sequence-models/week1/#representing-words>before</a>, and that we trained it on a large English corpus. Also keep in mind that we want to set $x^{&lt;t>} = y^{&lt;t-1>}$. What happens in the first time step? We just set $x^{&lt;1>}$ to be $\vec{0}$. If we run this through the first RNN unit, we will get a softmax output over the entire vocabulary. That is, $P(x) \forall x \in V$ where $V$ is our vocabulary. We can grab the most likely word, or sample the vocabulary used the softmax probabilities, and generate $x^{&lt;2>}$ from what we just estimated from $y^{&lt;1>}$, so on and so forth.</p><p>The important idea behind this is that, using an RNN like this amounts to calculating the conditional probabilities for each of the words in the sentence. For example, $y^{&lt;1>} = P(y^{&lt;1>})$, $y^{&lt;2>} = P(y^{&lt;2>} | y^{&lt;1>})$ and so on. When training the neural network, we use the following loss function:</p><p>$$
J\left(\hat{y}^{&lt;t>}, y^{&lt;t>}\right) = - \sum_{i=1}^m y_i^{&lt;t>} \log \hat{y}_i^{&lt;t>}
$$</p><p>Which is the cross-entropy loss we are already familiar with. Therefore, our cost function is just the sum over all time steps:</p><p>$$
\mathcal{L} = \sum_{t=1}^{T_y} J^{&lt;t>}\left(\hat{y}^{&lt;t>}, y^{&lt;t>}\right)
$$</p><p>This means that the model learns how to assign probabilities to sentences using conditional probability. For example the second output $\hat{y}^{&lt;2>} = P(y^{&lt;1>}) P(y^{&lt;2>} | y^{&lt;1>})$, the third output gives us $\hat{y}^{&lt;3>} = P(y^{&lt;3>} | y^{&lt;1>}, y^{&lt;2>})$. Since we are computing the loss with the sum of cross entropy, we are multiplying all these probabilities (summing in log space) to get the conditional probability of all these words together, given the ones that came before. This is why we can use an RNN as a language model. Even more so, we can generate novel sequences by using the output from one time step into the next step. Notice that we didn&rsquo;t do this just now, we passed the <em>target label</em> from the previous time step to the next input time step.</p><h3 id=vanishing-gradients-with-rnns>Vanishing Gradients with RNNs<a hidden class=anchor aria-hidden=true href=#vanishing-gradients-with-rnns>#</a></h3><p>It turns out that vanilla RNNs suffer greatly from vanishing gradient problems. This might not be surprising since each RNN unit is like a layer, and therefore part of the chain rule when we do back propagation. In English, this means that the network is not very good at capturing <em>long-term dependencies</em> in a sequence. For example, in the sentence &ldquo;The cat, which already ate an apple, was full&rdquo;, when compared to &ldquo;The cats, which already ate an apple, were full&rdquo; has a long-term dependency. If it&rsquo;s not clear, the dependency is between the pair $(\text{cat}, \text{was})$, and $(\text{cats}, \text{were})$. We still have not gotten to bidirectional RNNs, so the network is only scanning from left to right. This means that our optimization algorithm will have a harder and harder time updating the weights of the elements in the sequence that are further from the output towards the beginning of the sequence. This is what we mean by vanishing gradients in this problem.</p><p>In practice, a vanilla RNN implementation has high local dependencies. This means that some output at time step $t$ will be greatly influenced by the activations from the neighboring time steps before, but not much, if at all, by the ones further behind. This is of course, a problem. We want our language model to realize that seeing either <code>cat</code> or <code>cats</code> should affect the probabilities in the rest of the prediction, and also to memorize this fact as it keeps going down the sequence.</p><p>We can also have exploding gradients in some cases, which we can ameliorate with gradient clipping. That is, we restrict the gradient values to be within some arbitrary range chosen by us.</p><h2 id=gated-recurrent-unit>Gated Recurrent Unit<a hidden class=anchor aria-hidden=true href=#gated-recurrent-unit>#</a></h2><p>The Gated Recurrent Unit (GRU) is an improvement over vanilla RNNs that deals with the problem we just discussed, that of vanishing gradients, and therefore the lack of long-term dependency recognition. Before we define the GRU approach, let&rsquo;s go back to how the regular RNN calculated the activations for some time step $t$:</p><p>$$
a^{&lt;t>} = \tanh (W_a \left[ a^{&lt;t-1>}, x^{&lt;t>}\right] + b_a)
$$</p><p>Where we simply replaced $g_a$ to use the $\tanh$ activation function. Let&rsquo;s talk about what $W_a$ is doing. The part that&rsquo;s multiplied with $a^{&lt;t-1>}$ are the parameters that regulate how much the hidden state influences the activation, while the part that&rsquo;s multiplied with $x^{&lt;t>}$ are the parameters that regulate how much each new input sequence influences the activation. That is, in a vanilla RNN each activation is a combination of the hidden state from the previous time step, and the current time step&rsquo;s input.</p><p>What the GRU does, is that it allows us to parametrize the mixture between hidden states and the activation; this is on top of $W_a$. The mechanism that performs the mixture is the gate, which is where the name comes from. The gate can be full, closed or anything in between, think sigmoid. When it&rsquo;s open, the mixture from the past is heavy, when it&rsquo;s closed, the mixture from the past is light. Let&rsquo;s clarify this with math.</p><p>Let&rsquo;s say that $c$ is a memory cell, and that in this case $c^{&lt;t>} = a^{&lt;t>}$. We will also define a <em>candidate</em> memory cell $\tilde{c}^{&lt;t>}$:</p><p>$$
\tilde{c}^{&lt;t>} = \tanh (W_c \left[ c^{&lt;t-1>}, x^{&lt;t>}\right] + b_c)
$$</p><p>So far it&rsquo;s the same as before but with other variable names. Now, we introduce the <em>update</em> gate:</p><p>$$
\Gamma_u = \sigma (W_u \left[ c^{&lt;t-1>}, x^{&lt;t>}\right] + b_u)
$$</p><p>Let&rsquo;s keep in mind some properties of $\Gamma_u$:</p><ul><li>$W_u$ are the parameters that we learn by optimization.</li><li>The sigmoid guarantees values between $0$ and $1$.</li><li>The gate has the same dimensions as the hidden state $c^{&lt;t-1>}$. This means that element-wise multiplication can be used.</li><li>This is similar to applying a mask, but instead of a binary one, a continuous one because of the sigmoid activation.</li><li>If the value of the gate at some position is close to $0$, then the product is close to $0$ which dampens the effect of the previous hidden state into the next time stamp.</li><li>On the other hand, we can heighten the hidden state&rsquo;s effect into the next time stamp.</li></ul><p>We&rsquo;d like to add this gate to the $c^{&lt;t>}$ equation. We can do this by:</p><p>$$
c^{&lt;t>} = \Gamma_u \tilde{c}^{&lt;t>} + (1 - \Gamma_u) c^{&lt;t-1>}
$$</p><p>So that at every activation, we are generating the candidate, and applying the gate over it; thus generating the final activations for the current unit. This means that a GRU not only learns how to mix the past and the present, but also how some tokens or inputs can be &ldquo;context switches&rdquo;; and also how some are not context switches. This means that the model can learn longer-term structural information over a sequence. If there&rsquo;s a token that conditions the rest of the sequence, then the model can learn to update or open the gate. Otherwise, it can learn to keep the gate shut, thus enlarging or diminishing the effect from the past.</p><p>A key thing to note is the element-wise application of the gate. This means that the gate operates at the dimension level, instead of the input level. This means that the network can selectively update some dimensions more aggressively than other dimensions.</p><p>It turns out that the actual GRU unit has two gates, instead of one; but it&rsquo;s literally the same principle. The full GRU equations are:</p><p>$$
\begin{aligned}
\Gamma_r &= \sigma (W_r \left[ c^{&lt;t-1>}, x^{&lt;t>}\right] + b_r) \\
\tilde{c}^{&lt;t>} &= \tanh (W_c \left[\Gamma_r c^{&lt;t-1>}, x^{&lt;t>}\right] + b_c) \\
\Gamma_u &= \sigma (W_u \left[ c^{&lt;t-1>}, x^{&lt;t>}\right] + b_u) \\
c^{&lt;t>} &= \Gamma_u \tilde{c}^{&lt;t>} + (1 - \Gamma_u) c^{&lt;t-1>}
\end{aligned}
$$</p><p>The gate $\Gamma_r$ is the &ldquo;relevance&rdquo; gate. It&rsquo;s a parameter that regulates how relevant the previous memory cell $c^{&lt;t-1>}$ is in the next time step. While $\Gamma_u$ is the &ldquo;update&rdquo; gate. A parameter that regulates how relevant our candidate memory cell is relative to the previous memory cell.</p><blockquote><p>Note that in the literature $c$ and $\tilde{c}$ are usually referred as $h$ and $\tilde{h}$. Similarly, $\Gamma_u$ and $\Gamma_r$ are referred to as $u$ and $r$ respectively.</p></blockquote><h2 id=long-short-term-memory>Long Short-Term Memory<a hidden class=anchor aria-hidden=true href=#long-short-term-memory>#</a></h2><p>Long Short-Term Memory (LSTM) units are the extension of the GRU idea, even though LSTMs were published earlier. Let&rsquo;s start by reviewing the equations for GRUs since the LSTM is a small extension of them.</p><p>For the GRU we have:</p><ul><li>Candidate for replacing the current memory cell: $\tilde{c}^{&lt;t>} = \tanh (W_c \left[\Gamma_r c^{&lt;t-1>}, x^{&lt;t>}\right] + b_c)$</li><li>The relevance gate: $\Gamma_r = \sigma (W_r \left[ c^{&lt;t-1>}, x^{&lt;t>}\right] + b_r)$</li><li>The update gate: $\Gamma_u = \sigma (W_u \left[ c^{&lt;t-1>}, x^{&lt;t>}\right] + b_u)$</li><li>The current memory cell: $c^{&lt;t>} = \Gamma_u \tilde{c}^{&lt;t>} + (1 - \Gamma_u) c^{&lt;t-1>}$<ul><li>Which is a mixture of the current memory cell and the candidate memory cell mixed by the update gate.</li></ul></li><li>$a^{&lt;t>} = c^{&lt;t>}$<ul><li>This is something that will change in the LSTM.</li></ul></li></ul><p>In the LSTM we get rid of $\Gamma_r$, but we introduce two other gates: $\Gamma_f$ and $\Gamma_o$, the forget gate and output gate respectively.</p><p>For the LSTM we have:</p><p>$$
\tilde{c}^{&lt;t>} = \tanh (W_c \left[a^{&lt;t-1>}, x^{&lt;t>}\right] + b_c)
$$</p><p>Notice how we are back to using $a^{&lt;t>}$ instead of $c^{&lt;t>}$, and also that we dropped $\Gamma_r$ which we will redefine in a second.</p><p>The update gate remains the same, albeit with the input change:</p><p>$$
\Gamma_u = \sigma \left(W_u[a^{&lt;t-1>}, x^{&lt;t>}] + b_u\right)
$$</p><p>We dropped $\Gamma_r$ but only to redefine it more explicitly as $\Gamma_f$, the forget gate:</p><p>$$
\Gamma_f = \sigma \left(W_f[a^{&lt;t-1>}, x^{&lt;t>}] + b_f\right)
$$</p><p>We will add yet another gate, same as the others, $\Gamma_o$ the output gate:</p><p>$$
\Gamma_o = \sigma \left(W_o[a^{&lt;t-1>}, x^{&lt;t>}] + b_o\right)
$$</p><p>The current memory cell will now use both $\Gamma_u$ and $\Gamma_f$ instead of $\Gamma_u$ and $(1 - \Gamma_u)$:</p><p>$$
c^{&lt;t>} = \Gamma_u \tilde{c}^{&lt;t>} + \Gamma_f c^{&lt;t-1>}
$$</p><p>Finally, we use the output gate $\Gamma_o$ to generate the final activations:</p><p>$$
a^{&lt;t>} = \Gamma_o c^{&lt;t>}
$$</p><p>What does the LSTM get us for the extra size? It gets us more complexity and adaptability. By using more gates, and by keeping the hidden state and the output separate, we are subdividing the tasks more finely than in the GRU; therefore being able to achieve greater specialization.</p><h2 id=bidirectional-rnn>Bidirectional RNN<a hidden class=anchor aria-hidden=true href=#bidirectional-rnn>#</a></h2><p>Bidirectional RNNs (BRRNs) are simply running time forward and backward before doing a prediction. That is, before generating the first output, we&rsquo;ve seen the entire sequence. By keeping two hidden states, one for forward in time, and the other for backwards in time, and then combining both these states with the input, we can use information from the future to make a prediction now. Computationally it&rsquo;s almost two times as expensive as a single RNN.</p><p>In practice many people use bidirectional GRUs or LSTMs to also allow the model to parametrize the mixture over longer periods of time.</p><p>Next week&rsquo;s post is <a href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week2/>here</a>.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://en.wikipedia.org/wiki/Language_model>Language Model | Wikipedia</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://manmartgarc.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=https://manmartgarc.github.io/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=prev href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week4/><span class=title>« Prev</span><br><span>Convolutional Neural Networks: Week 4 | Face Recognition & Neural Style Transfer</span>
</a><a class=next href=https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week2/><span class=title>Next »</span><br><span>Sequence Models: Week 2 | NLP & Word Embeddings</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://manmartgarc.github.io/>Manuel Martinez</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>