<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Manuel Martinez</title>
    <link>https://manmartgarc.github.io/posts/</link>
    <description>Recent content in Posts on Manuel Martinez</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://manmartgarc.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Structuring ML Projects: Week 1 | ML Strategy</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week-01/</link>
      <pubDate>Fri, 23 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week-01/</guid>
      <description>This is the first week of the third course of DeepLearning.AI&amp;rsquo;s Deep Learning Specialization offered on Coursera. This course is less technical than the previous two, and focuses instead on general principles and intuition related to machine learning projects.
This week&amp;rsquo;s topics are:
 Introduction to ML Strategy  Why ML Strategy Orthogonalization   Setting Up our Goal  Single Number Evaluation Metric Satisficing and Optimizing Metrics Train/Dev/Test Distributions Size of Dev and Test Sets When to Change Dev/Test Sets and Metrics?</description>
    </item>
    
    <item>
      <title>Improving Deep Learning Networks: Week 3 | Hyperparameter Tuning, Batch Optimization, Programming Frameworks</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/</link>
      <pubDate>Wed, 21 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/</guid>
      <description>This is the third and last week of the second course of DeepLearning.AI&amp;rsquo;s Deep Learning Specialization offered on Coursera.
This week&amp;rsquo;s topics are:
 Hyperparameter Tuning  Tuning Process  Random Search Coarse-to-fine Grained Search   Using an Appropriate Scale when Searching  Python Implementation   Hyperparameter Tuning in Practice: Pandas vs. Caviar   Batch Normalization  Normalizing Activations in a Network Fitting Batch Norm into a Neural Network Why does Batch Norm work?</description>
    </item>
    
    <item>
      <title>Improving Deep Learning Networks: Week 2 | Optimization Algorithms</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/</link>
      <pubDate>Mon, 19 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/</guid>
      <description>This week is focused on the optimization or training process. Particularly, this week goes over ways to make the training process faster and more efficient, allowing us to iterate quicker when trying different approaches.
This week&amp;rsquo;s topics are:
 Mini-batch Gradient Descent Exponentially Weighted Moving Averages (EWMA)  Bias Correction   Gradient Descent with Momentum RMSProp Adam Learning Rate Decay Summary   Mini-batch Gradient DescentLink to headingWe discussed gradient descent briefly on the second week of the first course.</description>
    </item>
    
    <item>
      <title>Improving Deep Learning Networks: Week 1 | Practical Aspects of Deep Learning</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/</link>
      <pubDate>Sun, 18 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/</guid>
      <description>This is the first week in the second course of DeepLearning.AI&amp;rsquo;s Deep Learning Specialization offered on Coursera. The course deals with hyperparameter tuning, regularization and the optimization of the training process. The optimization ranges from computational complexity to performance. This is again a pretty technical week where you will benefit a lot from doing the programming exercises.
This week&amp;rsquo;s topics are:
 Setting up our Machine Learning problem  Train / Dev / Test sets Bias-Variance Tradeoff Basic Recipe for Machine Learning   Regularizing our Neural Network  Regularization Why Does Regularization Reduce Overfitting?</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning: Week 4 | Deep Neural Networks</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/</link>
      <pubDate>Fri, 16 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/</guid>
      <description>Final week of this course. Again this week is pretty technical and a lot of the learning is done while coding up your own examples via the weekly programming assignments. The purpose of this week is to extend previous&#39; weeks ideas into $L$-layered networks.
This week&amp;rsquo;s topics are:
 Deep L-Layer neural network Getting your matrix dimensions right Why deep representations? Parameters and Hyperparameters Summary   Deep L-Layer neural networkLink to headingThe number of hidden layers in a neural network determine whether it is &amp;ldquo;shallow&amp;rdquo; or &amp;ldquo;deep&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/</link>
      <pubDate>Thu, 15 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/</guid>
      <description>This week&amp;rsquo;s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.
This week&amp;rsquo;s topics are:
 Overview Neural Network Representation Computing a Neural Network&amp;rsquo;s Output Vectorizing across multiple examples Activation functions Random Initialization Summary   OverviewLink to headingIt&amp;rsquo;s time to refine notation and to disambiguate some of the concepts introduced in week 2.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning: Week 2 | Neural Network Basics</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week2/</link>
      <pubDate>Wed, 14 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week2/</guid>
      <description>Here we kick off the second week of the first course in the specialization. This week is very technical and many of the details that are shown in the course will be lost in the summarization. Also, a lot of the content is based on you doing programming assignments. There is simply no substitute to getting your hands dirty.
This week&amp;rsquo;s topics are:
 Binary Classification Logistic Regression Logistic Function Gradient Descent Computation Graph Python and Vectorization Broadcasting Summary   Binary ClassificationLink to headingBinary classification is a supervised learning approach where you train what&amp;rsquo;s called a classifier.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning: Week 1 | Introduction to Deep Learning</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week1/</link>
      <pubDate>Tue, 13 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week1/</guid>
      <description>Introduction to Deep LearningLink to headingThis is the first course in Coursera&amp;rsquo;s Deep Learning Specialization. I will try to summarize the major topics presented in each of the weeks of each course in a series of posts. The purpose of this is to both deepen my own understanding by explaining, but also for people who have not taken the specialization. Hopefully these posts will inspire you to do so.</description>
    </item>
    
    <item>
      <title>VSCode and Tiny Instances over Remote SSH</title>
      <link>https://manmartgarc.github.io/posts/tech-support/vscode-ec2/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/tech-support/vscode-ec2/</guid>
      <description>VSCode is very popularLink to headingA lot of people like VSCode, 74% of respondents in StackOverflow&amp;rsquo;s 2022 Developer Survey said that VSCode is the IDE that they&amp;rsquo;ve used in the past year and also plan to keep using it. Why is VSCode liked so, and what do people like about it is not the focus of the post; but for sure it has to do with its extendable functionality, much of which comes from extensions.</description>
    </item>
    
    <item>
      <title>Application Mistakes and Information frictions in College Admissions</title>
      <link>https://manmartgarc.github.io/posts/mistakes/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/mistakes/</guid>
      <description>This is a working paper with Ana√Øs Fabre, Tomas Larroucau, Christopher Neilson and Ignacio Rios. The paper explores how information and beliefs have an effect on outcomes within the tertiary education market in Chile, which is a centralized college admissions system. The paper includes two waves of surveys done in 2019 and 2020 and an RCT done in 2021 with both government and NGOs. The abstract is below, and you can find the latest version of the working paper here.</description>
    </item>
    
  </channel>
</rss>
