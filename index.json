[{"content":"Introduction You\u0026rsquo;ve probably heard about the Model Context Protocol (MCP) by now ‚Äî an emerging standard for connecting AI agents to external data sources and tools.\nI wanted to explore building a serverless HTTP-based MCP server using the typical APIGW + Lambda pattern that is common in serverless applications, i.e. the way some MCP server provider might want to host MCP servers for public consumption. When thinking about which tools the MCP server should have, I wondered how hard it would be to convert an existing REST API into an MCP server - especially given some standard specifications like OpenAPI.\nThe rest of this post will describe how I built an MCP server for the OpenDota API using FastMCP and deployed it to AWS Lambda + API Gateway and some lessons learned along the way.\nIf you don\u0026rsquo;t know what Dota 2 is, it\u0026rsquo;s a popular multiplayer online battle arena (MOBA) game where two teams of five players compete to destroy each other\u0026rsquo;s bases. The OpenDota API provides free access to a wealth of Dota 2 statistics, including player data, match histories, hero stats, and more.\nMotivation What\u0026rsquo;s an MCP server? Throughout this post, I‚Äôll use agent to mean an AI system capable of discovering and invoking MCP tools, and user to mean the human interacting with that agent.\nAn MCP server is a service that implements the Model Context Protocol (MCP) specification, allowing AI agents to interact with external tools and data sources in a standardized way. MCP servers expose a unified interface that any compatible agent can use. This is part of the trend of decoupling AI agents from specific API implementations, enabling interoperability across platforms and allowing developers to build once and deploy everywhere and even switch AI backends without rewriting tool integrations.\nSome justifications for MCP servers are:\nEmpower LLM-based AI agents with tools that provide real-time data access since it might not be included in their training data. Think of a weather API or stock prices. Limiting some of the undefined behavior that comes from the next-token-prediction nature of LLMs by providing structured tool interfaces. For example, arithmetic calculations or database queries. Allowing AI agents to leverage specialized external services (e.g., search engines, knowledge bases) that can enhance their capabilities beyond the base LLM. This is especially useful for domain-specific applications or private. For example, an enterprise knowledge base that contains proprietary information not available in public datasets. Transports A key part of the MCP specification is the Transports mechanism, which defines how agents communicate with MCP servers. Initially, the stdio transport was the only option‚Äîagents spawn MCP servers as subprocesses and communicate via standard input/output. However, new mechanisms like HTTP have emerged for remote deployment, allowing MCP servers to be hosted in the cloud and scaled independently.\nFor teams building AI tooling at scale, treating MCP servers like standard cloud services‚Äîdeployed, monitored, and scaled with other infrastructure‚Äîmakes sense. HTTP transport is the natural choice here, enabling you to integrate MCP into a larger ecosystem of sessions, authentication, rate limiting, and observability.\nBut we already have a REST API (at home) There are obviously a trillion existing REST APIs out there - and I wanted to see how easy it would be to convert one of them into an MCP server. Specifically given that there are some API specifications such as OpenAPI that provide machine-readable definitions of REST APIs.\nIt turns out there are a couple of options across multiple languages:\nrmcp-openapi: Can generate MCP servers from OpenAPI specs in Rust fastmcp: A Python framework that can auto-generate MCP servers from OpenAPI specifications openapi-mcp: A Go library to create MCP servers from OpenAPI specs All of these tools do roughly the same thing:\nParse the OpenAPI specification to extract endpoints, parameters, and schemas. Map REST endpoints to MCP tools, converting HTTP methods and paths into tool names and descriptions. Generate the necessary MCP server code to handle tool invocations and translate them into REST API calls. I tried using rmcp-openapi, but ran into issues with the specification parsing; specifically the $ref mechanism. What ended up working was FastMCP\u0026rsquo;s OpenAPI integration, but still required some customizations to handle authentication removal and AWS Lambda deployment as we\u0026rsquo;ll see next.\nArchitecture The goal is to build an MCP server that exposes the entire OpenDota API without introducing any stateful or long-lived servers, allowing AI agents to query Dota 2 statistics programmatically. The MCP server should be a serverless HTTP service deployed to APIGW + Lambda using AWS CDK for infrastructure as code.\nA user can then interact with the MCP server using HTTP requests and any MCP-compatible AI agent.\nSequence Diagram The architecture is very straightforward:\nA Lambda function hosts the FastMCP server that exposes the OpenDota API as MCP tools. API Gateway provides an HTTP endpoint that routes requests to the Lambda function. AI agents can send HTTP requests to the API Gateway endpoint to invoke MCP tools. This project uses:\nFastMCP: Python framework that auto-generates MCP tools from OpenAPI specifications Mangum: ASGI adapter for running FastMCP on AWS Lambda AWS CDK: Infrastructure as code for deploying API Gateway + Lambda UV: Modern Python package manager for dependency bundling AWS Lambda Powertools: Lambda utilities (though Mangum was preferred for ASGI compatibility) Project Structure ‚îú‚îÄ‚îÄ lib/ # CDK stack definitions ‚îú‚îÄ‚îÄ python-lambda/ # Python MCP server code ‚îÇ ‚îú‚îÄ‚îÄ lambda_function.py # AWS Lambda handler with Mangum adapter ‚îÇ ‚îú‚îÄ‚îÄ pyproject.toml # Python de pendencies ‚îÇ ‚îî‚îÄ‚îÄ uv.lock # Locked dependencies for reproducible builds ‚îú‚îÄ‚îÄ cdk.json # CDK configuration ‚îú‚îÄ‚îÄ package.json # Node.js dependencies ‚îî‚îÄ‚îÄ README.md # Quick start \u0026amp; deployment instructions Is this a good idea? I want to focus on the discussion around whether this automatic conversion from OpenAPI to MCP tools is a good idea in practice. Feel free to skip my meandering thoughts and jump to the implementation details if you just want to see how it was built.\nThere\u0026rsquo;s a good write-up by Jeremiah Lowin, the creator of FastMCP, on why this approach can be problematic in practice. The gist of it is that while automatic conversion is convenient, it often leads to bloated and inefficient agent interfaces that are hard to use effectively. Definitely recommend reading his post for a deeper dive into this topic - I\u0026rsquo;ll just summarize a few key points here:\nPurpose: REST APIs often have dozens or hundreds of endpoints. These are designed to be consumed by applications with specific use cases in mind. In contrast, MCP tools are intended to be more general-purpose and composable, allowing for greater flexibility in how they are used. Probably the main issue here is that OpenAPI specs are written for humans and lack the semantic clarity, i.e. natural language context, that explains why a tool should be used and not just how to use it.\nContext: Each MCP tool that\u0026rsquo;s available to an AI agent adds to the context window that the agent needs to consider when making decisions, i.e. the mechanism through which an agent can discover and invoke tools is by their descriptions being included in the prompt context. The main concern here is that each MCP server and its tools is eating up precious context. Additionally, ff you have too many tools, the agent may struggle to identify the most relevant ones for a given task - think of overlapping descriptions, i.e. some tools may have very similar names or functionalities, making it hard for the agent to choose the right one. This can lead to confusion and suboptimal tool usage.\nCycle Length: REST APIs response times are usually in the order of tens to hundreds of milliseconds. It\u0026rsquo;s common to fire off multiple API calls in quick succession to gather data. However, each MCP tool invocation involves additional overhead because even finding which tool to call requires an inference step based on the tool metadata. Then once the tool is selected, the agent has to generate a new prompt that includes the tool call, send it to the MCP server, wait for the response, and then process the result - which is another inference step. This adds significant latency compared to direct API calls. In practice, each MCP tool invocation can be orders of magnitude slower than a direct API call. If an agent has to make many tool calls to accomplish a task, the overall latency can become prohibitive.\nSo is it a good idea to auto-convert REST APIs to MCP tools and call it a day? No. Is it a good way to quickly prototype an MCP server and experiment with agent interactions? Absolutely. Just be aware of the limitations and be prepared to refine the tool set for production use. Most OpenAPI to MCP parsers allow you to customize which endpoints get included as tools, so you can easily curate a more focused tool set right off the bat - although this is not a substitute for thoughtful tool design.\nImplementation Overall the implementation is fairly straightforward thanks to FastMCP\u0026rsquo;s OpenAPI integration and Mangum for ASGI compatibility with AWS Lambda.\nTake a look at the full source code for this project on GitHub.\nAuthentication Removal One challenge was that the OpenDota API supports both public and authenticated endpoints. The public API is accessible without authentication, but the OpenAPI spec includes authentication schemas. I implemented a NoAuthClient that:\nStrips authentication from OpenDota API calls Removes security schemes from the OpenAPI specification Without these changes, the FastMCP solution was not working properly since it expected authentication to be provided and was actually sending invalid headers.\nEncoding Another issue I ran into was with encoding, particularly between MCP-server responses like list_tools and the tools themselves. FastMCP expects UTF-8 strings, but some OpenDota API responses include non-UTF-8 bytes (e.g., in player names). To handle this, I ensured that all responses are properly encoded and decoded as UTF-8, replacing invalid characters as needed.\nMangum To deploy the FastMCP server to AWS Lambda, I used Mangum, an adapter that allows ASGI applications to run on AWS Lambda. This involved wrapping the FastMCP app with Mangum\u0026rsquo;s handler, enabling it to process Lambda events and return appropriate HTTP responses. There\u0026rsquo;s a somewhat equivalent REST API from AWS Lambda Powertools, but it was easier to use Mangum since FastMCP is ASGI-based.\nAWS CDK The AWS CDK Python Lambda has come a very long way since I last used it. Today, you can use UV to manage your Python dependencies and package them seamlessly for Lambda deployment. All you need is a uv.lock file in your Lambda source directory, and CDK takes care of the rest.\nThere are instructions in the repo on how to deploy the CDK stack to your AWS account for testing and how to configure a tool so that it can call the deployed MCP server via HTTP.\nLessons Learned As mentioned earlier, auto-converting an entire REST API into MCP tools is not ideal for production use. In general a more AI-centric approach is recommended:\nStart with Agent Stories: Identify the key workflows and tasks that agents need to perform. Write down \u0026ldquo;As an agent, given {context}, I use {tools} to achieve {outcome}.\u0026rdquo; This helps focus on the actual use cases rather than exposing every possible endpoint. Curate Tools: Instead of exposing every endpoint, select and design tools that align with the identified agent stories. This may involve combining multiple API calls into a single tool or simplifying complex endpoints. Optimize for Context: Minimize the number of tools and their complexity to reduce context pollution. Focus on high-value tools that provide the most relevant data for the agent\u0026rsquo;s tasks. Don\u0026rsquo;t reinvent the wheel: There\u0026rsquo;s a lot of new AWS services that can help you build robust MCP servers without starting from scratch. For example AWS AgentCore provides a managed MCP server environment that you can leverage. Conclusion This project demonstrates both the power and the pitfalls of auto-generating MCP servers. FastMCP makes it trivially easy to expose any OpenAPI-compliant API, but that convenience can lead to bloated, inefficient agent interfaces.\nEven so, we were able to successfully build and deploy a serverless MCP server for the OpenDota API using AWS Lambda and API Gateway in a couple of hours. This is exceptional for prototyping and experimentation before you invest in a more curated production solution.\nI used Kiro CLI to interact with the MCP server once deployed. Here\u0026rsquo;s an example of querying player statistics using the FastMCP OpenDota MCP server:\nCLI Output ‚úì open-dota loaded in 2.84 s ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚¢Ä‚£¥‚£∂‚£∂‚£¶‚°Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£¥‚£∂‚£¶‚£Ñ‚°Ä‚†Ä‚†Ä‚¢Ä‚£¥‚£∂‚£∂‚£¶‚°Ä‚†Ä‚†Ä‚¢Ä‚£¥‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂‚£¶‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£¥‚£∂‚£∂‚£∂‚£∂‚£∂‚£¶‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚¢∞‚£ø‚†ã‚†Å‚†à‚†ô‚£ø‚°Ü‚†Ä‚¢Ä‚£æ‚°ø‚†Å‚†Ä‚†à‚¢ª‚°Ü‚¢∞‚£ø‚†ã‚†Å‚†à‚†ô‚£ø‚°Ü‚¢∞‚£ø‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†ô‚†ª‚£¶‚†Ä‚†Ä‚†Ä‚†Ä‚£¥‚°ø‚†ü‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†à‚†ô‚†ª‚¢ø‚£¶‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚£á‚£¥‚°ø‚†ã‚†Ä‚†Ä‚†Ä‚¢Ä‚£º‚†á‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°á‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚¢†‚£§‚£§‚£§‚£§‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°Ü‚†Ä‚†Ä‚£º‚°ü‚†Ä‚†Ä‚†Ä‚†Ä‚£Ä‚£Ä‚£Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢ª‚£ß‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°ø‚†ã‚†Ä‚†Ä‚†Ä‚¢Ä‚£æ‚°ø‚†Å‚†Ä‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°á‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚¢∏‚£ø‚†â‚†â‚†â‚£ø‚°á‚†Ä‚†Ä‚†Ä‚£ø‚°á‚†Ä‚£º‚°ü‚†Ä‚†Ä‚†Ä‚£∞‚°ø‚†ü‚†õ‚†ª‚¢ø‚£Ü‚†Ä‚†Ä‚†Ä‚¢ª‚£ß‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†ô‚†Å‚†Ä‚†Ä‚¢Ä‚£º‚°ü‚†Å‚†Ä‚†Ä‚†Ä‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°á‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚¢∏‚£ø‚£∂‚£∂‚°∂‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚†á‚¢∞‚£ø‚†Ä‚†Ä‚†Ä‚¢∞‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°Ü‚†Ä‚†Ä‚†Ä‚£ø‚°Ü ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†π‚£∑‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°á‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£º‚†ü‚†Ä‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°á‚†Ä‚†Ä‚†Ä‚£ø‚°á ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚°Ä‚†Ä‚†Ä‚†π‚£∑‚°Ñ‚†Ä‚†Ä‚†Ä‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°á‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚£§‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†π‚£ø‚°Ö‚†Ä‚†Ä‚†∏‚£ø‚†Ä‚†Ä‚†Ä‚†∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚†á‚†Ä‚†Ä‚†Ä‚£ø‚†á ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚£æ‚°ü‚£∑‚°Ä‚†Ä‚†Ä‚†ò‚£ø‚£Ü‚†Ä‚†Ä‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°á‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°ü‚£∑‚°Ä‚†Ä‚†Ä‚†Ä‚†ò‚£ø‚£Ü‚†Ä‚†Ä‚¢ª‚£ß‚†Ä‚†Ä‚†Ä‚†π‚£∑‚£¶‚£§‚£§‚£æ‚†è‚†Ä‚†Ä‚†Ä‚£º‚°ü ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°á‚†π‚£∑‚°Ä‚†Ä‚†Ä‚†à‚¢ª‚°á‚†Ä‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°á‚¢∏‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚°á‚†π‚£∑‚°Ä‚†Ä‚†Ä‚†Ä‚†à‚¢ª‚°á‚†Ä‚†Ä‚¢ª‚£ß‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚†â‚†â‚†Ä‚†Ä‚†Ä‚†Ä‚£º‚°ü ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚†∏‚£ø‚£Ñ‚°Ä‚¢Ä‚£†‚£ø‚†á‚†Ä‚†ô‚£∑‚°Ä‚†Ä‚¢Ä‚£º‚†á‚†Ä‚†∏‚£ø‚£Ñ‚°Ä‚¢Ä‚£†‚£ø‚†á‚†∏‚£ø‚£Ñ‚°Ä‚¢Ä‚£†‚£ø‚†á‚†Ä‚†ô‚£∑‚°Ä‚†Ä‚†Ä‚¢Ä‚£º‚†á‚†Ä‚†Ä‚†Ä‚†ª‚£∑‚£¶‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£¥‚£æ‚†ü ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä ‚†Ä‚†à‚†ª‚†ø‚†ø‚†ü‚†Å‚†Ä‚†Ä‚†Ä‚†à‚†ª‚†ø‚†ø‚†ü‚†Å‚†Ä‚†Ä‚†à‚†ª‚†ø‚†ø‚†ü‚†Å‚†Ä‚†Ä‚†à‚†ª‚†ø‚†ø‚†ü‚†Å‚†Ä‚†Ä‚†Ä‚†à‚†ª‚†ø‚†ø‚†ü‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†ô‚†ª‚†ø‚†ø‚†ø‚†ø‚†ü‚†ã‚†Å ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Did you know? ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ ‚îÇ ‚îÇ ‚îÇ You can use /editor to edit your prompt with a vim-like ‚îÇ ‚îÇ experience ‚îÇ ‚îÇ ‚îÇ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ Model: Auto (/model to change) | Plan: KIRO FREE (/usage for more detail) \u0026gt; I\u0026#39;m thinking what hero to choose given that my team already chose: 1. Axe 2. Winter Wyvern 3. Shadow Fiend 4. Witch Doctor The enemy team chose 1. Juggernaut 2. Primal Beast 3. Queen of Pain 4. Pugna Stop if you run into any MCP server errors and report them. Take your time and use the correct tools, especially those that allow you to see match results based on team comps. You can use delegate or subagents to speed up the process \u0026gt; I\u0026#39;ll help you find the best hero pick for your team. Let me analyze the matchups using the available Dota 2 data tools. Running tool get_constants_by_resource with the param (from mcp server: open-dota) ‚ãÆ { ‚ãÆ \u0026#34;resource\u0026#34;: \u0026#34;heroes\u0026#34; ‚ãÆ } Running tool get_find_matches with the param (from mcp server: open-dota) ‚ãÆ { ‚ãÆ \u0026#34;teamA\u0026#34;: [ ‚ãÆ 1, ‚ãÆ 15, ‚ãÆ 8, ‚ãÆ 79 ‚ãÆ ], ‚ãÆ \u0026#34;teamB\u0026#34;: [ ‚ãÆ 15, ‚ãÆ 93, ‚ãÆ 101, ‚ãÆ 84 ‚ãÆ ] ‚ãÆ } - Completed in 1.415s - Completed in 0.848s \u0026gt; Let me analyze the matchups for your team composition. I\u0026#39;ll check hero matchups against the enemy team to find the best pick. Running tool get_heroes_by_hero_id_select_matchups with the param (from mcp server: open-dota) ‚ãÆ { ‚ãÆ \u0026#34;hero_id\u0026#34;: 8 ‚ãÆ } Running tool get_heroes_by_hero_id_select_matchups with the param (from mcp server: open-dota) ‚ãÆ { ‚ãÆ \u0026#34;hero_id\u0026#34;: 112 ‚ãÆ } Running tool get_heroes_by_hero_id_select_matchups with the param (from mcp server: open-dota) ‚ãÆ { ‚ãÆ \u0026#34;hero_id\u0026#34;: 11 ‚ãÆ } Running tool get_heroes_by_hero_id_select_matchups with the param (from mcp server: open-dota) ‚ãÆ { ‚ãÆ \u0026#34;hero_id\u0026#34;: 30 ‚ãÆ } - Completed in 0.729s - Completed in 6.61s - Completed in 0.864s - Completed in 4.608s \u0026gt; Based on the matchup data analysis, here are the best hero picks for your team: Top Recommendations: 1. Rubick (Hero ID: 86) - 47.4% win rate vs your enemies - Excellent against Primal Beast, Juggernaut, and Queen of Pain - Provides utility and can steal key enemy abilities - Complements your team\u0026#39;s support/control focus 2. Ogre Magi (Hero ID: 84) - 49.3% win rate vs your enemies - Strong against Primal Beast and Juggernaut - Adds tankiness and stun potential - Works well with your existing supports 3. Disruptor (Hero ID: 87) - 52.7% win rate vs your enemies - Excellent against Primal Beast (125 wins in 237 games) - Strong disable against Queen of Pain - Provides good teamfight control 4. Jakiro (Hero ID: 64) - 57% win rate vs your enemies - Very strong matchup overall - Good against Pugna and Queen of Pain - Adds both damage and control Why these work: - Your team has Axe (initiator), Winter Wyvern (support), Shadow Fiend (carry), and Witch Doctor (support) - You need either another support/utility hero or a mid-laner - These picks provide disable/control against Primal Beast\u0026#39;s aggression and Queen of Pain\u0026#39;s mobility - They synergize well with Winter Wyvern\u0026#39;s crowd control Avoid: Pugna is particularly weak against your team composition (only 30 wins in 55 games vs Witch Doctor). I\u0026#39;d recommend Disruptor or Jakiro as your best bets for the strongest matchups against their entire team. ‚ñ∏ Credits: 0.31 ‚Ä¢ Time: 27s I\u0026rsquo;m under no illusions that this is even close to Dota Plus, a paid in-game service that provides real-time hero recommendations based on advanced analytics. But it\u0026rsquo;s pretty cool to see an AI agent using an MCP server to analyze matchups and suggest hero pick - feel free to use it to let your teammates know who\u0026rsquo;s carrying the game \u0026#x1f601;\nAcknowledgments OpenDota: Free Dota 2 statistics API FastMCP: Powerful MCP server framework rmcp-openapi: Rust OpenAPI to MCP server generator AWS Lambda Powertools: Robust Lambda utilities UV: Next-generation Python package management Jeremiah Lowin: For the important reality check on MCP auto-conversion ","permalink":"https://manmartgarc.github.io/posts/tech-support/fastmcp-opendota/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eYou\u0026rsquo;ve probably heard about the \u003ca href=\"https://modelcontextprotocol.io/docs/getting-started/intro\"\u003eModel Context Protocol (MCP)\u003c/a\u003e by now ‚Äî an emerging standard for connecting AI agents to external data sources and tools.\u003c/p\u003e\n\u003cp\u003eI wanted to explore building a serverless HTTP-based MCP server using the typical \u003ca href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html\"\u003eAPIGW + Lambda pattern\u003c/a\u003e that is common in serverless applications, i.e. the way some MCP server provider might want to host MCP servers for public consumption. When thinking about which tools the MCP server should have, I wondered how hard it would be to convert an existing REST API into an MCP server - especially given some standard specifications like OpenAPI.\u003c/p\u003e","title":"Building an MCP Server for OpenDota API with FastMCP"},{"content":"Introduction This post is a follow-up to the previous post on Lambda Functions with Go and CDK. In this post, we\u0026rsquo;ll go through deploying a Rust-based AWS Lambda function using the AWS Cloud Development Kit (CDK). We\u0026rsquo;ll focus on the developer workflow: how to get started, wire up your Rust Lambda, and use CDK to manage your infrastructure as code. If you\u0026rsquo;re already familiar with Lambda as a service, this guide will help you get productive with Rust and CDK quickly.\nI want to share all the documentation I found useful while writing this post up front, so you can get started quickly. The main resources are:\nAWS Docs: Creating Lambda functions in the AWS SDK for Rust Building Lambda Functions with Rust Unit testing with aws-smithy-mocks in the AWS SDK for Rust Code Packages: Rust Runtime for AWS Lambda @cdklabs/aws-lambda-rust Rust SDK for AWS Examples Cargo Lambda Why Rust? Rust is a modern, memory-safe, and high-performance language. For Lambda functions, Rust offers fast cold starts and low memory overhead, making it a great fit for event-driven workloads with high volume and low latency. With the growing ecosystem around cargo-lambda and the @cdklabs/aws-lambda-rust CDK construct, deploying Rust Lambdas is getting easier every day.\nProject Structure We\u0026rsquo;ll use a monorepo layout, with the CDK app (TypeScript) and the Rust Lambda function side by side:\nmy-cdk-rust-lambda/ ‚îú‚îÄ‚îÄ rust_lambda/ # Rust Lambda function ‚îÇ ‚îú‚îÄ‚îÄ src/ ‚îÇ ‚îú‚îÄ‚îÄ Cargo.toml ‚îÇ ‚îî‚îÄ‚îÄ ... ‚îú‚îÄ‚îÄ lib/ ‚îÇ ‚îî‚îÄ‚îÄ lambda-rust-stack.ts # CDK stack definition ‚îú‚îÄ‚îÄ bin/ ‚îÇ ‚îî‚îÄ‚îÄ lambda-rust.ts # CDK app entrypoint ‚îú‚îÄ‚îÄ package.json ‚îú‚îÄ‚îÄ cdk.json ‚îî‚îÄ‚îÄ ... Setting Up the CDK Project Start by making sure you have the AWS CDK CLI installed. If you don\u0026rsquo;t have it yet, you can install it globally with:\nnpm install -g aws-cdk You also need to install cargo lambda to easily create the Rust package for your Lambda function.\nInitialize a new CDK project:\nmkdir my-cdk-rust-lambda \u0026amp;\u0026amp; cd my-cdk-rust-lambda cdk init app --language typescript npm install @cdklabs/aws-lambda-rust aws-cdk-lib constructs Add your Rust Lambda code:\ncargo lambda new rust_lambda # Write your Lambda logic in rust_lambda/src/main.rs Defining the Stack Here\u0026rsquo;s a minimal CDK stack that deploys a Rust Lambda and an S3 bucket, wiring the bucket to trigger the Lambda on object creation:\nimport * as rust from \u0026#39;@cdklabs/aws-lambda-rust\u0026#39;; import * as cdk from \u0026#39;aws-cdk-lib\u0026#39;; import * as s3 from \u0026#39;aws-cdk-lib/aws-s3\u0026#39;; import * as s3events from \u0026#39;aws-cdk-lib/aws-s3-notifications\u0026#39;; import { Construct } from \u0026#39;constructs\u0026#39;; export class LambdaRustStack extends cdk.Stack { private readonly rustHandler: rust.RustFunction; private readonly bucket: s3.Bucket; constructor(scope: Construct, id: string, props?: cdk.StackProps) { super(scope, id, props); this.bucket = new s3.Bucket(this, \u0026#39;HelloRustBucket\u0026#39;, { enforceSSL: true, removalPolicy: cdk.RemovalPolicy.DESTROY, autoDeleteObjects: true }); this.rustHandler = new rust.RustFunction(this, \u0026#39;HelloRust\u0026#39;, { binaryName: \u0026#39;rust_lambda\u0026#39;, entry: \u0026#39;rust_lambda\u0026#39; }); this.bucket.grantReadWrite(this.rustHandler); this.bucket.addEventNotification(s3.EventType.OBJECT_CREATED, new s3events.LambdaDestination(this.rustHandler)); } } Whenever an object is created in the S3 bucket, the Rust Lambda function will be triggered, and its invocation event will contain a standard S3 event structure. We plan to implement a Lambda that processes the S3 event, for example, by deleting the object after processing it.\nWriting the Rust Lambda Our Rust Lambda can use the lambda_runtime and aws-sdk-s3 crates. Here\u0026rsquo;s a simplified example:\nuse aws_config::BehaviorVersion; use aws_sdk_s3::Client as S3Client; use lambda_runtime::{run, service_fn, tracing, Error}; mod event_handler; use event_handler::function_handler; #[tokio::main] async fn main() -\u0026gt; Result\u0026lt;(), Error\u0026gt; { tracing::subscriber::fmt().json().init(); let shared_config = aws_config::load_defaults(BehaviorVersion::v2025_01_17()).await; let s3_client = S3Client::new(\u0026amp;shared_config); run(service_fn(|event| function_handler(event, \u0026amp;s3_client))).await } Let\u0026rsquo;s break this down:\nThe lambda_runtime crate provides a couple of things: run, which is the main entry point for our Lambda function, and service_fn, which allows us to define a handler function that will be called with the event data. The tracing crate is used for logging and more, and we initialize it to output JSON logs. The aws_config crate provides implementations of region and credential resolution, which are used throughout the AWS SDK for Rust. The aws_sdk_s3 crate provides the S3 client that we use to interact with S3. Similarly, if we wanted to use DynamoDB, we would use the aws_sdk_dynamodb crate. Notice that we are using tokio, which is an asynchronous runtime for Rust. This allows the main() method to use async/await syntax, which is very useful for I/O-bound tasks like network requests. Finally, some variables are initialized in the main() method, such as shared_config and s3_client. These variables are shared across Lambda invocations because they\u0026rsquo;re part of the init phase. This is where we want to put our expensive initialization code, as it avoids re-initializing the S3 client on every invocation. The event_handler module contains the logic for processing S3 events and deleting objects. You can see its full implementation in GitHub ‚Äî but it\u0026rsquo;s not very complex; it just extracts the bucket name and object key from the S3 event and deletes the object using the S3 client.\nBuilding and Deploying If you just want to test the Rust code, you can run\ncargo test In Rust, the convention is to put the unit tests in the same file as the code, inside a #[cfg(test)] module. This means that you can run cargo test to run all the tests in your project, including the ones in the event_handler.rs module.\nLocal Lambda Testing You can test your Lambda locally using cargo-lambda:\ncargo lambda watch # In another terminal: cargo lambda invoke -F s3-example.json Keep in mind that since the Lambda is triggered by S3 events, you will need to provide a valid S3 event payload when invoking the Lambda locally. The --data-example s3-put flag provides a sample S3 PUT event.\nWhen we are happy with our Rust code, we can build the Lambda binary and deploy it using CDK:\ncdk deploy The @cdklabs/aws-lambda-rust CDK construct will handle packaging the Rust binary (using a Docker image with the Rust toolchain) and deploying the stack. The cdk deploy command will also create the S3 bucket and wire it up to the Lambda function. Now you can upload files to the S3 bucket, and the Lambda function will be triggered automatically ‚Äî you can check the logs in CloudWatch to see the Lambda function processing the S3 events and also deleting the objects after processing them.\nArchitecture Before we finish, I want to talk a little about the architecture of our CDK stack ‚Äî specifically about using S3 events directly to trigger the Lambda function.\nThis is fine for a simple use case, like this one, where we just want to process S3 events and delete the objects. However, the main drawback of this approach is that any events that fail to be processed by the Lambda function will be lost. This is a serious issue, because everything breaks all the time in production.\nA better approach is to use an SQS queue as an intermediary between the S3 bucket and the Lambda function. This way, we can gracefully handle two main failure modes:\nEvents that are unable to be processed by the Lambda function (e.g., malformed events, or events that the Lambda function cannot handle). We can report back batch item failures to SQS, so that even though the Lambda function succeeds, the events are sent back to the SQS queue for reprocessing later. If an event is sent back a certain number of times, we can send it to a dead-letter queue (DLQ) for further investigation. Events that are not processed by the Lambda function due to a transient error (e.g., network issues, or the Lambda function being throttled). In this case, the SQS queue will automatically retry the event because we don\u0026rsquo;t delete the message from the queue until the Lambda function successfully processes it. This means that we can handle transient errors gracefully, without losing any events. This retry mechanism is built into integrations between SQS and Lambda, so you don\u0026rsquo;t have to implement it yourself. You can also use the AWS SDK for Rust to send messages to the SQS queue from your Rust Lambda function, if you need to.\nConclusion With AWS CDK and the Rust Lambda ecosystem, you can quickly get started building high-performance serverless applications. The developer workflow is smooth: write your Lambda in Rust, define your infrastructure in TypeScript, and let CDK handle the rest!\nü¶Ä Happy building ü¶Ä\n","permalink":"https://manmartgarc.github.io/posts/tech-support/rust-lambda/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis post is a follow-up to the previous post on \u003ca href=\"/posts/tech-support/go-lambda/\"\u003eLambda Functions with Go and CDK\u003c/a\u003e. In this post, we\u0026rsquo;ll go through deploying a Rust-based AWS Lambda function using the AWS Cloud Development Kit (CDK). We\u0026rsquo;ll focus on the developer workflow: how to get started, wire up your Rust Lambda, and use CDK to manage your infrastructure as code. If you\u0026rsquo;re already familiar with Lambda as a service, this guide will help you get productive with Rust and CDK quickly.\u003c/p\u003e","title":"Lambda Functions with Rust and CDK"},{"content":"Introduction AWS Lambda is a serverless computing service that lets you run code without provisioning or managing servers. Lambdas are very flexible because you can run them on a schedule, in response to events, or even as an API endpoint that responds to an HTTP request. In this post, we\u0026rsquo;ll look at how to create a Lambda function using Go and the AWS Cloud Development Kit (CDK) - with a focus on using CDK to speed up the development process and improve operational efficiency.\nAWS Lambda I will try my best not to regurgitate the AWS Lambda documentation here. In a nutshell, the whole point of the Lambda service is to run code without having to worry about the underlying compute infrastructure; normally EC2 or Fargate. Lambdas can scale automatically, and you only pay for what you use.\nIf you\u0026rsquo;re wondering how and where your code is run, read more about Firecracker on this blog post by AWS.\nAll of this flexibility comes with trade-offs. Currently, Lambda executions are limited in time to 15 minutes, and the maximum memory you can allocate is 10 GB; and I say currently because these limits have been increased in the past and will most likely increase in the future. This is all to say that if you want to run a long-running process or a memory-intensive task, you will probably have a bad time. Even more so, if you want to run a stateful process, you will have to maintain state somewhere else, the function\u0026rsquo;s execution environment (scope) is virtually stateless and ephemeral. Lambdas are designed to run short-lived, stateless, scalable, and hopefully idempotent processes; you can see best practices here.\nLambda Runtimes Lambda supports a variety of runtimes, including Node.js, Python, Ruby, Java, .NET, and OS-only runtime. Excluding .NET, all the language-specific runtimes are for languages that are interpreted or run on a virtual machine, these are called managed runtimes.\nFor example, Java source code is first compiled into Java byte code, which is then interpreted by the Java Virtual Machine (JVM) which then translates the byte code into machine code instructions which depend on the CPU architecture; this is why there are different JVMs for different architectures. You can even customize some JVM settings like the memory allocation and the garbage collection strategy.\nWhen you run a Java Lambda, the Lambda service spins up a container that contains the JVM and your code; then the JVM executes the Java byte code. The process is virtually the same for other languages like Python, Ruby, and Node.js; the Lambda service spins up a container that contains the interpreter and your code, and then the interpreter executes your code. This whole lifecycle is described here, and it\u0026rsquo;s important to understand if we are very concerned about cost and performance optimization.\nWhat about ahead-of-time compiled languages like Go, C++ or Rust? These languages are compiled into machine code that can be executed directly by the CPU, which means that there is no need for an interpreter or a virtual machine. Because there\u0026rsquo;s less work to do when launching the runtime environment, the cold start time for Go and Rust Lambdas is much faster than for interpreted languages. For these languages you don\u0026rsquo;t use a managed runtime, but an OS-only runtime instead.\nThere\u0026rsquo;s an amazing ongoing benchmark set up by Maxime David where you can see the cold start, memory usage, and execution time for different runtimes and languages.\nChoosing a Runtime As always, design decisions are all about trade-offs. Depending on your team\u0026rsquo;s proficiency, you might choose a language everyone is familiar with but at the cost of lower performance and higher costs for the service since this would allow you to save on developer time. On the other hand, you might choose a language that is more performant and cost-effective but at the cost of developer time and possibly maintenance.\nLet\u0026rsquo;s assume our focus is on performance and cost-effectiveness. The options are Go, C++, or Rust. Let\u0026rsquo;s drop C++ out of the race right away because the White House told us to be good citizens and use only memory-safe languages. This leaves us with Go and Rust.\nThe biggest difference between Go and Rust is that Go is garbage collected and Rust is not. This means that Go has a runtime that manages memory for you, while Rust requires you to manage memory yourself (not by malloc but by its own, beautiful, ownership model). The garbage collector in Go can introduce latency and overhead, which can affect performance. Rust, on the other hand, has no garbage collector, which means that you have more control over memory management and can write more efficient code. Also, both languages can be statically or dynamically linked, which can affect the size of the binary and the cold start time; although Go is statically linked by default.\nAWS CDK The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework that allows you to define your infrastructure as code (IaC) and also helps you manage its lifecycle.\nIf you ever wanted to run a loop and create a bunch of buckets, queues, or whatever resource you can think of, you can do it with CDK. Even more powerful is to use CDK with an OOP language, where you can better define your service\u0026rsquo;s domain. Using the right OOP approach, your service can be more maintainable, scalable, and testable. At the end of the day, CDK is just a middle layer between you and the CloudFormation service, which is the service that actually creates the resources.\nWe will use the TypeScript CDK library to create a stack and this stack is the one where we will deploy our Go Lambda function.\nSetting up the CDK Follow the instructions here to install the CDK CLI and create a new project. Once you have your project set up, you can start creating your stack.\nYou can see the final repository here\nCreating the Lambda Function import { GoFunction } from \u0026#39;@aws-cdk/aws-lambda-go-alpha\u0026#39;; import * as cdk from \u0026#39;aws-cdk-lib\u0026#39;; import { AttributeType, Table } from \u0026#39;aws-cdk-lib/aws-dynamodb\u0026#39;; import { Construct } from \u0026#39;constructs\u0026#39;; export class LambdaGoStack extends cdk.Stack { private readonly goFunc: GoFunction; private readonly ddbTable: Table; constructor(scope: Construct, id: string, props?: cdk.StackProps) { super(scope, id, props); this.ddbTable = new Table(this, \u0026#39;HelloGoTable\u0026#39;, { partitionKey: {name: \u0026#39;Name\u0026#39;, type: AttributeType.STRING} }); this.goFunc = new GoFunction(this, \u0026#39;HelloGo\u0026#39;, { entry: \u0026#39;go-lambda\u0026#39;, environment: { \u0026#39;TABLE_NAME\u0026#39;: this.ddbTable.tableName } }); this.ddbTable.grantReadWriteData(this.goFunc); } } This code does the following:\nCreates a CloudFormation Stack. The stack has two things: a DynamoDB table and a Lambda function. The Lambda function is a GoFunction, which is a CDK construct that allows you to deploy a Go Lambda function using a Docker container for the build process. Then we give permissions to the Lambda function to read and write to the DynamoDB table. Before we can deploy this, we need to define the Lambda function!\nWriting the Lambda Function The Lambda function is a simple Go program that reads and writes to the DynamoDB table. The code is as follows:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/aws/aws-lambda-go/lambda\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/session\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/dynamodb\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/dynamodb/dynamodbattribute\u0026#34; ) type MyEvent struct { Name string `json:\u0026#34;name\u0026#34;` } type TableItem struct { Name string Timestamp string } func newTableItem(name string) TableItem { return TableItem{ Name: name, Timestamp: time.Now().Format(time.RFC3339), } } func HandleRequest(ctx context.Context, event *MyEvent) (*string, error) { if event == nil { return nil, fmt.Errorf(\u0026#34;received event is nil\u0026#34;) } // we create a new session to the AWS SDK sess := session.Must(session.NewSessionWithOptions(session.Options{ SharedConfigState: session.SharedConfigEnable, })) // we create a new DynamoDB client svc := dynamodb.New(sess) // we create a new item to be inserted in the table item := newTableItem(event.Name) // we marshal the item into a map av, err := dynamodbattribute.MarshalMap(item) if err != nil { log.Fatalf(\u0026#34;Got error marshalling new item: %s\u0026#34;, err) } // we construct the input request input := \u0026amp;dynamodb.PutItemInput{ Item: av, TableName: aws.String(os.Getenv(\u0026#34;TABLE_NAME\u0026#34;)), } // we put the item in the table _, err = svc.PutItem(input) if err != nil { log.Fatalf(\u0026#34;Got error calling PutItem: %s\u0026#34;, err) } return aws.String(fmt.Sprintf(\u0026#34;Hello %s!\u0026#34;, event.Name)), nil } func main() { lambda.Start(HandleRequest) } If you have questions about the directory structure, please refer to the repository.\nDeploying To deploy the stack, run the following commands:\ncdk deploy That\u0026rsquo;s it! Now all this infrastructure is deployed to your account. You can delete it by running:\ncdk destroy If you want to make further changes to any component, you can do so by modifying the code and running cdk deploy again.\nTesting Now that the stack is deployed, you can test the Lambda function by running the following command:\naws lambda invoke --function-name \u0026lt;FunctionName\u0026gt; --payload \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;Pepito\u0026#34;}\u0026#39; --cli-binary-format raw-in-base64-out /dev/stdout And we can verify that the item was inserted in the DynamoDB table by running:\naws dynamodb scan --table-name \u0026lt;TableName\u0026gt; Which returns:\n{ \u0026#34;Items\u0026#34;: [ { \u0026#34;Timestamp\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2024-09-29T00:40:29Z\u0026#34; }, \u0026#34;Name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Pepito\u0026#34; } } ], \u0026#34;Count\u0026#34;: 1, \u0026#34;ScannedCount\u0026#34;: 1, \u0026#34;ConsumedCapacity\u0026#34;: null } Conclusion This was a very simple example. We left out important details, particularly about the CI/CD cycle. The good thing is that there\u0026rsquo;s a plethora of tools that can help you with that, like deploying your CDK stack with GitHub Actions or AWS CodePipeline. As long as there\u0026rsquo;s a mechanism to run the cdk deploy command, you can use any tool you like. Next time, we\u0026rsquo;ll do the same exercise but with Rust \u0026#x1f980;.\n","permalink":"https://manmartgarc.github.io/posts/tech-support/go-lambda/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://aws.amazon.com/lambda/\"\u003eAWS Lambda\u003c/a\u003e is a serverless computing service that lets you run code without provisioning or managing servers. Lambdas are very flexible because you can run them on a \u003ca href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html\"\u003eschedule\u003c/a\u003e, in response to \u003ca href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-services.html\"\u003eevents\u003c/a\u003e, or even as an \u003ca href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html\"\u003eAPI endpoint\u003c/a\u003e that responds to an HTTP request. In this post, we\u0026rsquo;ll look at how to create a Lambda function using Go and the AWS Cloud Development Kit (CDK) - with a focus on using CDK to speed up the development process and improve operational efficiency.\u003c/p\u003e","title":"Lambda Functions with Go and CDK"},{"content":"This is the fourth and last week of the fifth course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. The main topic for this week is transformers, a generalization of the attention model that has taken the deep learning world by storm since its inception in 2017.\nThis week\u0026rsquo;s topics are:\nTransformer Network Intuition Self-Attention Multi-Head Attention Transformer Network Architecture More Information Transformer Network Intuition We started with RNNs (known as part of the prehistoric era now), a simple model that reutilizes the same weights at each time steps; allowing to combine previous step\u0026rsquo;s hidden states with the current one. To solve some issues with vanilla RNNs, we introduced GRUs and LSTMs; both more flexible and more complex than simple RNNs. However, one of the things that they all share in common is that the input must be processed sequentially, i.e. one token at a time. This is a problem with large models, where we want to parallelize computation as much as possible. Amdahl\u0026rsquo;s Law gives us a theoretical speed up limit based on the fraction of parallelizable compute in a computer program. Unfortunately, since the entire model is sequential the speed-ups are miniscule. The transformer architecture allows us to process the entire input at once, and in parallel; allowing us to train much more complex models which in turn generate richer feature representations of our sequences.\nThe transformer architecture combines the attention model with a CNN architecture. The idea is to use the attention model\u0026rsquo;s ability to recognize relevance between pairs of tokens, with the computational efficiency of CNNs; which can be parallelized quite easily. Let\u0026rsquo;s dive into the two main components of the transformer architecture.\nSelf-Attention Self-attention is the same idea as attention when we used RNNs. However, since our model is not sequential anymore, we need to calculate the attention in one go. Let\u0026rsquo;s remember that attention is simply some value that describes how relevant a pair of tokens $\u0026lt;t,t\u0026rsquo;\u0026gt;$ are with respect to generating some output. In the RNN case, we learned some embedding $e^{\u0026lt;t,t\u0026rsquo;\u0026gt;}$ as a function of the previous step\u0026rsquo;s post-attention hidden state and each token $t\u0026rsquo;$ pre-attention hidden state. We no longer have previous hidden states since we are doing it all in one go. Let\u0026rsquo;s see how this implemented.\nThe first thing to take into account is that since our model is not sequential anymore, we have lost the temporal structure we got from using RNNs. This means that we will have to come up with a way to encode positions, which we will call the positional encodings. For now, just think that we have two things: our word embeddings from the input, and some positional encoding that encodes the position of each word in the sentence.\nMIT Introduction to Deep Learning\nNotice that in the figure, we add the positional encoding to the word embeddings; therefore imbuing the embedding with positional information which was absent before.\nNow we need to come up with a way to define attention. We can think of attention as a way for input nodes to communicate with each other. How can we imbue each node to talk with each other? We will define three things for each node in the input:\nKey $k^{\u0026lt;t\u0026gt;}$: What do I have? Query $q^{\u0026lt;t\u0026gt;}$: What am I looking for? Value $v^{\u0026lt;t\u0026gt;}$: What do I publicly reveal/broadcast to others? 1 Let\u0026rsquo;s define mathematically attention first, and then we will go over what each of these vectors represent:\n$$ A^{\u0026lt;t\u0026gt;}(q^{\u0026lt;t\u0026gt;}, K, V) = \\sum_{t\u0026rsquo;=1}^{T_x} \\frac{\\exp(q^{\u0026lt;t\u0026gt;}k^{\u0026lt;t\u0026rsquo;\u0026gt;})}{\\sum_{j=1}^{T_x}\\exp(q^{\u0026lt;t\u0026gt;}k^{\u0026lt;j\u0026gt;})} v^{\u0026lt;t\u0026rsquo;\u0026gt;} $$\nLet\u0026rsquo;s use as an example, the input sentence Jane visite l\u0026rsquo;Afrique en septembre., and let\u0026rsquo;s focus on $x^{\u0026lt;3\u0026gt;} = \\text{l\u0026rsquo;Afrique}$ and calculate $A^{\u0026lt;3\u0026gt;}$.\nFirst, $q^{\u0026lt;3\u0026gt;}, k^{\u0026lt;3\u0026gt;}, v^{\u0026lt;3\u0026gt;}$ are generated with three weight matrices $W^Q, W^K, W^V$ which are learnable parameters:\n$$ \\begin{aligned} q^{\u0026lt;3\u0026gt;} \u0026amp;= W^Q x^{\u0026lt;3\u0026gt;} \\\\ k^{\u0026lt;3\u0026gt;} \u0026amp;= W^K x^{\u0026lt;3\u0026gt;} \\\\ v^{\u0026lt;3\u0026gt;} \u0026amp;= W^V x^{\u0026lt;3\u0026gt;} \\end{aligned} $$\nTo compute $A^{\u0026lt;3\u0026gt;}$ we will allow $x^{\u0026lt;3\u0026gt;}$ to communicate to all other tokens what it\u0026rsquo;s looking for: $q^{\u0026lt;3\u0026gt;}$. Each of the tokens will respond with $k^{\u0026lt;t\u0026rsquo;\u0026gt;}$, answering what they have. This is the key part: if the dot product between $q^{\u0026lt;3\u0026gt;}$ and $k^{\u0026lt;t\u0026rsquo;\u0026gt;}$ is high, it means that $k^{\u0026lt;t\u0026rsquo;\u0026gt;}$ has what $q^{\u0026lt;3\u0026gt;}$ is looking for; we are simply looking for a similarity between the query and key vectors. We will allow each token to communicate with all others, and then normalize their contributions with a softmax. We also use $v^{\u0026lt;t\u0026rsquo;\u0026gt;}$ to weight the contribution, allowing token $t\u0026rsquo;$ to not just say that it has what someone else is looking for, but what it is, regardless of what someone else is looking for. Finally, we sum all of these up into $A^{\u0026lt;3\u0026gt;}$. Let\u0026rsquo;s revisit these steps in more detail again.\nRemember that we are doing all of this in one go, therefore we need to do this in a vectorized way using matrix multiplication. Let\u0026rsquo;s redefine attention with matrices:\n$$ \\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_K}}V \\right) $$\nWe get each of these matrices by multiplying the positional embeddings with each of the $W^Q, W^K, W^V$ matrices:\nMIT Introduction to Deep Learning\nLet\u0026rsquo;s break down the matrix version of the attention formula. Let\u0026rsquo;s focus on this term:\n$$ \\frac{QK^T}{\\sqrt{d_K}} $$\nThis term is calculating the pair-wise similarity between queries and keys for all the inputs:\nMIT Introduction to Deep Learning\nThis means that we will have a matrix with the dimensions of our maximum input size, where each row and column corresponds to a position of the input. Along the diagonal, we will have the similarity between each token and itself. We would like to normalize the values to sum up to one (across some specific dimension!); we can use our trusty softmax to do that. We therefore get:\n$$ \\text{attention weighting} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_K}} \\right) $$\nMIT Introduction to Deep Learning\nThis matrix encodes which token is relevant for every token in the output. We know to which token to pay attention to, but what about that token do we pay attention to? This is what $V$ encodes. Multiplying the previous with $V$ allows us to extract features with high attention. We finally get to:\n$$ \\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_K}}V \\right) $$\nMIT Introduction to Deep Learning\nIf we do this for every token in our output and get $A^{\u0026lt;t\u0026gt;} \\forall t \\in T_x$ we will get an attention embedding for all the inputs. This constitutes a single head. It turns out that we will use a head similar to how we use a filter in the context of CNNs. This is the part that we can run in parallel, on top of the vectorization of the $A^{\u0026lt;t\u0026gt;}$ calculation. By using different heads, we allow the model to focus on different features when generating the queries, keys and values. That is we can learn to pay attention to different things, as many things as we have heads. This means that if we have $5$ heads, we will have $W_1^Q, W_1^K, W_1^V, \\dots, W_5^Q, W_5^K, W_5^V$. This is called multi-head attention.\nMulti-Head Attention Similar to how we can stack filters in a CNN to learn different features, we will stack multiple heads to learn different attention representations for each token pair. We know that attention a single-head attention is defined as:\n$$ \\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_K}}V \\right) $$\nWe will index each head with the subscript $i$ so that:\n$$ \\text{head}_i = \\text{Attention}(W_i^Q, W_i^K, W_i^V) $$\nThis allows us to define multi-head attention as:\n$$ \\text{MultiHead}(Q, K, V) = \\text{concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W_o $$\nNotice that $W_o$ is another matrix with learnable parameters, which allows us to dial up or down the signal coming from the multi-head component.\nWe said that the transformer architecture allows for parallelization before, and this is exactly the part that runs in parallel. That is, every head runs the communication (attention) scheme in parallel.\nTransformer Network Architecture Alright, let\u0026rsquo;s do a quick recap:\nWe have dealt with the loss of temporal structure by using positional encodings. We have defined a way which allows nodes to communicate with each other, and learn to which of their friends to pay attention to. We have done this $h$ times, the number of heads, to allow the nodes to ask different combinations of questions and answers: What do you mean? Where are you? Etc. The output of this is some encoding of the input that has all this information clumped together into a super rich embedding of the inputs. This is what we call the encoder part:\nAttention is All You Need\nThe three arrows in the encoder part (left) that go into the Multi-Head Attention component are the three query, keys and values matrices $Q, K, V$ for each of the heads. Remember, we learn $Q, K, V$ via optimization; and we have as many of these representations as we have heads. An additional step shown in the figure is that we add a skip connection with normalization; similar to how we implemented skip-connections in U-Nets.\nWhat about the decoder? The decoder will take the inputs, but shifted to the right for each context length, and learn new $Q, K, V$ representations from the training labels. In machine translation, these are $Q, K, V$ in English instead of French. It will then be able to get its own questions $Q$ in English, and allow it to reach out into the encoder for finding keys and values. This is sometimes called the cross-attention module. After this, we run the embeddings through a feed-forward layer to select the most important features and generate the softmax probabilities for the next token in the prediction.\nMore Information I personally feel like the transformer content was an afterthought in the course. Compared to other content, the transformer content was very shallow and short. There are many amazing communicators that talk about transformers, here are some that I found helpful:\nAndrej Karpathy | Let\u0026rsquo;s build GPT: from scratch, in code, spelled out. CS25 I Stanford Seminar - Transformers United 2023: Introduction to Transformers w/ Andrej Karpathy MIT 6.S191: Recurrent Neural Networks, Transformers, and Attention Transformer models and BERT model: Overview Attention is All You Need CS25 | Stanford Seminar\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week4/","summary":"\u003cp\u003eThis is the fourth and last week of the \u003ca href=\"https://www.coursera.org/learn/nlp-sequence-models?specialization=deep-learning/\"\u003efifth course\u003c/a\u003e of DeepLearning.AI\u0026rsquo;s \u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep Learning Specialization\u003c/a\u003e offered on Coursera. The main topic for this week is transformers, a generalization of the attention model that has taken the deep learning world by storm since its inception in 2017.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week4/#transformer-network-intuition\"\u003eTransformer Network Intuition\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week4/#self-attention\"\u003eSelf-Attention\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week4/#multi-head-attention\"\u003eMulti-Head Attention\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week4/#transformer-network-architecture\"\u003eTransformer Network Architecture\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week4/#more-information\"\u003eMore Information\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"transformer-network-intuition\"\u003eTransformer Network Intuition\u003c/h2\u003e\n\u003cp\u003eWe started with RNNs (known as part of the \u003ca href=\"https://www.youtube.com/watch?v=XfpMkf4rD6E\u0026amp;t=1436s\"\u003eprehistoric era now\u003c/a\u003e), a simple model that reutilizes the same weights at each time steps; allowing to combine previous step\u0026rsquo;s hidden states with the current one. To solve some issues with vanilla RNNs, we introduced GRUs and LSTMs; both more flexible and more complex than simple RNNs. However, one of the things that they all share in common is that the \u003cem\u003einput\u003c/em\u003e must be processed sequentially, i.e. one token at a time. This is a problem with large models, where we want to parallelize computation as much as possible. \u003ca href=\"https://en.wikipedia.org/wiki/Amdahl%27s_law\"\u003eAmdahl\u0026rsquo;s Law\u003c/a\u003e gives us a theoretical speed up limit based on the fraction of parallelizable compute in a computer program. Unfortunately, since the entire model is sequential the speed-ups are miniscule. The transformer architecture allows us to process \u003cem\u003ethe entire\u003c/em\u003e input at once, and in parallel; allowing us to train much more complex models which in turn generate richer feature representations of our sequences.\u003c/p\u003e","title":"Sequence Models: Week 4 | Transformers"},{"content":"This is the third week of the fifth course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. This week goes over sequence-to-sequence models using beam search to optimize the classification step. We also go over the important concept of attention which generalizes a couple of things seen in the last week.\nThis week\u0026rsquo;s topics are:\nSequence to Sequence Architectures Basic Seq2Seq Models Picking the Most Likely Sentence Why not Greedy Search? Beam Search Refinements Error Analysis Attention Developing Intuition Defining the Attention Model Sequence to Sequence Architectures The basic example for sequence-to-sequence approaches was also covered in the first week of the course; where we discussed the many-to-many RNN approach where $T_x \\neq T_y$. This encoder-decoder approach is what we will start discussing in the context of machine translation, a sequence-to-sequence application example.\nBasic Seq2Seq Models Imagine that we want to translate a sentence in French to English. The sentence is Jane visite l\u0026rsquo;Afrique en septembre, and we\u0026rsquo;d like the translation to be Jane is visiting Africa in September. As always we\u0026rsquo;ll use $x^{\u0026lt;t\u0026gt;}$ to represents the words in the French input and $y^{\u0026lt;t\u0026gt;}$ to represent the words in the output.\nEncoder-Decoder Seq2Seq\nNotice that the encoder, in green, can use any type of RNN, GRU or even LSTM to learn some encoding of the entire input; that is, before generating any predictions we have seen the entire input and generated some encoding. From this learned encoding, the decoder part in purple, will generate each output $\\hat{y}^{\u0026lt;t\u0026gt;}$, using the previous\u0026rsquo; steps prediction for the next one until reaching the $T_y$ time step.\nA very similar architecture works very well for image captioning, that is generating captions for an image. The idea is to have some trained CNN, something like AlexNet, and change the last layer; which in AlexNet is a softmax. We do this by replacing the softmax with some RNN, GRU or LSTM for generating the sequence of captions. Notice that this is very similar to the previous approach. The difference is that the CNN is acting as an encoder, while the sequence model is acting as the decoder.\nWhatever approach we take, a question remains: how do we pick the most likely sequence? Can we cast this problem into a conditional probability one like we did before?\nPicking the Most Likely Sentence We can definitely think of machine translation as building a conditional language model. The difference being that we are not starting with a hidden state of $\\vec{0}$, but instead we start the sequence generation from an encoding learned by the encoder network; just like the encoder-decoder image. In this case, our language model is the decoder.\nRemember that in a language model, we try to model the probability of a sentence. We do this by using the conditional probabilities of each word as we move down in the sequence. So if the output sequence is I would like to have a balloon, and we are at the \u0026ldquo;balloon\u0026rdquo; step, we want our model to estimate the probability of seeing \u0026ldquo;balloon\u0026rdquo; given that it saw \u0026ldquo;I would like to have a\u0026rdquo; before. In machine translation we are doing the same except for a small change.\nWe would like to generate a sentence in the target language that maximizes the language model probability given that we saw the source language sentence (in its encoded form). In other words, we want to estimate:\n$$ P(y^{\u0026lt;1\u0026gt;}, \\dots, y^{\u0026lt;T_y\u0026gt;} \\mid x^{\u0026lt;1\u0026gt;}, \\dots, x^{\u0026lt;T_x\u0026gt;}) $$\nWhere $x^{\u0026lt;t\u0026gt;}$ is our source language sentence, and $y^{\u0026lt;t\u0026gt;}$ is our target language sentence.\nHow do we compare the likelihood of each of the sentences the model might consider? We might imagine that given the source language sentence Jane visite l\u0026rsquo;Afrique en septembre there might be many \u0026ldquo;likely\u0026rdquo; translations:\nJane is visiting Africa in September. Jane is going to be visiting Africa in September. In September, Jane will visit Africa. Her African friend welcomed Jane in September. Think of all these sentences as sampled from the $P(\\underbrace{y^{\u0026lt;1\u0026gt;}, \\dots, y^{\u0026lt;T_y\u0026gt;}}_{\\text{English}} \\mid \\underbrace{x}_{\\text{French}})$ distribution. Of course, all these probabilities should not be the same if your language model is modelling the translation task correctly. We want the best one:\n$$ \\argmax_{y^{\u0026lt;1\u0026gt;}, \\dots, y^{\u0026lt;T_y\u0026gt;}} P(y^{\u0026lt;1\u0026gt;}, \\dots, y^{\u0026lt;T_y\u0026gt;} \\mid x) $$\nWhy not Greedy Search? Finding the maximum should be easy, right? We just consider one at a time and then pick the highest one when we run out of elements. As you might know already, there\u0026rsquo;s an issue with this. Maximizing a linear function applied over a set of elements can be tricky; the knapsack problem is the quintessential problem that illustrates the issue. If we apply greedy search, and pick the most likely one at each step, we might not pick the set of words that together maximize the probability. That is, the problem is that the token that looks good to the decoder might turn out later to have been the wrong choice! 1\nThis approach is called greedy search, greedy of course because we greedily pick the largest value we find every time we look. Let\u0026rsquo;s think about the probabilities the model is modeling: we might see more common words sneaking into our translation when we don\u0026rsquo;t want it. For example a proper translation of the French sentence is Jane is visiting Africa in September. However, if we use greedy search, we might get results such as Jane is going to be visiting Africa in September, simply because going is more common than visiting. Notice that after picking going, all the other sentences picked after are also conditioned by this choice.\nWhat else can we do? If we are not checking enough, let\u0026rsquo;s check them all. As you might imagine, considering the entire sequence at a time might result in an exponential explosion of checks. On one extreme, we have exhaustive search; which checks all possible permutations of the sequence with a time-complexity of $O(|V|^{T_y})$. If $|V| = 10000$ and $T_y = 10$ then exhaustive search will evaluate $10000^{10} = 10^{40}$ sequences. On the other extreme, we have greedy search; with a time-complexity of $O(|V|T_y)$; with the same numbers we would need $10000 \\times 10 = 10^5$ sequence checks. 2 I hope you don\u0026rsquo;t need to be convinced that $10^5$ is a lot better than $10^{40}$.\nIt seems like neither approach would work. We\u0026rsquo;d like to use greedy search because it\u0026rsquo;s cheap, but it won\u0026rsquo;t work because there\u0026rsquo;s no guarantee of finding the optimal value. On the other hand, we\u0026rsquo;d like to use exhaustive search because it does guarantee finding the optimal value, but it\u0026rsquo;s computationally unfeasible. Wouldn\u0026rsquo;t it be nice to have something in between?\nBeam Search Beam search allows us to choose somewhere between greedy search and exhaustive search. It has a single hyperparameter called the beam size $B$, which controls how close we are to either greedy or exhaustive search. If we set $B=1$ then we are using greedy search, which means that if we set $B = k$ then we are considering $k$-best choices at each time. Let\u0026rsquo;s walk through the example shown in the course by Andrew.\nLet\u0026rsquo;s start by trying to predict the first word in the output $P(y^{\u0026lt;1\u0026gt;} \\mid x)$ and use $B = 3$. If we use greedy search, we would just pick the most likely word as $\\hat{y}^{\u0026lt;1\u0026gt;}$ and set that for the next time step\u0026rsquo;s prediction. Since we are using $B = 3$, we will consider three different words, and not just any words, but the three most-likely tokens in the step. The $B$ most likely probabilities from the softmax at this step is called the search frontier, and the $B$ most likely tokens are called the hypotheses. Let\u0026rsquo;s say that for this step we get: [in, jane, september] as the $B$-most likely tokens. Let\u0026rsquo;s go into the next step.\nRemember that we kept only 3 tokens in the past step since $B = 3$. For each of them we will calculate the conditional probability of the second token in the sequence:\nFor in we calculate $P(y^{\u0026lt;2\u0026gt;} \\mid x, \\text{in})$ For jane we calculate $P(y^{\u0026lt;2\u0026gt;} \\mid x, \\text{jane})$ For september we calculate $P(y^{\u0026lt;2\u0026gt;} \\mid x, \\text{september})$ Since our vocabulary size $|V| = 10000$ and $B = 3$ we will calculate $3 \\times 10,000 = 30,000$ probabilities. It\u0026rsquo;s out of these $30,000$ probabilities that we will select the $B$ highest ones, that is, the three highest ones. Let\u0026rsquo;s say that the ones we pick are the following:\n[in, september] [jane, is] [jane, visits] Notice that we got rid of the third selection september in the first step as the first token in the output sequence. This is exactly what greedy search doesn\u0026rsquo;t allow us to do. Our decision to trade exploration for exploitation paid off! Now we just repeat the step again. Let\u0026rsquo;s say that we end up with:\n[in, september, jane] [jane, is, visiting] [jane, visits, africa] So on and so forth until one of the sentences predicts the end-of-sentence token, or we reach the maximum output sequence length.\nRefinements Since we are calculating conditional probabilities, and therefore all probabilities are $\\in [0, 1]$, we don\u0026rsquo;t want our numbers to shrink so much and run into numerical underflow. The common trick to do this is to transform a multiplication into summation. Remember one of the properties of logarithms is that the log of a product is the sum of the logs: $\\log_a xy = \\log_ax + \\log_ay$. Therefore, instead of calculating:\n$$ \\argmax_y \\prod_{t=1}^{T_y} P(y^{\u0026lt;t\u0026gt;} \\mid x, y^{\u0026lt;1\u0026gt;}, \\dots, y^{\u0026lt;t - 1\u0026gt;}) $$\nWhere $\\prod$ is the product symbol, much like $\\sum$ is the summation symbol; we want to calculate:\n$$ \\argmax_y \\sum_{t=1}^{T_y} \\log P(y^{\u0026lt;t\u0026gt;} \\mid x, y^{\u0026lt;1\u0026gt;}, \\dots, y^{\u0026lt;t - 1\u0026gt;}) $$\nThis trick is found pretty much anywhere you are calculating conditional probabilities.\nThere\u0026rsquo;s another issue however, related to the length of the output sequence $y$. Since we are multiplying probabilities, or summing them in log space, longer sequences will have smaller probabilities assigned to them. This is simply because we are multiplying numbers between $[0, 1]$; $0.5^3 = 0.125$ while $0.5^2 = 0.25$. To ameliorate this, we will introduce a weighting term that normalizes the probabilities by the size of $T_y$\n$$ \\argmax_y \\frac{1}{T_y^{\\alpha}}\\sum_{t=1}^{T_y} \\log P(y^{\u0026lt;t\u0026gt;} \\mid x, y^{\u0026lt;1\u0026gt;}, \\dots, y^{\u0026lt;t - 1\u0026gt;}) $$\nSetting $\\alpha = 0.7$ to be somewhere in between $\\alpha = 0$, no normalization, and $\\alpha = 1$, full normalization.\nFinally, regarding the beam size. As we mentioned, setting $B=1$ amounts to greedy search, which is fast but worse result. As we increase $B$, we get slower performance but better results. $B$ is a parameter that regulates the mixture of exploitation and exploration. Remember that to find exact maximums we need to use exhaustive search. Anything other than that has no guarantees. In beam search, we can adjust how much \u0026ldquo;guarantee\u0026rdquo; in finding an optimal solution by varying $B$.\nError Analysis Since we introduced another source of error by using beam search, we need to be able to know whether errors are coming from the sequence model, or if they are arising from using beam search. That is, do we need to keep training/add more data, or do we need to use a larger beam size?\nLet\u0026rsquo;s keep using the French example: Jane visite l\u0026rsquo;Afrique en septembre. Let\u0026rsquo;s say that the human-generated label in our dev set for this example is:\n$y^*$: Jane visits Africa in September. And let\u0026rsquo;s say that our algorithm is predicting:\n$\\hat{y}$: Jane visited Africa last September. Obviously, our algorithm\u0026rsquo;s prediction is wrong! The whole meaning of the sentence is different. How can we zone-in into the root cause of the problem?\nWe can grab the RNN component of our model, that is the encoder and decoder, which computes $P(y \\mid x)$, and compare two things:\nRun $y^*$ through the RNN, to estimate $P(y^* \\mid x)$. Run $\\hat{y}$ through the RNN, to estimate $P(\\hat{y} \\mid x)$ What are we really doing here? We are saying: Hey RNN! Do you think the bad translation is more likely than the good translation? If the RNN is predicting the bad translation to be more likely than the good translation, then it doesn\u0026rsquo;t matter what beam search is doing; the model still needs more training. On the other hand, if the model thinks that the good translation is more likely than the bad one, then the only way we are getting a bad translation at the end is due to beam search. In this case, increasing the beam width should help improve our model\u0026rsquo;s performance. In practice, we calculate the fraction of errors due to either the RNN or the beam search, and take action based on what is more efficient.\nAttention Through this whole week we have been using the encoder-decoder architecture, one RNN reads in the sentence and learns an encoding, while another RNN decodes the encoding. There is another approach, called the attention mechanism approach, which makes this work a lot better albeit for larger compute costs.\nDeveloping Intuition Let\u0026rsquo;s consider what happens in very long sequences, say a paragraph, when we try to apply machine translation with the encoder-decoder architecture. The encoder will memorize the entire thing, which is pretty long, and then pass it into the decoder. Think instead, how a human translator would tackle the problem of translating a whole paragraph. The translator might start with the first few sentences, translate them, and then move on. Even more so, the translator can read the entire paragraph before starting to translate, using the latter context of the paragraph to guide the beginning of the translation.\nThe encoder will try its best to encode as much useful information in the encoding, but as sentence length grows, this task gets harder and harder. In practice, the encoder-decoder architecture\u0026rsquo;s performance suffers as sentence length grows because of this. The attention approach is a remedy for this issue; the performance is not hurt as sentence length grows.\nIn the attention model, we no longer have an encoder-decoder architecture. Instead, we have a bidirectional RNN (choose your flavor) called the pre-attention RNN. Hooked to each of the outputs of the pre-attention bidirectional RNN, we will have a uni-directional RNN (choose your flavor) called the post-attention RNN. The role of the pre-attention RNN is to figure out how much attention to pay to each of the input words when predicting some word. Since the pre-attention RNN is bidirectional, we can pay attention to the entire input sequence when generating the first prediction.\nWe do this by learning some parameter $\\alpha^{\u0026lt;t,t\u0026rsquo;\u0026gt;}$. Each parameter tells us, how much attention (a value $\\in [0, 1]$) we should give French word $t\u0026rsquo;$ when generating the English word $t$. Let\u0026rsquo;s go into more detail.\nDefining the Attention Model Let\u0026rsquo;s start with the pre-attention bidirectional RNN, which in practice is usually implemented as a bidirectional LSTM network. Remember that in a bidirectional RNN, we have two hidden states $a^{\u0026lt;t\u0026gt;}$; one going forward $\\overrightarrow{a}^{\u0026lt;t\u0026gt;}$, and one going backwards $\\overleftarrow{a}^{\u0026lt;t\u0026gt;}$. To simplify the notation, we will define a single hidden state:\n$$ a^{\u0026lt;t\u0026gt;} = (\\overrightarrow{a}^{\u0026lt;t\u0026gt;}, \\overleftarrow{a}^{\u0026lt;t\u0026gt;}) $$\nWhich is equal to both states concatenated together.\nLet\u0026rsquo;s now focus on the post-attention RNN. This part will have a hidden state $s^{\u0026lt;t\u0026gt;}$, which is used to generate $\\hat{y}^{\u0026lt;t\u0026gt;}$. What is the input of this post-attention RNN? It\u0026rsquo;s some context $c^{\u0026lt;t\u0026gt;}$ which is the output at step $t$ of the pre-attention bidirectional RNN. It\u0026rsquo;s this context which defines the attention mechanism.\nThe context will be the weighted sum of all $a^{\u0026lt;t\u0026gt;}$, the hidden state in the pre-attention layer, with their respective attention weights $\\alpha^{\u0026lt;t, t\u0026rsquo;\u0026gt;}$. We want all attention weights for a particular token $t$ to sum up to one:\n$$ \\sum_{t\u0026rsquo;=1}^{T_x} \\alpha^{\u0026lt;t,t\u0026rsquo;\u0026gt;} = 1 $$\nWe can then use these weights when calculating the context, we can control the mixture of each of the input tokens as it relates to this particular\u0026rsquo;s time step\u0026rsquo;s prediction:\n$$ c^{\u0026lt;t\u0026gt;} = \\sum_{t\u0026rsquo;=1}^{T_x} \\alpha^{\u0026lt;t,t\u0026rsquo;\u0026gt;} a^{\u0026lt;t\u0026rsquo;\u0026gt;} $$\nWe should be convinced that when $\\alpha^{\u0026lt;t,t\u0026rsquo;\u0026gt;} = 0$ it means that $t\u0026rsquo;$ is not that important when generating $t$, therefore the hidden state for $t\u0026rsquo;$ is not that important, and will be weighted down in the sum.\nHow do we calculate these attention weights $\\alpha^{\u0026lt;t,t\u0026rsquo;\u0026gt;}$? Let\u0026rsquo;s be reminded that $\\alpha^{\u0026lt;t,t\u0026rsquo;\u0026gt;}$ defines the amount of attention $y^{\u0026lt;t\u0026gt;}$ should pay to $a^{\u0026lt;t\u0026rsquo;\u0026gt;}$. We can define this attention as a softmax, to guarantee that the weights sum up to one as we mentioned earlier:\n$$ \\alpha^{\u0026lt;t,t\u0026rsquo;\u0026gt;} = \\frac{\\exp(e^{\u0026lt;t,t\u0026rsquo;\u0026gt;})}{\\sum_{t\u0026rsquo;=1}^{T_x} \\exp(e^{\u0026lt;t,t\u0026rsquo;\u0026gt;})} $$\nBut where do these new terms $e^{\u0026lt;t,t\u0026rsquo;\u0026gt;}$ come from? We will use a small neural network with only one hidden layer to learn these. The small network will take $s^{\u0026lt;t-1\u0026gt;}$ and $a^{\u0026lt;t\u0026rsquo;\u0026gt;}$ to produce $e^{\u0026lt;t,t\u0026rsquo;\u0026gt;}$. Let\u0026rsquo;s keep in mind that $s^{\u0026lt;t-1\u0026gt;}$ is the hidden state of the post-attention layer for the previous time step, while $a^{\u0026lt;t\u0026rsquo;\u0026gt;}$ is a hidden state in the pre-attention layer for the $t\u0026rsquo;$ time step. This means that this layer will learn to combine the hidden state of the previous token in the prediction, that is how relevant the previous prediction token is to the current one, with each of the input token hidden states, that is each of the tokens in the input.\nIt makes sense to define attention as: I have some memory of what I\u0026rsquo;ve translated so far, and attention is the thing that relates each of the words in the input, with the memory of what I\u0026rsquo;ve done so far. This means that $\\alpha^{\u0026lt;t,t\u0026rsquo;\u0026gt;}$ and $e^{\u0026lt;t,t\u0026rsquo;\u0026gt;}$ will depend on $s^{\u0026lt;t-1\u0026gt;}$ and $a^{\u0026lt;t\u0026rsquo;\u0026gt;}$ when considering $t$.\nAttention Mechanism\nIn the picture, the annotation vectors are the pre-attention layer, the attention mechanism represents the layer where we estimate $\\alpha^{\u0026lt;t,t\u0026rsquo;\u0026gt;}$ and everything above the dotted line is the post-attention layer.\nNext week\u0026rsquo;s post is here.\nJurafsky and Martin | Speech and Language Processing, 3rd Edition\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDive Into Deep Learning | Beam Search\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week3/","summary":"\u003cp\u003eThis is the third week of the \u003ca href=\"https://www.coursera.org/learn/nlp-sequence-models?specialization=deep-learning/\"\u003efifth course\u003c/a\u003e of DeepLearning.AI\u0026rsquo;s \u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep Learning Specialization\u003c/a\u003e offered on Coursera. This week goes over sequence-to-sequence models using beam search to optimize the classification step. We also go over the important concept of attention which generalizes a couple of things seen in the last week.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week3/#sequence-to-sequence-architectures\"\u003eSequence to Sequence Architectures\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week3/#basic-seq2seq-models\"\u003eBasic Seq2Seq Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week3/#picking-the-most-likely-sentence\"\u003ePicking the Most Likely Sentence\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week3/#why-not-greedy-search\"\u003eWhy not Greedy Search?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week3/#beam-search\"\u003eBeam Search\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week3/#refinements\"\u003eRefinements\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week3/#error-analysis\"\u003eError Analysis\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week3/#attention\"\u003eAttention\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week3/#developing-intuition\"\u003eDeveloping Intuition\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week3/#defining-the-attention-model\"\u003eDefining the Attention Model\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"sequence-to-sequence-architectures\"\u003eSequence to Sequence Architectures\u003c/h2\u003e\n\u003cp\u003eThe basic example for sequence-to-sequence approaches was also covered in the \u003ca href=\"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week1/\"\u003efirst week\u003c/a\u003e of the course; where we discussed the many-to-many RNN approach where $T_x \\neq T_y$. This encoder-decoder approach is what we will start discussing in the context of machine translation, a sequence-to-sequence application example.\u003c/p\u003e","title":"Sequence Models: Week 3 | Sequence Models \u0026 Attention Mechanism"},{"content":"This is the second week of the fifth course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. In this week, we go a little more in depth into natural language applications with sequence models, and also discuss word embeddings‚Äîan amazing technique for extracting semantic meaning from words.\nThis week\u0026rsquo;s topics are:\nIntroduction to Word Embeddings Word Representation Using Word Embeddings Properties of Word Embeddings Cosine Similarity Embedding Matrix Word Embeddings Learning Word Embeddings Word2Vec Negative Sampling GloVe Word Vectors Applications Using Word Embeddings Sentiment Classification De-biasing Word Embeddings Introduction to Word Embeddings Word Representation Word embeddings are a way of representing words. The approach borrows from dimensionality reduction, and combines it with optimization. These two things allow us to create new word representations that are empirically good with respect to some task. Let\u0026rsquo;s go over how this is possible.\nWe have been using one-hot encoding for words in our vocabulary. This works great as a way of representing words for a mathematical model; i.e. we build a pretty decent language model. The issue however, is two-fold: a vocabulary size of $10,000$ is tiny in practice. Do we really need to calculate the softmax for all these words? The second issue is that of semantics: at no point does the model realize that apple, orange and banana are similar things. Let\u0026rsquo;s look at the latter first.\nIn a one-hot encoding all elements are $0$ except for the element indexed by the index of the word in our vocabulary. If our vocabulary size $|V| = 10,000$, then we represent each word with $9,999$ zeros and one $1$ in the index of that word. Let\u0026rsquo;s use these indexes for the words in the example next:\n$i_{man} = 5391$ $i_{woman} = 9853$ $i_{king} = 4914$ $i_{queen} = 7157$ $i_{apple} = 456$ $i_{orange} = 6257$ So that $o_{man} = o_{5391}$ is the one-hot encoding of the word. Remember that $o_w \\in \\mathbb{R}^{10,000}$.\nHow could our algorithm handle semantic meaning? The same way that we define most about anything, with matrices and linear algebra. Currently, our words live in $\\mathbb{R}^{10,000}$ space. That is, they are a $10,000$ dimensional one-hot vector, where all the entries are $0$ except for one. Let\u0026rsquo;s think about the dot-product, which is the inner product of Euclidean space. Algebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers. Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them. 1 Let\u0026rsquo;s focus on the geometrical definition.\nIf the dot-product between two vectors is $0$ then the vectors are either orthogonal or one of them is the zero vector $\\vec{0}$. On the other hand, positive and negative values map to the angle value between them. What does it mean that two vectors are orthogonal? It means that they are perpendicular to each other. What does that mean? It depends on the application: in optics, orthogonality is related to polarization states. In statistics, orthogonal random variables are those whose covariance is $0$. What happens when we dot two one-hot vectors for different words?\nSince, by definition, each of the one-hot vectors is unique for every word in our dictionary, all of them will have a $1$ in a unique position. This means that the dot product between any pair $i,j$ where $i \\neq j$ will be $0$. This means that the vector space defined by all of our one-hot vectors has a dimension for each word, since dimension are orthogonal to each other, all vectors are orthogonal to each other. Which in turn means that our representation imposes the assumption that there is literally no relationship between any pair of words. Even pairs such as $(\\text{happy}, \\text{happier})$.\nThe question is then: how can we project the $\\mathbb{R}^{10,000}$ vector space into a lower dimensional space, say $\\mathbb{R}^{300}$? We could try to use dimensionality reduction techniques, such as PCA, but it will not work that well. This is because all dimensions are orthogonal, and there\u0026rsquo;s no \u0026ldquo;common\u0026rdquo; information to encode the higher dimensional space into a lower dimensional space. We need something different, something learnable, we need word embeddings!\nSay that we define $4$ dimensions: Gender, Royal, Age, Food. We\u0026rsquo;d like to map each of our $\\mathbb{R}^{10,000}$ one-hot vectors into $\\mathbb{R}^{4}$. We can do this by hand. Let\u0026rsquo;s map the words we had before into these by hand:\nMan Woman King Queen Apple Orange Gender -1 1 -0.95 0.97 0.00 0.01 Royal 0.01 0.02 0.93 0.95 -0.01 0.00 Age 0.03 0.02 0.7 0.69 0.03 -0.02 Food 0.09 0.01 0.02 0.01 0.95 0.97 The numbers are fictitious, but set by hand to illustrate the example. The important thing is to notice that by embedding these words in our four new dimensions we have captured semantic meaning, and now the vectors live in \u0026ldquo;semantic\u0026rdquo; space. This in turns, allows us to define distance, and therefore similarity. For example, we should expect our embeddings to comply with $e_{apple} - e_{queen} \u0026gt; e_{apple} - e_{orange}$, where $e_{w}$ is the embedding vector, in our case in $\\mathbb{R}^{4}$, of the word $w$. That is, the distance between the word apple and queen, should be greater than the distance between apple and orange because apples and oranges are fruits.\nHow do we learn these embeddings, and how many dimensions are enough is something we go over in a bit.\nRemember that embeddings are synonymous with encodings. We want to have efficient word encodings that are much smaller but denser than our one-hot vectors; which are big but sparse.\nUsing Word Embeddings So how do we use these embeddings? It turns out that learning good word embeddings takes a lot of text: between $1$ and $100$ billion words in the corpus. That\u0026rsquo;s just the data requirements, training costs are also going to be significant. Instead, we can be lazy and use pre-trained embeddings from somewhere online. After we have these embeddings, we can employ transfer learning and fine-tune a neural network model to our task. We could even fine-tune the embeddings with our data.\nThis approach is similar to that of face encoding. In the face recognition approach, we used a Siamese network to learn some $\\mathbb{R}^{128}$ dimensional face embedding that is a good embedding for comparing two faces. It would even work with a picture that was not in the training set; a necessity of the face recognition application. In the case of word embeddings we are doing the same, except two things. The encodings are good encodings with respect to a language model, and also the cardinality of the inputs is fixed. It\u0026rsquo;s equal to the size of the vocabulary used to learn the embeddings. Remember that we can use a unique token to represent unknown words, which would also get its own embedding representation.\nProperties of Word Embeddings We ended up with word embeddings by wanting to have a better representation of words that capture semantic meaning. But what does \u0026ldquo;capturing semantic meaning\u0026rdquo; really mean, and how is that helpful? One of the things that we can do with semantics, is to reason about analogies: $a$ is to $b$ as $c$ is to $d$; this seems to be an ability that helps with NLP tasks.\nLet\u0026rsquo;s say that we have some embedding $e_w \\in \\mathbb{R}^{300}$, since our embedding vectors are no longer orthogonal we can use linear algebra to describe the relationship between things in this new space; such as length, distance and so on. In the paper that introduces this idea, the authors write in the abstract:\n\u0026hellip; For example, the male/female relationship is automatically learned, and with the induced vector representations, ‚ÄúKing - Man + Woman‚Äù results in a vector very close to ‚ÄúQueen.‚Äù\nThis means that there is some dimension in the embedding that represents the semantic meaning of \u0026ldquo;gender\u0026rdquo;, and this dimension, the difference between \u0026ldquo;King\u0026rdquo; and \u0026ldquo;Man\u0026rdquo;, plus that of \u0026ldquo;Woman\u0026rdquo; approximates \u0026ldquo;Queen\u0026rdquo;. We can do math with semantic topics. Since we can do math, let\u0026rsquo;s define \u0026ldquo;Man\u0026rdquo; is to \u0026ldquo;Woman\u0026rdquo; as \u0026ldquo;King\u0026rdquo; is to $w$. Mathematically:\n$$ \\begin{aligned} e_{man} - e_{woman} \u0026amp;\\approx e_{king} - e_{w} \\\\ e_{w} \u0026amp;\\approx e_{king} - e_{man} + e_{woman} \\end{aligned} $$\nIf we have a way to compare two vectors, i.e. a similarity function, we can search over all our words $w \\in V$ in linear time comparing each $e_w$ to $e_{king} - e_{man} + e_{woman}$, and then picking the most similar as $e_w$. How can we say if two vectors are similar?\nCosine Similarity Let\u0026rsquo;s keep in mind that the dot product is related to the angle between two vectors. If $x \\cdot y = 0$, then $x \\perp y$, that is, they are either orthogonal or $x$ or $y = \\vec{0}$. In the case that they are orthogonal, the angle between them is $90\\degree$. What if $x \\cdot y \\neq 0$? It can be anything, from $(-\\infty, \\infty)$ except $0$. In similarity functions we always like the interval $[0, 1]$ more than $[-\\infty, \\infty]$. We can shrink the domain to be in the interval $[0, 1]$ usually by normalizing by something. We can use the Euclidean norm of the two vectors $x, y$ to normalize the size of the dot product. What does that product between the lengths of two vectors have to do with the dot product, and the angle between the vectors?\nThe Euclidean dot product formula is:\n$$ A \\cdot B = ||A|| \\ ||B|| \\cos{\\theta} $$\nWhich is where the cosine similarity formula comes from:\n$$ S_c(A, B) := \\cos{\\theta} = \\frac{A \\cdot B}{||A|| \\ ||B||} $$\nRemembering the sine and cosine definitions: $\\cos(90) = 0, \\cos(0) = 1$ in angles.\nWe can redefine our search problem for analogy reasoning as:\n$$ \\argmax_{w \\in W} := S_c(e_w, e_{king} - e_{man} + e_{woman}) $$\nThe analogy reasoning is just a show trick. It\u0026rsquo;s a way of exemplify what embeddings are doing. In practice, using word embeddings allows our models to learn about the connections between words in semantic space; which is very useful for performance.\nEmbedding Matrix So to formalize what an embedding is, let\u0026rsquo;s go step by step and calculate some embedding $e_w$. Let $E$ be an embedding matrix of dimensions $(300, 10000)$. It\u0026rsquo;s a mapping from 10,000 dimensional space to 300 dimensional space. The 10,000 comes from our vocabulary size; that is, for each word in our vocabulary $w \\in W$ we will have a 300 dimensional vector $e_w$ which represents the embedding of $w \\in \\mathbb{R}^{300}$. Remember that the one-vector is the same but in more dimensions: $o_w \\in \\mathbb{R}^{10000}$. What happens if we multiply $E o_w$? The output dimension should be $(300, 1)$ since:\n$$ \\underset{(300, 10000)}{E}\\underset{(10000, 1)}{o_w} = \\underset{(300, 1)}{e_w} $$\nNotice that because of the way matrix multiplication works, only one column of $E$ is selected, and it\u0026rsquo;s exactly the $w_{th}$ one. This makes generating word embeddings pretty computationally expensive, even after learning $E$. Which is the reason why many pre-trained embeddings are distributed as a dictionary mapping $w \\to e_w$ directly, without the multiplication step.\nWord Embeddings Learning Word Embeddings How can we learn the embedding matrix $E$? We can learn it via supervised learning. If we have some vocabulary $V$ with size $|V| = 10,000$, and $o_w$ represents the one-hot vector encoding of the word with index $w$. The training pair is a pair consisting of a phrase and the next word, such as $X_i=\\text{\u0026ldquo;I want a glass of orange\u0026rdquo;}$ and $Y_i=\\text{\u0026ldquo;juice\u0026rdquo;}$\nWe can randomly initialize $E$ and define $e_w = Eo_w$. The idea is to take each $o_w^{\u0026lt;t\u0026gt;}$ in our training pair and map it to $e_w^{\u0026lt;t\u0026gt;}$ via $E$. Finally, we use all $e_w^{\u0026lt;t\u0026gt;}$ into a fully connected layer that\u0026rsquo;s run into a softmax. The idea is to correctly classify the next word using the optimization on the network parameters $\\theta$, one of which is $E$ the embedding matrix, using cross-entropy loss.\nIn practice, this means that the algorithm will learn embeddings of words with the directed goal of next token prediction. Since the quality of the embeddings is directly related to word prediction, the algorithm has full incentive to encode relations of similar words into one of the 300 features. This brings us to another important thing.\nSo far we have been using semantic topics such as \u0026ldquo;Man\u0026rdquo; or \u0026ldquo;Queen\u0026rdquo;. We do this not because the embeddings will correspond to topics we understand, but to aid the explanation of the topic. In practice, deciphering what an embedding dimension really means is hard, and it amounts to guess work.\nDifferent extensions of this basic model generalize the pairing of examples for the supervised learning task. We can define a context $c$ and a target word $t$. We were using $c$ to be all the words leading up to $t$. But we are not required to do this. We can use a context where we use 2 words on the left and on the right of the target word, even just the last word. The Word2Vec algorithm is an extension of this idea, and it has a variation that uses Negative sampling for generating the pairs. Let\u0026rsquo;s discuss the Word2Vec approach first.\nWord2Vec In Word2Vec we keep using the supervised learning approach, but we pick just a pair of words $c, t$, the context and target respectively. Notice that these do not need to be adjacent words. In the skip-gram model, we pick a word $t$ and generate the content by picking a neighboring word $c$ at random. Therefore, the classification task is to classify which words are more likely to occur nearby each other. We will see a more sophisticated way of sampling in negative sampling but for now let\u0026rsquo;s keep in mind that we are still doing essentially the same as before:\n$$ o_c \\to E \\to e_c \\to \\text{softmax} \\to \\hat{y} $$\nThe softmax output $\\hat{y}$ is the estimate of the probability that we see $c$ given $t$: $P(t\\mid c)$. It\u0026rsquo;s defined as:\n$$ P(t \\mid c) = \\frac{e^{\\theta_t^Te_c}}{\\sum_{j=1}^{10000}e^{\\theta_j^Te_c}} $$\nAnd our loss is:\n$$ \\mathcal{L}(\\hat{y}, y) = - \\sum_{i=1}^{10000} y_i \\log \\hat{y}_i $$\nSo far so good, and this will work. But there\u0026rsquo;s a problem: calculating the softmax for $10,000$ classes might be feasible, but what about bigger vocabularies? What about a million? It\u0026rsquo;s no longer feasible. There are a few solutions.\nThe hierarchical softmax is a similar approach to negative sampling, where the idea is to reduce the computational complexity of the $\\text{softmax}$ function down from $O(|V|)$. The hierarchical softmax has a time complexity of $O(\\log |V|)$. The approach is based on Huffman coding a coding scheme used for lossless data compression. Chris McCormick has an amazing tutorial on his website where you can learn a lot more about word2vec. It seems that negative sampling and hierarchical softmax work in an equivalent manner, with negative sampling being more intuitive to understand.\nNegative Sampling In negative sampling we still have a context word and a target: $c, t$. Except we will create fake example pairs, negative samples, in addition to the real pair. For example, if the real pair is (orange, juice), we might make $k=4$ negative samples:\n(orange, king) (orange, book) (orange, the) (orange, of) Notice that the context $c$ remains fixed, while the target $t$ is sampled at random. For smaller datasets we want to pick $k \\in [5, 20]$ while for larger datasets we can get away with $k \\in [2, 5]$. Think of $k$ as a sampling size parameter, which reduces the standard error of our estimates as it goes up.\nThe next thing is to label all positive samples (real) with a $1$ label, that is $y=1$; while we give a $0$ label to the negative samples, that is $y = 0$. The idea is for the softmax to learn to classify which pairs are positive and which ones are negative. That is we want to estimate:\n$$ P(y=1 \\mid c, t) = \\sigma(\\theta_t^Te_c) $$\nThis is equivalent to training $10,000$ different binary classifiers at the end of the network, but only sampling $k$ of those classifiers at each time. Therefore, we can think of negative sampling as an estimator of the $\\text{softmax}$ function.\nBut how do we sample the negative samples? The authors recommend a way that\u0026rsquo;s somewhere in between the empirical distribution $P(w_i)$ and $\\frac{1}{|V|}$. Using the former will over-sample stop words because they are very common, while the latter will treat extremely uncommon words the same as stop words. The actual probability mass function is given by:\n$$ P(w_i) = \\frac{f(w_i)^{\\frac{3}{4}}}{\\sum_{j=1}^{10000}f(w_j)^{\\frac{3}{4}}} $$\nGloVe Word Vectors Global Vectors For Word Representations (GloVE) is another approach to learn $E$ and $e_w$, the embedding matrix and word embeddings. It does something very similar to word2vec.\nIn word2vec we were picking $c, t$ two words that appear in proximity. Here we will be more explicit and define $X_{ij}$ as the number of times that word $j$ appears in the context of word $i$. Notice that depending on how we define the context $X_{ij}$ could be equal to $X_{ji}$. If the context wraps around the target it will be symmetrical, while if you only look backwards it will not.\nIn this case we want to minimize the following:\n$$ \\mathcal{L} = \\sum_{i=1}^{|V|} \\sum_{j=1}^{|V|} f(X_{ij})(\\theta_i^Te_j + b_i + b_j - \\log X_{ij})^2 $$\n$f(X_{ij})$ is some weighting function that allows us to deal with taking $\\log 0$ but can also introduce useful information about the frequency of the pair $ij$.\nIt turns out that $\\theta_i$ and $e_j$ as symmetrical vectors since they are both optimized in the same way. This means that in practice, to calculate the embedding of word $w$ we take the average of the two:\n$$ e_w = \\frac{\\theta_w + e_w}{2} $$\nApplications Using Word Embeddings Sentiment Classification Okay, enough about embeddings. How can we use them? The same way we use one-hot encodings! Let\u0026rsquo;s walk through the steps from a basic NN to using an RNN.\nWe could take a review, such as \u0026ldquo;This desert is excellent\u0026rdquo; and want to classify it into five classes, the reviews\u0026rsquo; stars. We do the same thing again, map each $w$ into $o_w$ and then into $e_w$ via an embedding matrix $E$. After we have $e_{the}, \\dots, e_{excellent}$ we could take the average of these vectors across the dimensions (columns), resulting in a $300$ dimensional vector, if we use $300$ dimensional embeddings. We could then run this through a softmax with $K = 5$ and generate a prediction for the review score. This will work okay, but we will run into the same issue we ran original when introducing RNNs. The model cannot use temporal structure to affect the prediction.\nWe can simply plug-and-play an RNN using the same embeddings, and generate a prediction at the end, exactly like the many-to-one RNN approach. This will work a lot better for reviews that say things like: \u0026ldquo;A great waste of my very, good, excellent time\u0026rdquo;. The former model might score this review highly because of the presence of positive sentiment words, but an RNN might be more adept at figuring out that \u0026ldquo;waste\u0026rdquo; negates all the other positive words.\nDe-biasing Word Embeddings You might have heard of instances where earlier versions of machine translation approaches would translate the non-gendered words in English such as \u0026ldquo;Doctor\u0026rdquo; into gendered versions in the target language, such as \u0026ldquo;Doctor\u0026rdquo; in Spanish as opposed to \u0026ldquo;Doctora\u0026rdquo;, the female version of the noun. This is a reflection of the corpus the model was trained on. Algorithmic bias is a contentious topic because it reveals biases we either forgot were there or those that we do not like to be reminded of. In this case, male doctors were much more frequent than female doctors in the target translation language, and as such, the language model learned to estimate that empirical probability distribution. But this example, generally benign in its real outcomes, is probably the most benevolent example of algorithmic bias.\nAs predictive models make their way into diverse aspects of life, the more important the outcome associated with the prediction generated by an algorithm, the more care we must take with the implicit biases in its training data. One noxious example is that of bail decisions, an approach supported by quasi-experimental methods as an improvement over judges deciding by themselves as measured by the change in the defendant\u0026rsquo;s welfare.\nIn economics, we differentiate taste-based discrimination from statistical discrimination. The former emphasizes the role of prejudice on decision-making, while the latter is more mechanical. In the absence of perfect information, people must make guesses, based on observable attributes to make rational decisions. A very interesting study of statistical discrimination occurred in the context of the \u0026ldquo;ban the box\u0026rdquo; (BTB) policies. BTB policies prevented employers from asking about job applicant\u0026rsquo;s criminal records until late in the hiring process, under the idea that it was unfair for people with criminal records. The study however finds the opposite effect: with the absence of information, employers engage in statistical discrimination against demographic groups that include more ex-offenders. The study finds that BTB policies decrease the probability of employment by $3.4$ percentage points for young, low-skilled black men.\nIs there a way we could de-bias training data so that the predictive model aligns better with our ethical and moral compass? For the case of machine translation, a group of authors did just that.\nWith our knowledge of embeddings it\u0026rsquo;s pretty intuitive to understand the basics of the approach by Bolukbasi, et al. in Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. The first step is to identify the bias direction. We do this by taking pairs of intrinsically gendered nouns, such as man and woman and averaging the bias across all such pairs we find. The bias for the pair (male, female) for example is defined as $e_{male} - e_{female}$. After we identified the direction of bias, we want to project the bias dimension away. Finally, we want to equalize pairs. This means that definitional words, such as grandfather, should be equidistant from non-definitional nouns, such as doctor.\nNext week\u0026rsquo;s post is here.\nDot-Product | Wikipedia\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week2/","summary":"\u003cp\u003eThis is the second week of the \u003ca href=\"https://www.coursera.org/learn/nlp-sequence-models?specialization=deep-learning/\"\u003efifth course\u003c/a\u003e of DeepLearning.AI\u0026rsquo;s \u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep Learning Specialization\u003c/a\u003e offered on Coursera. In this week, we go a little more in depth into natural language applications with sequence models, and also discuss word embeddings‚Äîan amazing technique for extracting semantic meaning from words.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week2/#introduction-to-word-embeddings\"\u003eIntroduction to Word Embeddings\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week2/#word-representation\"\u003eWord Representation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week2/#using-word-embeddings\"\u003eUsing Word Embeddings\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week2/#properties-of-word-embeddings\"\u003eProperties of Word Embeddings\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week2/#cosine-similarity\"\u003eCosine Similarity\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week2/#embedding-matrix\"\u003eEmbedding Matrix\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week2/#word-embeddings\"\u003eWord Embeddings\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week2/#learning-word-embeddings\"\u003eLearning Word Embeddings\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week2/#word2vec\"\u003eWord2Vec\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week2/#negative-sampling\"\u003eNegative Sampling\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week2/#glove-word-vectors\"\u003eGloVe Word Vectors\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week2/#applications-using-word-embeddings\"\u003eApplications Using Word Embeddings\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week2/#sentiment-classification\"\u003eSentiment Classification\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week2/#de-biasing-word-embeddings\"\u003eDe-biasing Word Embeddings\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"introduction-to-word-embeddings\"\u003eIntroduction to Word Embeddings\u003c/h2\u003e\n\u003ch3 id=\"word-representation\"\u003eWord Representation\u003c/h3\u003e\n\u003cp\u003eWord embeddings are a way of representing words. The approach borrows from dimensionality reduction, and combines it with optimization. These two things allow us to create new word representations that are empirically good with respect to some task. Let\u0026rsquo;s go over how this is possible.\u003c/p\u003e","title":"Sequence Models: Week 2 | NLP \u0026 Word Embeddings"},{"content":"This is the first week of the fifth course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. This week we go over some motivation for sequence models. These are models that are designed to work with sequential data, otherwise known as time-series. Let\u0026rsquo;s get started.\nThis week\u0026rsquo;s topics are:\nWhy Sequence Models? Notation Representing Words Recurrent Neural Network Forward Propagation Different Types of RNNs Language Model and Sequence Generation Vanishing Gradients with RNNs Gated Recurrent Unit Long Short-Term Memory Bidirectional RNN Why Sequence Models? Time-series get to be their own thing, just like in regression analysis. This time, since we are focusing on prediction instead of inference, we are less concerned about the statistical properties of the parameters we estimate, but we\u0026rsquo;d like our models to do very well in their prediction tasks. But how can we exploit temporal information, without using classical methods such as AR methods? The current bag of tricks we have developed so far will only take us some distance. Here are a couple of hiccups:\nVanilla neural networks require all training samples $X$ to be of the same length. What happens when we want to use text as input and each sample can have different length? Sure we can pad, but I hope you\u0026rsquo;re convinced that\u0026rsquo;s not a great idea from the get go. Vanilla neural networks treat every feature $x^{(i)}_i$ as independent. The whole point of time-series is that they have two components: time-invariant and time-variant components. We definitely want to exploit the temporal structure of our data, which the vanilla models won\u0026rsquo;t allow us to. Vanilla neural networks with dense layers explode very quickly in their size. We need some approach, similar to that of CNNs, where parameters are shared to have a feasible approach in terms of computation. Before we get to the thick of it, let\u0026rsquo;s define the notation used in the course and disambiguate some terminology as well.\nNotation Our input will be, as usual, some vector of features. We denote each time step with $t$, so that $x^{\u0026lt;t\u0026gt;}$ is the $t_{th}$ element in our input vector. We also denote the $i_{th}$ training example as $x^{(i)}$, so that $x^{(i)\u0026lt;t\u0026gt;}$ denotes the $i_{th}$ training example\u0026rsquo;s $t_{th}$ time step. Similarly, we will also denote the size of our input with the variable $T_x$. All of this applies also to our labels $y$. Sometimes $T_x = T_y$, and sometimes it\u0026rsquo;s not. We will discuss these variations under the different types of RNNs.\nRepresenting Words Since a lot of sequence models deal with words, we need a handy way of representing words and even sentences. We need to start by defining a vocabulary. A vocabulary is simply the set of all words in our corpus. A corpus is natural-language processing equivalent to a dataset. If we are training a model on data scraped from news sites, then all the news articles we scraped will make our corpus.\nSince we have a set of all words in our corpus, more precisely tokens in our corpus, we can assign each word a number. This number is the position of the word in our vocabulary. For example, the word aaron might be in position $2$, while the word zulu might be in position $10,000$. This is assuming that our vocabulary is sorted and that it\u0026rsquo;s size, or cardinality is $10,000$.\nWe can now transform every word in a sentence into a one-hot vector. This simply means creating a vector of size equal to our vocabulary, that is $10,000$ and fill it with zeros. Next, we get the index of a word from our vocabulary, and set that index\u0026rsquo;s element in our vector to $1$. So that every word is represented by a $10,000$ dimensional vector where all the entries are $0$ except for the position that maps to a word in our vocabulary. More precisely, we can represent each element in our input $x^{\u0026lt;t\u0026gt;}$ as a one-hot vector. Thus, a sentence can be represented by a matrix, where we concatenate each word. If we have a sentence with $20$ words, then our representation will be of dimensions $(10000, 20)$.\nSince we can also represent some output sequence $y$ in the same fashion, we will use supervised learning to learn a mapping that goes from $x \\to y$.\nAs we mentioned before, there are two main issues with using a standard neural network approach with sequence data.\nInput and outputs can be of different lengths between them and also across training examples. Does not share features across different positions of the sequence. Every element is processed independently of another, which is a pretty bad approach since we are dealing with data that is intrinsically linked in time. We can solve both these issues using a surprisingly simple model, a recurrent neural network (RNN).\nRecurrent Neural Network RNNs is an architectural approach that allows us to express our desire to share parameters, and also to generalize input and output lengths. Let\u0026rsquo;s define that.\nAt each step an RNN will take some input $x^{\u0026lt;t\u0026gt;}$, combine it with the activations (hidden state) from the previous time step $a^{\u0026lt;t-1\u0026gt;}$, and generate $a^{\u0026lt;t\u0026gt;}$. It will use $a^{\u0026lt;t\u0026gt;}$ to generate some output $\\hat{y}^{\u0026lt;t\u0026gt;}$ and also pass $a^{\u0026lt;t\u0026gt;}$ to the next layer $l^{\u0026lt;t+1\u0026gt;}$ to repeat the process again. This means that at each time step we are putting in an input, combining it with the activations from the previous layer and generating an output; so on and so forth until we reach the end of the time steps. Andrew mentions that a lot of the diagrams from RNNs, and later GRUs and LSTM networks, are hard to understand. In that spirit I will focus more on the equations because they are clearer to me.\nA key thing to take note of is that we are sharing parameters across all time steps. There are three important parameter matrices: $W_{ax}$, $W_{aa}$ and $W_{ay}$. In the notation of the course, the subscripts $W_{ax}$ denote that this particular $W$ is being used to generate $a$, and it\u0026rsquo;s using $x$. This means that $W_{ax}$ is used to combine the inputs $x^{\u0026lt;t\u0026gt;}$ and generate $a^{\u0026lt;t\u0026gt;}$. Similarly, $W_{aa}$ is also used to generate $a^{\u0026lt;t\u0026gt;}$, but it uses $a^{\u0026lt;t-1\u0026gt;}$, the activations from the previous time step. This will become clearer once we go over the equations. However, just keep in mind that we have three knobs (parameter matrices):\n$W_{ax}$: regulates how each $x^{\u0026lt;t\u0026gt;}$ affects the activations $a^{\u0026lt;t\u0026gt;}$. $W_{aa}$: regulates how the activations from the previous layer $a^{\u0026lt;t-1\u0026gt;}$ affect our current activations $a^{\u0026lt;t\u0026gt;}$. $W_{ay}$: regulates how the activations $a^{\u0026lt;t\u0026gt;}$ are combined into the current output $\\hat{y}^{\u0026lt;t\u0026gt;}$. Notice that these three matrices are reused at every time step. This means that our parameters will not grow or shrink with respect to the length of our output or input sequences.\nAlso notice the main point of RNNs: as long as $t \u0026gt; 1$ then the output $y^{\u0026lt;k\u0026gt;}$ will be computed using all time steps $t_1, \\dots, t_k$. That is, the future is affected by the past (but not the other way around). If you\u0026rsquo;re interested doing away with the arrow of time, you can use bidirectional RNNs, but that comes later.\nForward Propagation Let\u0026rsquo;s finally define the equations that govern the forward pass in an RNN and hopefully do away with ambiguities in the writing.\nWe said that at every time step, an RNN combines the hidden state from the previous time step with the current inputs to generate an output. How do we get started? That is, what do at $t = 0$? We initialize the hidden state, $a^{\u0026lt;t\u0026gt;}$ to be the zero vector, therefore $a^{\u0026lt;0\u0026gt;} = \\vec{0}$. Thus, we can define the general form of the forward propagation equation:\n$$ \\begin{aligned} a^{\u0026lt;t\u0026gt;} \u0026amp;= g_a (W_{aa} a^{\u0026lt;t - 1\u0026gt;} + W_{ax} x^{\u0026lt;t\u0026gt;} + b_a) \\\\ \\hat{y}^{\u0026lt;t\u0026gt;} \u0026amp;= g_y (W_{ya} a^{\u0026lt;t\u0026gt;} + b_y) \\end{aligned} $$\nNotice the activation functions $g_a, g_y$ might be different. In practice, we use the $\\tanh$ for $g_a$ and the sigmoid for $g_y$ in the case of classification.\nThese two operations are all that\u0026rsquo;s going in an RNN, repeated for every time step of the output. As mentioned before, keep in mind that $T_x = T_y$ for now.\nAnother thing done in the course is to simplify the notation of the equation above, by concatenating $W_{aa}$ and $W_{ax}$ together. For example, if $W_{aa}$\u0026rsquo;s dimensions are $(100, 100)$ and $W_{ax}$\u0026rsquo;s dimensions are $(100, 10000)$, then we can concatenate these into $W_a$. So that:\n$$ \\underset{(100, 10100)}{W_a} = \\left[\\underset{(100, 100)}{W_{aa}} | \\underset{(100, 10000)}{W_{ax}}\\right] $$\nSimilarly, we can stack $a^{\u0026lt;t-1\u0026gt;}$ and $x^{\u0026lt;t\u0026gt;}$ to match the dimensions of $W_a$. So that:\n$$ \\underset{(10100, 1)}{\\left[ a^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}\\right]} = \\begin{bmatrix} \\underset{(100, 1)}{a^{\u0026lt;t-1\u0026gt;}} \\\\ \\hline \\\\ \\underset{(10000, 1)}{x^{\u0026lt;t\u0026gt;}} \\end{bmatrix} $$\nThis way we can simplify the activation function to:\n$$ a^{\u0026lt;t\u0026gt;} = g_a (W_a \\left[ a^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}\\right] + b_a) $$\nWhich is what we\u0026rsquo;ll use through the rest of the week. Notice that this is just a notational trick and also to keep all hidden state parameters together into a single matrix. We are not adding or removing any parameters by doing this notational refinement.\nDifferent Types of RNNs It turns out that setting $T_x = T_y$ is pretty restrictive. We can have different types of RNNs that have different sizes for their input and output. We have the following:\nMany-to-one approach: In sentiment classification we take a sequence and want a single output. We do this by editing the usual RNN approach. Instead of having an output at every time step, $\\hat{y}^{\u0026lt;t\u0026gt;}$, we only have one at the end. A key thing is that we still keep the hidden state going through each time step like a regular RNN. One-to-many approach: In a case like music generation, we might input a single thing, such as a chord, and would like to generate a sequence based on this. This is the mirror opposite of the many-to-one approach. We still have one output at each time step $\\hat{y}^{\u0026lt;t\u0026gt;}$, but we only have one input at the first time step. Many-to-many approach: In this approach, we have two cases: $T_x = T_y$: This is the usual approach. Each RNN unit takes in an input and produces an output. $T_x \\neq T_y$: This approach is usually used in machine translation. The idea is that you first have an encoder, and then a decoder. The encoder takes the inputs and learns an encoding of the inputs, still passing the hidden-state forward in time. While the decoder generates the outputs. Here is an image from the course:\nTypes of RNNs\nLanguage Model and Sequence Generation Language modeling is a probabilistic model of natural language that can generate probabilities of a series of words, based on text corpora in one or multiple languages it was trained on. 1 This means that for a given sentence, our a language model should be able to estimate the probability of that sentence occurring in our corpus. Notice that we can estimate probabilities for sentences that were not seen during training.\nIf we have a sentence such as \u0026ldquo;Cats average 15 hours of sleep a day.\u0026rdquo;, then we can model each token as a one-hot vector. Our tokens will be [cats, average, 15, hours, of, sleep, a, day, \u0026lt;eos\u0026gt;], where \u0026lt;eos\u0026gt; is a special token that denotes the end of sentence. This is useful, because we want our model to be able to learn when it\u0026rsquo;s likely for a sentence to end. We can see how the RNN approach can be used almost directly to estimate the probabilities of this sentence.\nImagine that we train an RNN with the same settings as before, and that we trained it on a large English corpus. Also keep in mind that we want to set $x^{\u0026lt;t\u0026gt;} = y^{\u0026lt;t-1\u0026gt;}$. What happens in the first time step? We just set $x^{\u0026lt;1\u0026gt;}$ to be $\\vec{0}$. If we run this through the first RNN unit, we will get a softmax output over the entire vocabulary. That is, $P(x) \\forall x \\in V$ where $V$ is our vocabulary. We can grab the most likely word, or sample the vocabulary used the softmax probabilities, and generate $x^{\u0026lt;2\u0026gt;}$ from what we just estimated from $y^{\u0026lt;1\u0026gt;}$, so on and so forth.\nThe important idea behind this is that, using an RNN like this amounts to calculating the conditional probabilities for each of the words in the sentence. For example, $y^{\u0026lt;1\u0026gt;} = P(y^{\u0026lt;1\u0026gt;})$, $y^{\u0026lt;2\u0026gt;} = P(y^{\u0026lt;2\u0026gt;} | y^{\u0026lt;1\u0026gt;})$ and so on. When training the neural network, we use the following loss function:\n$$ J\\left(\\hat{y}^{\u0026lt;t\u0026gt;}, y^{\u0026lt;t\u0026gt;}\\right) = - \\sum_{i=1}^m y_i^{\u0026lt;t\u0026gt;} \\log \\hat{y}_i^{\u0026lt;t\u0026gt;} $$\nWhich is the cross-entropy loss we are already familiar with. Therefore, our cost function is just the sum over all time steps:\n$$ \\mathcal{L} = \\sum_{t=1}^{T_y} J^{\u0026lt;t\u0026gt;}\\left(\\hat{y}^{\u0026lt;t\u0026gt;}, y^{\u0026lt;t\u0026gt;}\\right) $$\nThis means that the model learns how to assign probabilities to sentences using conditional probability. For example the second output $\\hat{y}^{\u0026lt;2\u0026gt;} = P(y^{\u0026lt;1\u0026gt;}) P(y^{\u0026lt;2\u0026gt;} | y^{\u0026lt;1\u0026gt;})$, the third output gives us $\\hat{y}^{\u0026lt;3\u0026gt;} = P(y^{\u0026lt;3\u0026gt;} | y^{\u0026lt;1\u0026gt;}, y^{\u0026lt;2\u0026gt;})$. Since we are computing the loss with the sum of cross entropy, we are multiplying all these probabilities (summing in log space) to get the conditional probability of all these words together, given the ones that came before. This is why we can use an RNN as a language model. Even more so, we can generate novel sequences by using the output from one time step into the next step. Notice that we didn\u0026rsquo;t do this just now, we passed the target label from the previous time step to the next input time step.\nVanishing Gradients with RNNs It turns out that vanilla RNNs suffer greatly from vanishing gradient problems. This might not be surprising since each RNN unit is like a layer, and therefore part of the chain rule when we do back propagation. In English, this means that the network is not very good at capturing long-term dependencies in a sequence. For example, in the sentence \u0026ldquo;The cat, which already ate an apple, was full\u0026rdquo;, when compared to \u0026ldquo;The cats, which already ate an apple, were full\u0026rdquo; has a long-term dependency. If it\u0026rsquo;s not clear, the dependency is between the pair $(\\text{cat}, \\text{was})$, and $(\\text{cats}, \\text{were})$. We still have not gotten to bidirectional RNNs, so the network is only scanning from left to right. This means that our optimization algorithm will have a harder and harder time updating the weights of the elements in the sequence that are further from the output towards the beginning of the sequence. This is what we mean by vanishing gradients in this problem.\nIn practice, a vanilla RNN implementation has high local dependencies. This means that some output at time step $t$ will be greatly influenced by the activations from the neighboring time steps before, but not much, if at all, by the ones further behind. This is of course, a problem. We want our language model to realize that seeing either cat or cats should affect the probabilities in the rest of the prediction, and also to memorize this fact as it keeps going down the sequence.\nWe can also have exploding gradients in some cases, which we can ameliorate with gradient clipping. That is, we restrict the gradient values to be within some arbitrary range chosen by us.\nGated Recurrent Unit The Gated Recurrent Unit (GRU) is an improvement over vanilla RNNs that deals with the problem we just discussed, that of vanishing gradients, and therefore the lack of long-term dependency recognition. Before we define the GRU approach, let\u0026rsquo;s go back to how the regular RNN calculated the activations for some time step $t$:\n$$ a^{\u0026lt;t\u0026gt;} = \\tanh (W_a \\left[ a^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}\\right] + b_a) $$\nWhere we simply replaced $g_a$ to use the $\\tanh$ activation function. Let\u0026rsquo;s talk about what $W_a$ is doing. The part that\u0026rsquo;s multiplied with $a^{\u0026lt;t-1\u0026gt;}$ are the parameters that regulate how much the hidden state influences the activation, while the part that\u0026rsquo;s multiplied with $x^{\u0026lt;t\u0026gt;}$ are the parameters that regulate how much each new input sequence influences the activation. That is, in a vanilla RNN each activation is a combination of the hidden state from the previous time step, and the current time step\u0026rsquo;s input.\nWhat the GRU does, is that it allows us to parametrize the mixture between hidden states and the activation; this is on top of $W_a$. The mechanism that performs the mixture is the gate, which is where the name comes from. The gate can be full, closed or anything in between, think sigmoid. When it\u0026rsquo;s open, the mixture from the past is heavy, when it\u0026rsquo;s closed, the mixture from the past is light. Let\u0026rsquo;s clarify this with math.\nLet\u0026rsquo;s say that $c$ is a memory cell, and that in this case $c^{\u0026lt;t\u0026gt;} = a^{\u0026lt;t\u0026gt;}$. We will also define a candidate memory cell $\\tilde{c}^{\u0026lt;t\u0026gt;}$:\n$$ \\tilde{c}^{\u0026lt;t\u0026gt;} = \\tanh (W_c \\left[ c^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}\\right] + b_c) $$\nSo far it\u0026rsquo;s the same as before but with other variable names. Now, we introduce the update gate:\n$$ \\Gamma_u = \\sigma (W_u \\left[ c^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}\\right] + b_u) $$\nLet\u0026rsquo;s keep in mind some properties of $\\Gamma_u$:\n$W_u$ are the parameters that we learn by optimization. The sigmoid guarantees values between $0$ and $1$. The gate has the same dimensions as the hidden state $c^{\u0026lt;t-1\u0026gt;}$. This means that element-wise multiplication can be used. This is similar to applying a mask, but instead of a binary one, a continuous one because of the sigmoid activation. If the value of the gate at some position is close to $0$, then the product is close to $0$ which dampens the effect of the previous hidden state into the next time stamp. On the other hand, we can heighten the hidden state\u0026rsquo;s effect into the next time stamp. We\u0026rsquo;d like to add this gate to the $c^{\u0026lt;t\u0026gt;}$ equation. We can do this by:\n$$ c^{\u0026lt;t\u0026gt;} = \\Gamma_u \\tilde{c}^{\u0026lt;t\u0026gt;} + (1 - \\Gamma_u) c^{\u0026lt;t-1\u0026gt;} $$\nSo that at every activation, we are generating the candidate, and applying the gate over it; thus generating the final activations for the current unit. This means that a GRU not only learns how to mix the past and the present, but also how some tokens or inputs can be \u0026ldquo;context switches\u0026rdquo;; and also how some are not context switches. This means that the model can learn longer-term structural information over a sequence. If there\u0026rsquo;s a token that conditions the rest of the sequence, then the model can learn to update or open the gate. Otherwise, it can learn to keep the gate shut, thus enlarging or diminishing the effect from the past.\nA key thing to note is the element-wise application of the gate. This means that the gate operates at the dimension level, instead of the input level. This means that the network can selectively update some dimensions more aggressively than other dimensions.\nIt turns out that the actual GRU unit has two gates, instead of one; but it\u0026rsquo;s literally the same principle. The full GRU equations are:\n$$ \\begin{aligned} \\Gamma_r \u0026amp;= \\sigma (W_r \\left[ c^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}\\right] + b_r) \\\\ \\tilde{c}^{\u0026lt;t\u0026gt;} \u0026amp;= \\tanh (W_c \\left[\\Gamma_r c^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}\\right] + b_c) \\\\ \\Gamma_u \u0026amp;= \\sigma (W_u \\left[ c^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}\\right] + b_u) \\\\ c^{\u0026lt;t\u0026gt;} \u0026amp;= \\Gamma_u \\tilde{c}^{\u0026lt;t\u0026gt;} + (1 - \\Gamma_u) c^{\u0026lt;t-1\u0026gt;} \\end{aligned} $$\nThe gate $\\Gamma_r$ is the \u0026ldquo;relevance\u0026rdquo; gate. It\u0026rsquo;s a parameter that regulates how relevant the previous memory cell $c^{\u0026lt;t-1\u0026gt;}$ is in the next time step. While $\\Gamma_u$ is the \u0026ldquo;update\u0026rdquo; gate. A parameter that regulates how relevant our candidate memory cell is relative to the previous memory cell.\nNote that in the literature $c$ and $\\tilde{c}$ are usually referred as $h$ and $\\tilde{h}$. Similarly, $\\Gamma_u$ and $\\Gamma_r$ are referred to as $u$ and $r$ respectively.\nLong Short-Term Memory Long Short-Term Memory (LSTM) units are the extension of the GRU idea, even though LSTMs were published earlier. Let\u0026rsquo;s start by reviewing the equations for GRUs since the LSTM is a small extension of them.\nFor the GRU we have:\nCandidate for replacing the current memory cell: $\\tilde{c}^{\u0026lt;t\u0026gt;} = \\tanh (W_c \\left[\\Gamma_r c^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}\\right] + b_c)$ The relevance gate: $\\Gamma_r = \\sigma (W_r \\left[ c^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}\\right] + b_r)$ The update gate: $\\Gamma_u = \\sigma (W_u \\left[ c^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}\\right] + b_u)$ The current memory cell: $c^{\u0026lt;t\u0026gt;} = \\Gamma_u \\tilde{c}^{\u0026lt;t\u0026gt;} + (1 - \\Gamma_u) c^{\u0026lt;t-1\u0026gt;}$ Which is a mixture of the current memory cell and the candidate memory cell mixed by the update gate. $a^{\u0026lt;t\u0026gt;} = c^{\u0026lt;t\u0026gt;}$ This is something that will change in the LSTM. In the LSTM we get rid of $\\Gamma_r$, but we introduce two other gates: $\\Gamma_f$ and $\\Gamma_o$, the forget gate and output gate respectively.\nFor the LSTM we have:\n$$ \\tilde{c}^{\u0026lt;t\u0026gt;} = \\tanh (W_c \\left[a^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}\\right] + b_c) $$\nNotice how we are back to using $a^{\u0026lt;t\u0026gt;}$ instead of $c^{\u0026lt;t\u0026gt;}$, and also that we dropped $\\Gamma_r$ which we will redefine in a second.\nThe update gate remains the same, albeit with the input change:\n$$ \\Gamma_u = \\sigma \\left(W_u[a^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}] + b_u\\right) $$\nWe dropped $\\Gamma_r$ but only to redefine it more explicitly as $\\Gamma_f$, the forget gate:\n$$ \\Gamma_f = \\sigma \\left(W_f[a^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}] + b_f\\right) $$\nWe will add yet another gate, same as the others, $\\Gamma_o$ the output gate:\n$$ \\Gamma_o = \\sigma \\left(W_o[a^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}] + b_o\\right) $$\nThe current memory cell will now use both $\\Gamma_u$ and $\\Gamma_f$ instead of $\\Gamma_u$ and $(1 - \\Gamma_u)$:\n$$ c^{\u0026lt;t\u0026gt;} = \\Gamma_u \\tilde{c}^{\u0026lt;t\u0026gt;} + \\Gamma_f c^{\u0026lt;t-1\u0026gt;} $$\nFinally, we use the output gate $\\Gamma_o$ to generate the final activations:\n$$ a^{\u0026lt;t\u0026gt;} = \\Gamma_o c^{\u0026lt;t\u0026gt;} $$\nWhat does the LSTM get us for the extra size? It gets us more complexity and adaptability. By using more gates, and by keeping the hidden state and the output separate, we are subdividing the tasks more finely than in the GRU; therefore being able to achieve greater specialization.\nBidirectional RNN Bidirectional RNNs (BRRNs) are simply running time forward and backward before doing a prediction. That is, before generating the first output, we\u0026rsquo;ve seen the entire sequence. By keeping two hidden states, one for forward in time, and the other for backwards in time, and then combining both these states with the input, we can use information from the future to make a prediction now. Computationally it\u0026rsquo;s almost two times as expensive as a single RNN.\nIn practice many people use bidirectional GRUs or LSTMs to also allow the model to parametrize the mixture over longer periods of time.\nNext week\u0026rsquo;s post is here.\nLanguage Model | Wikipedia\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week1/","summary":"\u003cp\u003eThis is the first week of the \u003ca href=\"https://www.coursera.org/learn/nlp-sequence-models?specialization=deep-learning/\"\u003efifth course\u003c/a\u003e of DeepLearning.AI\u0026rsquo;s \u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep Learning Specialization\u003c/a\u003e offered on Coursera. This week we go over some motivation for sequence models. These are models that are designed to work with sequential data, otherwise known as time-series. Let\u0026rsquo;s get started.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week1/#why-sequence-models\"\u003eWhy Sequence Models?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week1/#notation\"\u003eNotation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week1/#representing-words\"\u003eRepresenting Words\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week1/#recurrent-neural-network\"\u003eRecurrent Neural Network\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week1/#forward-propagation\"\u003eForward Propagation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week1/#different-types-of-rnns\"\u003eDifferent Types of RNNs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week1/#language-model-and-sequence-generation\"\u003eLanguage Model and Sequence Generation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week1/#vanishing-gradients-with-rnns\"\u003eVanishing Gradients with RNNs\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week1/#gated-recurrent-unit\"\u003eGated Recurrent Unit\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week1/#long-short-term-memory\"\u003eLong Short-Term Memory\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/sequence-models/week1/#bidirectional-rnn\"\u003eBidirectional RNN\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-sequence-models\"\u003eWhy Sequence Models?\u003c/h2\u003e\n\u003cp\u003eTime-series get to be their own thing, just like in regression analysis. This time, since we are focusing on prediction instead of inference, we are less concerned about the statistical properties of the parameters we estimate, but we\u0026rsquo;d like our models to do very well in their prediction tasks. But how can we exploit temporal information, without using classical methods such as AR methods? The current bag of tricks we have developed so far will only take us some distance. Here are a couple of hiccups:\u003c/p\u003e","title":"Sequence Models: Week 1 | Recurrent Neural Networks"},{"content":"This is the fourth and last week of the fourth course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. In this week, we go over special applications in the field of computer vision with CNNs: face recognition and neural style transfer. This week introduces new important concepts that will be useful even beyond the context of CNNs.\nThis week\u0026rsquo;s topics are as follows:\nFace Recognition What is Face Recognition? One Shot Learning Siamese Network Triplet Loss Face Verification and Binary Classification Neural Style Transfer What is Neural Style Transfer? What are deep CNNs learning? Cost Function Content Cost Function Style Cost Function Face Recognition What is Face Recognition? Let\u0026rsquo;s start by going over the important distinction between face verification and face recognition.\nIn the case of face verification, we get a pair of things: an input image, and also a name or ID. The output of such a system is a binary choice that says whether the image corresponds to the name or ID, or whether it does not correspond. On the other hand, in the case of face recognition, we only get a single thing: an image. The output of such a system is whether the input image matches to any $K$ identities already stored in our system\u0026rsquo;s database.\nThis means that face verification is a $1 \\to 1$ procedure, while face recognition is a $1 \\to K$ procedure. The latter is whether we find a match in or not, and this usually means doing $K$ comparisons in the worst case.\nSo far it doesn\u0026rsquo;t sound too bad, but here\u0026rsquo;s the real issue. Imagine that you\u0026rsquo;re implementing a face recognition system at your company\u0026rsquo;s building. How many images can you get of each person? Ideally you\u0026rsquo;d like to get as many as possible, but that\u0026rsquo;s unfeasible; even worse, in practice, we usually only have access to one or at most two pictures of each person. Having only one training example for each \u0026ldquo;class\u0026rdquo; is what defines one-shot learning; having a few is usually called few-shot learning.\nOne Shot Learning So imagine that we go around our company and finally get a face picture of everyone that should be allowed to get into the building. We want to train a model so that when someone walks into the door, and is one of the people allowed, that the model recognizes that this person is allowed from only one picture. Obviously, we also want to keep away unwanted people!\nThe most immediate idea is to train a classifier with a CNN architecture. If we have $K$ employees, then the output layer of our CNN will have $K + 1$ hidden units in the softmax, the extra one for when the input doesn\u0026rsquo;t match anyone. The issue is that this CNN will have terrible performance because of the size of our training data‚Äîremember, we only have $K$ pictures. Even worse, what happens when we hire someone else? We would have to retrain the whole network every time. There has to be a better way.\nThe better way is to move away from classification and to think of a way to learn a similarity function. A similarity function is a function that takes in a pair of elements and outputs their similarity. You might have heard of Jaccard similarity or cosine similarity. In this case, we want to learn a function that takes two images:\n$$ \\text{d}(\\text{img}_1, \\text{img}_2) = \\text{degree of difference between the images} $$\nThen we can set some threshold $\\tau$ and binarize our similarity function:\n$$ \\text{Verification}(\\text{img}_1, \\text{img}_2) = \\begin{cases} \\text{Same} \u0026amp; \\text{if} \\ \\text{d}(\\text{img}_1, \\text{img}_2) \\leq \\tau \\\\ \\text{Different} \u0026amp; \\text{otherwise} \\end{cases} $$\nHopefully, when our colleague Alice walks in the door of the building, the picture taken from the device at the entrance will have the lowest difference when compared to Alice\u0026rsquo;s picture in our database, amongst all other employees. Also, someone who is not our colleague, should not have a similarity less than $\\tau$ amongst anybody else in our employee set. We will see how these kinks are ironed out when we get to triplet loss. Let\u0026rsquo;s first see how we can learn such a function as $\\text{d}(\\text{img}_1, \\text{img}_2)$.\nSiamese Network The idea behind Siamese networks is pretty straightforward: we run a pair of images, $(\\text{img}_1, \\text{img}_2)$, through the same CNN with a fully connected layer at the end, and then use these outputs to compare the images. Let\u0026rsquo;s dig deeper into what this means.\nRemember that CNNs usually reduce the spatial dimensions of the input volume while increasing the channel dimension. Imagine that we have a CNN that takes an image of a face with dimensions $100 \\times 100 \\times 3$ and after reducing this volume through some number of convolutional and max pooling layers, we get a $128$ dimensional vector. This is the same as in a vanilla CNN, but right before we run the fully connected layer through a softmax layer for classification.\nLet\u0026rsquo;s call our input image $x^{(1)}$. Let\u0026rsquo;s call running $x^{(1)}$ through our CNN, and transforming the $100 \\times 100 \\times 3$ input into a $128$ element vector $f(x^{(1)})$, so that $f(x^{(1)}) \\in \\mathbb{R}^{128}$. This $128$ dimensional vector is an encoding of the input; it simply some particular representation of the original image containing a face.\nIf we let $x^{(1)}$ be the image of our employee in the database, and $x^{(2)}$ be the image of the face of the person who just walked in, you might imagine what we want to do. We\u0026rsquo;ll run $x^{(2)}$ through our CNN and encode it in the same way, so that we get $f(x^{(2)})$. Notice that $f(x^{(2)}) \\in \\mathbb{R}^{128}$ as well, and that the encoding was generated with the same CNN as $f(x^{(1)})$.\nNow we can redefine our similarity function: $\\text{d}(\\text{img}_1, \\text{img}_2)$ in terms of these numerical encodings, so that our new similarity function is:\n$$ d(x^{(1)}, x^{(2)}) = ||f(x^{(1)}) - f(x^{(2)})||_2^2 $$\nThis is our old friend, the L2-norm of the difference between the two encodings.\nYou might be thinking, why not compare the two images with the L2-norm directly? Think about what happens when you compare two images of the same person but with different lighting, hairstyle, makeup, etc. It won\u0026rsquo;t work well.\nBut how do we train such a network? Remember we want a CNN that takes as input an image, and generates a $128$ dimensional encoding $f(x^{(i)})$. But of course, not just any encoding! We want that $128$ dimensional encoding to have certain properties:\nIf $x^{(i)}, x^{(j)}$ are the same person, we want $||f(x^{(1)}) - f(x^{(2)})||_2^2$ to be small. If $x^{(i)}, x^{(j)}$ are different persons, we want $||f(x^{(1)}) - f(x^{(2)})||_2^2$ to be large. It turns out that we can use back-propagation to learn such parameters, as long as we can define a loss function that has these properties, i.e. it generates good encodings for comparing images of faces. This is how triplet loss was defined.\nTriplet Loss Triplet loss is called so because it uses three elements: an anchor, a positive and a negative example. The anchor is a picture of one of our employees. The positive example is another picture of the same employee. The negative example is simply a picture of someone else. That is, we distance between the anchor and the positive example should be low, while the distance between the anchor and the negative example should be high. We will use the letters $A, P, N$ to refer to the anchor, positive and negative examples respectively.\nRemember the two properties we wanted out of our encodings, we wanted that the L2-norm of the difference between two encodings to be small if they are of the same person and large if they are not. At least we want the distance to be larger between the anchor and the negative than the distance between the anchor and the positive example. In math, we want:\n$$ \\begin{aligned} ||f(A) - f(P)||_2^2 \u0026amp;\\leq ||f(A) - f(N)||_2^2 \\\\ ||f(A) - f(P)||_2^2 - ||f(A) - f(N)||_2^2 \u0026amp;\\leq 0 \\end{aligned} $$\nThere\u0026rsquo;s an issue with this approach. There is a trivial solution where we set everything to $0$. To prevent our network from learning this solution, we add a margin; similar to the one used in support vector machines. The margin, called $\\alpha$ can be a hyperparameter, so that we have the following:\n$$ ||f(A) - f(P)||_2^2 - ||f(A) - f(N)||_2^2 + \\alpha \\leq 0 $$\nSetting $\\alpha$ allows us to specify how much bigger the difference between the anchor and the negative compared to the anchor and the positive examples should be.\nWe are ready to define our loss function, given three images $A, P, N$:\n$$ \\mathcal{L}(A, P, N) = \\max(||f(A) - f(P)||_2^2 - ||f(A) - f(N)||_2^2 + \\alpha, 0) $$\nWe use the $\\max$ operator here because as long as we have made the difference between the L2-norm of the differences less than $0$ (plus the margin), we have done \u0026ldquo;good\u0026rdquo;. Otherwise, we have done well, and the difference is above zero.\nWe can also now define our cost function, over some $m$ training samples:\n$$ J = \\sum_{i=1}^m \\mathcal{L}(A^{(i)}, P^{(i)}, N^{(i)}) $$\nNotice that to form the triplets, we need at least two pictures of the same person! For example, we could have a training set of $10,000$ images of $1,000$ people, some of them repeated more than once. You might be wondering, sure, but how do we make the triplets? It turns out that this is very important.\nIf we choose $A, P, N$ triplets at random, then the constraints are easily satisfied; but our network will not perform well. We need to choose triplets that are \u0026ldquo;hard\u0026rdquo; to train on, i.e. $\\text{d}(A, P) \\approx \\text{d}(A, N)$. By doing this, we are forcing the network to deal with the harder cases, where people look similar but are not the same person. The details on how to build triplets are described in the FaceNet paper by Schroff, et al.\nFace Verification and Binary Classification It turns out that the triplet loss approach is not the only way to build a face recognition system. We could also use the result of the two CNNs, i.e., the $128$-dimensional embeddings of the pictures, and then feed these to a logistic regression unit and perform binary classification on them; estimating whether the two embeddings are the same or not. The final logistic layer would look like this:\n$$ \\hat{y} = \\sigma \\left( \\sum_{k=1}^{128} w_k ||f(x^{(i)})_{k} - f(x^{(j)})_k||_2 + b \\right) $$\nWhere we are still using the embeddings, but we are calculating an element-wise squared difference, each multiplied by its own weight.\nIn practice, and in both approaches, we can speed up the latency of the system by precomputing the encodings of our employees, and only computing the encoding of the person we are trying to recognize on the fly.\nNeural Style Transfer What is Neural Style Transfer? You might still remember back when neural style transfer was the latest, hottest thing in machine learning. Perhaps even more so, the pointed questions it raised about intellectual property rights. Today, many companies use stable diffusion, a text-to-image system, as an interface for performing neural style transfer.\nNeural style transfer, in a nutshell, is being able to imbue an image with another style. For example, we might have a picture of our cats, but we\u0026rsquo;d like to make that picture look like it was painted by Rembrandt. Neural style transfer allows us to \u0026ldquo;transfer\u0026rdquo; the style of a Rembrandt painting into a picture of our cat, so that it looks like Rembrandt himself painted our furry friend.\nWe will be using the notation $C$ to refer to the content image, in our case our cat. The letter $S$ will represent the style image, in our case a Rembrandt painting. Finally, we will use $G$ to refer to the generated image, that is our cat in the style of Rembrandt.\nWhat are deep CNNs learning? Before getting into neural style transfer, we must understand, at a high level, how the input changes through the layers in a CNN. There is an amazing paper by Zeiler and Fergus, 2013 in which they come up with novel ways to visualize what visual features the filters are learning on each of the layers.\nThe gist of it is that the filters in the shallower (earlier) layers of the network learn to pick apart basic features in our image‚Äîthink vertical lines, diagonal lines, etc. As we progress down the layers, deeper into the CNN, the filters start to learn more abstract features, such as concentric circles, colors, and lines, etc. Even later on, we see that some filters specialize in certain regions of the face: noses, eyes, etc. So on and so forth.\nThis is important to keep in mind, especially in the context of neural style transfer, because we can choose to give a unique weight to each layer in the combination of content and style.\nCost Function Similar to all other applications of supervised learning, we need to establish a cost function that will guide the optimization process. Let\u0026rsquo;s get started.\nWe have our content image $C$ and our style image $S$, and we\u0026rsquo;d like to generate an image $G$ which is some mixture of both $C$ and $S$. We will define our loss function in terms of these elements:\n$$ J(G) = \\alpha J_{content}(C, G) + \\beta J_{style}(S, G) $$\nLet\u0026rsquo;s unpack the formula. Our total cost is a function of two separate costs. The first one is the content cost, that is, how bad our generated image is relative to the original content image, which in our example is a picture of a cat. The second one is the style cost, how bad our generated image is relative to the original style image, which in our example is a Rembrandt painting. We have two hyperparameters $\\alpha$ and $\\beta$ which allow us to adjust the mixture between the two costs.\nBut how do we get $G$? We start by initializing it randomly. In practice, we add some random noise to the original content image $C$, but imagine that we start $G$ completely at random; that is, a random noise picture of dimensions $100 \\times 100 \\times 3$. Then we can use gradient descent to update our random picture $G$:\n$$ G := G - \\frac{\\partial}{\\partial G} J(G) $$\nWe are not updating any parameters! We are directly updating the pixel values of our generated image $G$ at each step of gradient descent, or whatever garden variety optimization algorithm we choose to use. Let\u0026rsquo;s now break down each of the components of the cost function.\nContent Cost Function Remember, our cost function was:\n$$ J(G) = \\alpha J_{content}(C, G) + \\beta J_{style}(S, G) $$\nWe will focus on the $J_{content}(C, G)$ component.\nSay that we are using a pre-trained CNN, such as VGG-19, and that we focus on some layer $l$. Keeping in mind what happens at each layer of a CNN, we will hedge our bets and pick some $l$ that\u0026rsquo;s in the middle‚Äînot too deep and not too shallow. We will focus on the activations for this layer, $a^{[l]}$.\nWe will run both images, $C$ and $G$, through $a^{[l]}$ and generate $a^{l}, a^{l}$. The idea is that if we compare $a^{l}, a^{l}$, and they are similar, then the images have similar content. This means that the filters are picking up similar activations on the features that they specialize in for that particular layer.\nThis comparison is done by our old friend, the L2-norm of the difference:\n$$ J^{[l]}_{content}(C, G) = \\frac{1}{4 n_H^{[l]} n_W^{[l]} n_C^{[l]}}\\sum_{\\text{all entries}}||a^{[l](C)} - a^{[l](G)}||_2^2 $$\nThe normalization factor was chosen by the authors and takes into account the dimensions of $a^{[l]}$.\nThis will make our optimization algorithm set pixel values on $G$ that minimize this difference, so that the generated image has similar content to the content image.\nStyle Cost Function Calculating the content cost is so far not that crazy; doing element-wise squared differences is a reasonable approach for the content. But how can we quantify \u0026ldquo;style\u0026rdquo; and compare our generated image $G$ to our style image $S$ to see how far off they are from each other? Here is where things get fascinating.\nThe authors define \u0026ldquo;style\u0026rdquo; as the pair-wise correlation between the activations across channels. Since each channel learns a different feature, style is defined as the interaction between the features. More specifically, their covariance. If one feature picks up stark lines, and another one picks up colors, then if both of these features have positive or negative covariance, then we can say that this is one of the elements of style. When we do a pair-wise comparison between all channels, we get a pair-wise matrix, which we will call $G^{[l]}$, the style matrix. This matrix will tell us how each of the channels covary with each other, and this, in a sense, is the essence of style.\nWe want to compare the style matrix for both $S$, our style image, and $G$, our generated image, and repeat the comparison we did for the content image, but this time on the style matrix or Gram matrix of both $G$ and $S$. Both style matrices $G^{l}$ and $G^{l}$ will be of the same dimensions: $n_C^{[l]} \\times n_C^{[l]}$.\nLet\u0026rsquo;s start with the style matrix of $S$. We define $a^{l}_{i,j,k}$ as one entry in the activations for layer $l$ using $S$ as the input. We will construct $G^{l}$, the Gram matrix of the activations. The entry $kk\u0026rsquo;$ in the $G^{l}$ is defined as:\n$$ G^{[l](S)}_{kk\u0026rsquo;} = \\sum_{i=1}^{n_H} \\sum_{j=1}^{n_W} a^{[l](S)}_{i,j,k} a^{[l](S)}_{i,j,k\u0026rsquo;} $$\nWe will repeat the same, but for $G$, our generated image:\n$$ G^{[l](G)}_{kk\u0026rsquo;} = \\sum_{i=1}^{n_H} \\sum_{j=1}^{n_W} a^{[l](G)}_{i,j,k} a^{[l](G)}_{i,j,k\u0026rsquo;} $$\nLet\u0026rsquo;s recap:\nWe have two images, $S$ and $G$. We defined the \u0026ldquo;style\u0026rdquo; of an image as the gram matrix of the volume. That is, the element-wise channel covariance. We calculated the gram or style matrix from the activations of layer $l$ for both $G$ and $S$. That is we ran both $G$ and $S$ through the same layer and got some output. It is from this output that we calculate the style matrix for $G$ and $S$ separately. Now, we compare the two in the same fashion we did for the content cost. The style cost is defined as:\n$$ J^{[l]}_{style}(S, G) = \\frac{1}{\\left(2 n_H^{[l]} n_W^{[l]} n_C^{[l]} \\right)^2} \\sum_{k=1}^{n_C^{[l]}} \\sum_{k\u0026rsquo;=1}^{n_C^{[l]}} \\left( G_{kk\u0026rsquo;}^{[l](S)} - G_{kk\u0026rsquo;}^{[l](G)}\\right)^2 $$\nWhere again, the normalization factor in front was set by the authors.\nA final thing to notice about the style function: notice that it\u0026rsquo;s indexed by $l$; we calculate the style function at every layer $l$. The authors define the cost function at the network level as:\n$$ J_{style}(S, G) = \\sum_{l=1}^L \\lambda^{[l]} J^{[l]}_{style}(S, G) $$\nUsing the $\\lambda^{[l]}$ parameter for each layer allows us to mix the more basic features in the shallower layers or the more abstract features in the deeper layers. If we want the generated image to softly follow the style, we choose larger weights for the deeper layers and smaller for the shallower layers. On the other hand, if we want our generated image to strongly follow the style image, we do the opposite: select smaller weights for the deeper layers but larger weights for the shallower layers.\nNow we can come back to the cost function we defined earlier:\n$$ J(G) = \\alpha J_{content}(C, G) + \\beta J_{style}(S, G) $$\nThat\u0026rsquo;s it, this is the last week of the CNN course. The next course is sequential models, which, to me, are much more interesting in terms of applications. The programming exercises for this week were fantastic as well, and I highly suggest that you do them as well.\nNext week\u0026rsquo;s post is here, and it\u0026rsquo;s the first week in the sequence models course.\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week4/","summary":"\u003cp\u003eThis is the fourth and last week of the \u003ca href=\"https://www.coursera.org/learn/convolutional-neural-networks/\"\u003efourth course\u003c/a\u003e of DeepLearning.AI\u0026rsquo;s \u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep Learning Specialization\u003c/a\u003e offered on Coursera. In this week, we go over special applications in the field of computer vision with CNNs: face recognition and neural style transfer. This week introduces new important concepts that will be useful even beyond the context of CNNs.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week4/#face-recognition\"\u003eFace Recognition\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week4/#what-is-face-recognition\"\u003eWhat is Face Recognition?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week4/#one-shot-learning\"\u003eOne Shot Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week4/#siamese-network\"\u003eSiamese Network\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week4/#triplet-loss\"\u003eTriplet Loss\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week4/#face-verification-and-binary-classification\"\u003eFace Verification and Binary Classification\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week4/#neural-style-transfer\"\u003eNeural Style Transfer\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week4/#what-is-neural-style-transfer\"\u003eWhat is Neural Style Transfer?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week4/#what-are-deep-cnns-learning\"\u003eWhat are deep CNNs learning?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week4/#cost-function\"\u003eCost Function\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week4/#content-cost-function\"\u003eContent Cost Function\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week4/#style-cost-function\"\u003eStyle Cost Function\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"face-recognition\"\u003eFace Recognition\u003c/h2\u003e\n\u003ch3 id=\"what-is-face-recognition\"\u003eWhat is Face Recognition?\u003c/h3\u003e\n\u003cp\u003eLet\u0026rsquo;s start by going over the important distinction between face verification and face recognition.\u003c/p\u003e","title":"Convolutional Neural Networks: Week 4 | Face Recognition \u0026 Neural Style Transfer"},{"content":"This is the third week of the fourth course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. The week focuses on object detection and localization, important applications of computer vision where CNNs serve as a building block to more specialized applications.\nThis week\u0026rsquo;s topics are:\nObject Localization Landmark Detection Object Detection Sliding Windows Detection Convolutional Implementation of Sliding Windows Turning fully connected layers into convolutional layers Implementing sliding windows convolutionally Bounding Box Predictions Intersection Over Union Non-max Suppression Anchor Boxes Semantic Segmentation Transpose Convolutions U-Net Architecture Object Localization Object localization is, intuitively, not just detecting an object in an image, but also being able to describe its position in the image. We previously discussed how we can train a classifier using images with CNNs. The new twist is the localization of the object in the image.\nIn image classification, we would train a CNN with a softmax layer at the end, which outputs the probability distribution of a sample $i$ belonging to one of the class set distributions. We will extend this naturally by adding to this final layer some units that are not under the softmax function. Specifically, we will add four hidden units in the output: $b_x, b_y, b_h, b_w$, which will describe the center point of a bounding box around the object $(b_x, b_y)$ and the bounding box\u0026rsquo;s height and width $(b_h, b_w)$.\nWe will literally cram all these things into the final layer so that our output $\\hat{y}$ will be an estimate of $y$ as follows:\n$$ y = \\begin{bmatrix} p_c \\\\ b_x \\\\ b_y \\\\ b_h \\\\ b_w \\\\ c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_K \\end{bmatrix} $$\nWhere $p_c$ is the probability that the image contains any object. $b_x, b_y, b_h, b_w$ represent the bounding box for the object. Finally, $c_1, c_2, \\dots, c_K$ represent the element-wise softmax output for each of the $K$ classes.\nUsing this formulation for the output, we can easily write a loss function $\\mathcal{L}(\\hat{y}, y)$, which uses squared deviations for all the elements in the output vector. In practice, we use squared deviations for all the output elements except the class probabilities, where we use the usual cross-entropy loss in the case of multi-class problems.\nThus, we have a way to train a neural network to perform object classification and localization, as long as we can provide it with an annotated training set with objects and bounding boxes.\nLandmark Detection The idea of having a neural network output some real-numbered value that describes the location of things in the input is very powerful. The idea works because essentially the neural network is performing a regression task for those particular real-valued inputs and outputs, in our case the coordinates for the bounding boxes. Landmark detection is all about this.\nIn landmark detection we annotate many \u0026ldquo;landmarks\u0026rdquo;, each of which is a pair of 2D coordinates in the image. If you\u0026rsquo;ve ever seen footage of actors in goofy black costumes with trackers mounted on their surface, you\u0026rsquo;ve seen the idea of landmarks. For example, if we have a data set consisting of faces, and we annotate some number of landmarks on each face, say 128, we can train a neural network to generate landmarks from unseen data. This can be useful in face recognition, where the idea (basically) is to compare two landmarks of faces.\nAnother idea is that of pose detection, which is more related to the situation described above with actors in costumes with trackers. This is very useful in sports science and analytics, and in video games as well.\nThe idea is very similar, in addition to annotating our training set with the landmarks, we need to edit the final layer in the neural network to output its regression estimates of each landmark. If we have $128$ landmarks, then:\n$$ y = \\begin{bmatrix} p_c \\\\ l_1 \\\\ l_2 \\\\ \\vdots \\\\ l_{128} \\end{bmatrix} $$\nWhere $p_c$ still represents the probability that the image contains an object, but $l_1, \\dots, l_128$ represent the neural network\u0026rsquo;s predictions of the landmarks.\nA key thing is that each landmark, say $l_1$, represents the same \u0026ldquo;position\u0026rdquo; in the image across many training samples. If we are doing face recognition, and $l_1$ is a landmark on the bottom of the bridge of the nose, it must be the same across all training samples for it to work.\nObject Detection Object detection is the next step up from object classification with localization; it occurs when there can be many objects in a single image, and we are interested in the location of all those objects in the image. We combine both previous concepts: localization and landmark detection.\nSliding Windows Detection This approach is the repeated application of classification and localization not in the image as a whole, but in subsets of the image, typically grids. If we divide our input image into cells in a grid, we can run each grid, representing a subset of the image, through the model and generate an output like the one in object detection. That is, for each grid, we will get a vector. If we divide our image into a $19\\times19$ grid, then we will have an output of size $19 \\times 19 \\times N$ where $N$ is the dimension of $\\hat{y}$.\nIf you\u0026rsquo;re thinking that this approach is not computationally efficient, you are right. This issue makes this approach usually unfeasible. Another issue is the granularity of the grid‚Äîhow much is enough? As we make the grid finer grained, how do we deal with overlapping bounding boxes for an object that spans many grid cells? Not to mention the higher computational cost of a more finely grained grid. If we go with a coarser grid, we might hurt performance because, depending on the size ratio between the objects and the grids, we might get a grid having all the objects in the image; and thus hurt performance.\nAll these problems are tackled in an approach called You Only Look Once (YOLO), a seminal paper by Redmond et al. Let\u0026rsquo;s start with the computational cost.\nConvolutional Implementation of Sliding Windows Turning fully connected layers into convolutional layers When we run our classification and localization model on each cell grid, we are running a training sample through a vanilla CNN where some convolutional layers are followed by fully connected layers and then finally into a softmax for predicting the class probabilities. There is a way to convert the latter fully connected layers into convolutional layers, which we will use to reduce computation. The key thing to remember is that each cell in the feature map output is the result of running our model on a subset of the image, that is, a grid.\nIn the vanilla implementation, let\u0026rsquo;s say that we have the following architecture:\nWe start with a $14 \\times 14 \\times 3$ input. Run it through a convolutional layer with $16$ different $5 \\times 5$ filters. We get a $10 \\times 10 \\times 16$ output. We run this through $16$ different $2 \\times 2$ filters in a max pooling layer. Get a $5 \\times 5 \\times 16$ output. We run this through a $400$ fully connected layer. We run this through another $400$ unit, fully connected layer. Finally, we have a $K$ dimensional, let\u0026rsquo;s say $4$ output from the softmax layer. It turns out that we can replace steps $6, 7$ and $8$ with convolutional layers. The key idea is that we can take the $5 \\times 5 \\times 16$ output from layer $4$, and convolve it with $400$ different $5 \\times 5$ filters, to get a $1 \\times 1 \\times 400$ output feature map. Since the filter size is $5 \\times 5$, and the filter must match the channels of the input, each filter will be of size $5 \\times 5 \\times 16$, the same as the input. Each of these filters has randomly initialized weights which the network can learn. This means that we are allowing the network to learn $400$ different linear combinations of the $5 \\times 5 \\times 16$ input through optimization with respect to our supervised learning goal.\nNotice also that a $400$ fully connected layer will have $(400, 5 \\times 5 \\times 16) = (400, 400)$ weight matrix dimensions, for a total of $160,400$ parameters including the biases. This is the same as the convolutional implementation since each filter is $5 \\times 5 \\times 16$, and we have $400$ of them. The key idea is not yet computational savings, but a mathematical equivalence between fully connected layers and convolutional layers.\nImplementing sliding windows convolutionally Let\u0026rsquo;s summarize our network after converting the fully connected layers into convolutional layers:\nWe start with a $14 \\times 14 \\times 3$ input. Run it through a CONV layer with $16$ different $5 \\times 5$ filters. We get a $10 \\times 10 \\times 16$ volume. We run this through $16$ different $2 \\times 2$ filters in a max pooling layer. Get a $5 \\times 5 \\times 16$ output We run this through a convolutional layer with $400$ different $5 \\times 5$ filters. We get a $1 \\times 1 \\times 400$ output. We run this through a convolutional layer with $400$ different $1 \\times 1$ filters. Get a $1 \\times 1 \\times 400$ output. Finally, run this through $K$ different $1 \\times 1$ filters to get a $1 \\times 1 \\times K$ output, where $K$ is the number of classes. Nothing different so far, except that we turned our fully connected layers into convolutional layers.\nRemember that in the sliding window implementation, we must run each grid subset of each training sample through the entire network to produce one grid cell in the output. If we have a $20 \\times 20$ grid, we must run it $400$ times for each training sample; and this is why it\u0026rsquo;s computationally unfeasible.\nAlso remember that if our grid size does not tile the image perfectly, we must add some padding on the right and bottom sides of our image, so that the grid does not go out of bounds. This last idea turns out to be the key to generating all the output grids with a single pass of each training sample through our network. This is why we bothered converting fully connected layers to convolutional layers. Because in each of the passes, for each grid cell, there are a lot of parameters that are shared. However, fully connected layers cannot reuse these parameters. We know that convolutional layers work by parameter sharing.\nLet\u0026rsquo;s say for example that in our $14 \\times 14 \\times 3$ example, we choose to use sliding window of size $13 \\times 13$ with a stride $s = 2$. The output should be $2 \\times 2 \\times K$. In this case, we would have to run our network for times, one for each output grid cell, on each training sample. It turns out that we can add the padding in the beginning, making our image $16 \\times 16 \\times 3$ and run it through the same \u0026ldquo;fully\u0026rdquo; convolutional network to get a $2 \\times 2 \\times 4$ output which is mathematically equivalent to running each grid cell separately. Here is how it looks:\nWe start with a $16 \\times 16 \\times 3$ input. Run it through a CONV layer with $16$ different $5 \\times 5$ filters. We get a $12 \\times 12 \\times 16$ volume. We run this through $16$ different $2 \\times 2$ filters in a max pooling layer. Get a $6 \\times 6 \\times 16$ output We run this through a convolutional layer with $400$ different $5 \\times 5$ filters. We get a $2 \\times 2 \\times 400$ output. We run this through a convolutional layer with $400$ different $1 \\times 1$ filters. Get a $2 \\times 2 \\times 400$ output. Finally, run this through $K$ different $1 \\times 1$ filters to get a $2 \\times 2 \\times K$ output. With this approach, we ran all the output grids in one pass through the network. In the output, each cell corresponds to one sliding window position. Depending on the number of sliding windows we want to use, we can pad the input differently. This is how we get more efficient computation.\nBounding Box Predictions We now have a way to run many grids at the same time in a single pass through our network. However, our bounding boxes are still limited to being defined by the grid we defined, which is a problem. Predefining the grid size limits our model\u0026rsquo;s ability to recognize objects that are bigger than our chosen grid, span multiple ones, or are not really square. Another issue is what to do with overlapping grids. What if two objects are in the same grid? What if our object spans two grids? We need to deal with this.\nWe first deal with multiple objects in the same image. In the YOLO algorithm, the authors use the object localization approach to represent the inputs. They also use the convolutional implementation of sliding window to generate some grid of outputs. Each cell in the output has the same dimensions as each $\\hat{y}$ in the object localization approach. If $K = 3$, that is if we have three classes, then each cell in the output will have $5 + K = 8$ elements:\n$$ y = \\begin{bmatrix} p_c \\\\ b_x \\\\ b_y \\\\ b_h \\\\ b_w \\\\ c_1 \\\\ c_2 \\\\ c_3 \\end{bmatrix} $$\nFor example, if we have a $100 \\times 100 \\times 3$ image, and we want to use a $3 \\times 3$ grid, we can design a fully convolutional neural network to output a $3 \\times 3 \\times 5 + K$, in this case $3 \\times 3 \\times 8$. We are back to running object localization with a sliding window, but now in a convolutional manner. Notice also that because of the way that we have defined the input, we only get one bounding box per grid. We still have not solved the issue of multiple objects in the same grid, but we have solved the issue of an object spanning multiple grids by assigning the object\u0026rsquo;s midpoint to the grid which contains its midpoint. The key idea here is that by using convolutional sliding windows and assigning each object to the center point of each grid, we can get bounding box predictions for each grid if there is an object detected in it, and also which object it is. In practice, the finer-grained grid we use, the lower the chance of having two objects in the same grid; therefore a finer-grained grid than $3 \\times 3$ is used in practice.\nAnother key idea relates to the specification of the bounding boxes. Using $b_x, b_y, b_h, b_w$ is usually normalized to be relative to each grid the object is assigned to. $0 \\leq b_x, b_y \\leq 1$ since they describe the object\u0026rsquo;s center relative to the size of the grid. On the other hand, $b_h, b_w$ could be greater than one if the object\u0026rsquo;s bounding box spans multiple grids. The units are again relative to the size of the grid, so a $b_w$ value of $1.5$ means one and a half grids wide. Next, we will solve what happens when we have overlapping bounding boxes due to overlapping grids.\nIntersection Over Union Intersection over union (IoU) is the approach that we will use for two main things: evaluating our bounding box predictions‚Äîthat is, comparing our prediction to the manually annotated bounding boxes‚Äîbut also to resolve overlapping bounding boxes for the same object.\nIoU is similar to the Jaccard Index, meaning, it\u0026rsquo;s the quotient between the intersection and the union of two sets. In our case, we are interested in the intersection and union of a pair of bounding boxes.\nIn the case of evaluation, we can say that if our predicted bounding box has an IoU $\\geq 0.5$ that it\u0026rsquo;s correct; sometimes we can use a higher threshold also.\nNon-max Suppression We will reuse the IoU idea to implement non-max suppression. The idea is very straightforward: we first drop all the bounding boxes that have some $p_c \\leq 0.6$, that is, we are not very sure that an object is there. Then we pick the bounding box with the highest $p_c$ and consider that as our prediction. Afterwards, we will discard any remaining box with $\\text{IoU} \\geq 0.5$ relative to the box we chose in the previous step. This is done across all grids.\nAnchor Boxes Anchor boxes are yet another tool that is used in the YOLO algorithm. The idea is to predefine some and have more than just one possible shape for bounding boxes. For example, if our example has both tall and skinny rectangles representing pedestrians, and wide and short rectangles representing cars, we might want to use two anchor boxes.\nIn practice, for every anchor box we add, our output $\\hat{y}$ will grow twice as big. That is, if our output was originally $3 \\times 3 \\times 5 + K$, now it will be $3 \\times 3 \\times 5 + K \\times 2$ if we use $2$ anchor boxes.\nAnchor boxes help us resolve when two different objects are assigned to the same grid midpoint, and we could resolve as many objects assigned to the same grid cell as we have anchor boxes.\nSemantic Segmentation Semantic segmentation can be thought of as pixel-level object classification. This is very widely used in autonomous vehicle applications, where the system might want to know where the road starts and ends. Unlike using bounding boxes, where the contours of an object are not exact, semantic segmentation gives us exact object regions in the picture. The approach to solve this problem uses a new architecture, called U-Nets. The original motivation was medical imaging, where precisely locating tumors or body anatomy within medical images was necessary.\nHow can we approach this problem? We could run a CNN classifier for each pixel, but as we saw with the sliding window approach, this is very inefficient. Even using convolutional sliding windows, how can we prevent our output from shrinking in spatial dimensions, as is always the case with CNNs? U-Nets solve this problem by implementing a new operation, the transpose convolution. After reducing the spatial dimensions and growing depth via CNNs, we can use transpose convolutions to \u0026ldquo;blow up\u0026rdquo; the volume back to the original spatial dimensions, while reducing the depth dimension.\nIntuition behind U-Nets\nU-Nets can also be thought of as an autoencoder, where the downsampling part, done by the convolutional layers, is an encoder, and the upsampling part, done by transpose convolutional layers, is a decoder. Let\u0026rsquo;s dive in on how transpose convolutions work to upsample a volume.\nTranspose Convolutions As we mentioned, the key idea behind the transpose convolution is to convolve an input with a filter and generate an output that is bigger than the input.\nLet\u0026rsquo;s say that we have a $2 \\times 2$ input, and we\u0026rsquo;d like to get a $4 \\times 4$ output. To get this desired output size, we have to use a $3 \\times 3$ filter with padding $p = 1$ and a stride $s = 2$. The basic idea is that we multiply each scalar in the input by the entire filter, element wise, and place the resulting 9 values in the output. As we move the filter in the output, we simply add the values that overlap. There is a great explanation of the transpose convolution here.\nU-Net Architecture The basic idea here is that we use regular convolutions to generate and learn our features, usually reducing the spatial dimensions and adding channels. After this, we blow up the intermediate feature map back into the original dimensions with transpose convolutions. But this is not all.\nAnother key idea is to use skip-connections, something we have seen before. By using skip connections from the earlier layers, where the spatial information is richer, into the later layers, where the spatial information is poorer but contextual information is richer, we are able to keep the spatial information from degenerating as we go deeper into the network. This key detail is what helps us reconstruct the spatial information into the output, but also use the contextual information used to classify each pixel.\nA standard U-Net\nThe picture shows why the name U-Net. Notice also that the blocks are the volumes seen from the channel perspective. That is, a wider block has more channels than a thinner block. Let\u0026rsquo;s start with the first, leftmost, half of the network: the downsampling part:\nWe start with our input image of a car. We run the input through two convolutional layers, keeping the spatial dimensions the same, i.e. using a \u0026ldquo;same\u0026rdquo; padding, but adding some filters. These are the black arrows. Then we apply a max pooling operation, and reduce the spatial dimensions by some factor. This is the red downwards arrow. We repeat these two operations until we reach the bottom of the picture. In the bottom of the picture, we have shrunk the spatial dimensions but increased the number of channels by quite a bit, as is usual in CNNs. Our volume has presumably the information needed to classify each pixel, but we have lost the spatial information in the downsampling process. This is where transpose convolutions and skip connections enter the picture.\nAs we go back up into the right half of the picture, we will do the following:\nStart from our down sampled volume, and apply a transpose convolution to increase the number of spatial dimensions but also reduce the number of filters. We concatenate, via the skip connection, the output from the layer before last the max pool operation is applied. We apply regular convolutions to this volume, keeping the dimensions the same. Repeat this operation until we reach our original spatial dimensions. In practice, we usually reduce the spatial dimensions by a factor of 2, and then blow them back up by a factor of two every time until we are back to the original spatial dimensions.\nNext week\u0026rsquo;s post is here.\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week3/","summary":"\u003cp\u003eThis is the third week of the \u003ca href=\"https://www.coursera.org/learn/convolutional-neural-networks/\"\u003efourth course\u003c/a\u003e of DeepLearning.AI\u0026rsquo;s \u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep Learning Specialization\u003c/a\u003e offered on Coursera. The week focuses on object detection and localization, important applications of computer vision where CNNs serve as a building block to more specialized applications.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week3/#object-localization\"\u003eObject Localization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week3/#landmark-detection\"\u003eLandmark Detection\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week3/#object-detection\"\u003eObject Detection\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week3/#sliding-windows-detection\"\u003eSliding Windows Detection\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week3/#convolutional-implementation-of-sliding-windows\"\u003eConvolutional Implementation of Sliding Windows\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week3/#turning-fully-connected-layers-into-convolutional-layers\"\u003eTurning fully connected layers into convolutional layers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week3/#implementing-sliding-windows-convolutionally\"\u003eImplementing sliding windows convolutionally\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week3/#bounding-box-predictions\"\u003eBounding Box Predictions\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week3/#intersection-over-union\"\u003eIntersection Over Union\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week3/#non-max-suppression\"\u003eNon-max Suppression\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week3/#anchor-boxes\"\u003eAnchor Boxes\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week3/#semantic-segmentation\"\u003eSemantic Segmentation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week3/#transpose-convolutions\"\u003eTranspose Convolutions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week3/#u-net-architecture\"\u003eU-Net Architecture\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"object-localization\"\u003eObject Localization\u003c/h2\u003e\n\u003cp\u003eObject localization is, intuitively, not just detecting an object in an image, but also being able to describe its position in the image. We \u003ca href=\"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week2/\"\u003epreviously\u003c/a\u003e discussed how we can train a classifier using images with CNNs. The new twist is the localization of the object in the image.\u003c/p\u003e","title":"Convolutional Neural Networks: Week 3 | Detection Algorithms"},{"content":"This is the second week of the fourth course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. This week is largely a literature review, going over different architectures and approaches that have made large contributions to the field.\nThis week\u0026rsquo;s topics are:\nCase Studies Classic Networks LeNet5 AlexNet VGG16 ResNets Why do ResNets Work? Networks in Networks | 1x1 Convolutions Inception Network Inception Network Architecture MobileNet Depth-wise Convolution Point-wise Convolution MobileNet Architecture Practical Advice for Using ConvNets Transfer Learning Data Augmentation Case Studies We should obviously keep up with the computer vision literature if we are interested in implementing new ideas. However, since CNNs have so many hyperparameters and settings, it\u0026rsquo;s essential to pay attention to the empirically justified advances occurring in the field. Since so many computer vision tasks are similar, many of the core ideas of new approaches can be applied, sometimes identically and sometimes with minor editions, to new applications. Finally, in the age of big data and cheap compute, we can get away with virtually free-riding on somebody else\u0026rsquo;s compute by using their pre-trained model. Let\u0026rsquo;s start with the \u0026ldquo;classic\u0026rdquo; networks that hit the field from the late 90s through the early 10s.\nClassic Networks LeNet5 LeNet5 is a CNN architecture introduced by LeCun et al. in 1998; the paper is here. The application is the recognition of handwritten digits. This paper was one of the first to make a serious splash in the field. Because of this, some design decisions made by the authors were made without the benefit of a lush and active field as it is today. For example, the use of non-ReLU activation functions and average pooling instead of max pooling are things you don\u0026rsquo;t really see today. However, there are some key ideas that the authors introduced.\nA big idea here is that of progressively reducing the height and width of the input volume in each layer, while simultaneously increasing the depth. That is, $n_H, n_W$ go down the layers while $n_C$ goes up. Another idea here is to add fully connected layers‚Äîtwo consecutive ones in this paper‚Äîbefore a softmax layer for the classification. The last four layers in LeNet5 go from a $5 \\times 5 \\times 16$ convolutional layer to a $120$-hidden-unit fully connected layer, then into an $84$-hidden-unit fully connected layer, and finally into a softmax layer with $10$ classes. LeNet5\u0026rsquo;s architecture has about $60,000$ parameters, which is minuscule compared to modern approaches, and even to AlexNet.\nAlexNet This architecture was introduced by Krizhevsky et al. at the ImageNet competition in 2012; the paper is here. AlexNet is very similar to LeNet5, but it\u0026rsquo;s much bigger, with around $60$ million parameters‚Äîabout $1,000$ times bigger. The authors dealt with the increased computational complexity by implementing parallel GPU training, using ReLU activation functions, and also utilizing local response normalization‚Äîa normalization technique applied point-wise across the channels of a volume, which is not very commonly used today.\nVGG16 This architecture was introduced by Simonyan and Zisserman in 2015. Compared to AlexNet, where there are many convolutional layers with different filter sizes and strides, the authors focused on network depth. By using the same hyperparameters for filter sizes, stride, and padding across many layers, they were able to train a deeper network of around 16 to 19 layers. The network is still very large relative to AlexNet and LeNet5, with around $138$ million parameters.\nResNets We move away from \u0026ldquo;classic\u0026rdquo; networks by introducing a key component: the skip connection. He, et al. introduce residual networks, or ResNets, which use skip connections for improving performance. A skip connection is simple, the activations for layer $l$ are summed to the activation of layer $l+2$, so that the activation formula for $l+2$ is:\n$$ a^{[l+2]} = g^{[l+2]}(z^{[l+2]} + a^{[l]}) $$\nTo read more about the author\u0026rsquo;s rationale for coming up with this idea, you can read the original paper. However, there are some key results that arise from using skip connections.\nIn theory, adding more layers to a plain vanilla network could only help with respect to the training error. More complexity means less error. However, in reality, adding more layers usually hurts the performance of our optimization algorithm. The network depth offers diminishing returns, and even negative returns after some point, in terms of performance on the training error. Skip connections make the relationship between depth and training error performance decrease monotonically, allowing us to train much deeper networks‚Äîusually 100 layers‚Äîwithout having the tradeoff between depth and training set performance.\nWhy do ResNets Work? The key idea as to why ResNets have the almost magical property of undoing the link between depth and training set performance is that they allow the network to efficiently learn the identity function. Remember that the identity function is a function that always returns the input value. Intuitively, this means that in the worst case, using skip connections will be as bad as not using them. In practice, using skip connections is actually better because we add more model complexity without facing the \u0026ldquo;cost\u0026rdquo; of adding more complexity when we don\u0026rsquo;t use skip connections.\nMathematically, think about the following. We already established that a skip connection simply adds the activations of two layers back, to the activations of the current layer:\n$$ a^{[l+2]} = g^{[l+2]}(z^{[l+2]} + a^{[l]}) $$\nWe can expand $z^{[l+2]}$:\n$$ a^{[l+2]} = g^{[l+2]}(W^{[l+2]}a^{[l+1]}+b^{[l+2]} + a^{[l]}) $$\nIf $W^{[l+2]}, b^{[l+2]} = 0$, then:\n$$ \\begin{aligned} a^{[l+2]} \u0026amp;= g^{[l+2]}(a^{[l]}) \\\\ a^{[l+2]} \u0026amp;= a^{[l]} \\end{aligned} $$\nThis means that if adding an extra layer actually hurts the network, the network can learn parameters that \u0026ldquo;undo\u0026rdquo; the additional layer! You might be thinking, how come the last two equalities hold? They hold because $g^{[l]} \\ \\forall l \\rightarrow \\text{ReLU}$. That is, we use ReLU as the activation function for all layers. Since ReLU is the identity function when the input is greater than $0$, then it\u0026rsquo;s the same; pretty neat.\nThis only works if the input dimensions are the same in the networks with skip connections. If this is not the case, you can add a convolution step to make sure that the dimensions align without any performance costs.\nNetworks in Networks | 1x1 Convolutions This idea is pretty cool, and it\u0026rsquo;s used in a lot of other architectures that will be discussed later in this week\u0026rsquo;s content. You might already have thought about this: what happens if we convolve a filter of size $1$, that is $f=1$, and use \u0026ldquo;same\u0026rdquo; padding? Lin et al. propose this approach and call it a \u0026ldquo;network in network\u0026rdquo; approach. Let\u0026rsquo;s think about this in the basic case: when the number of channels $n_C = 1$. If the number of channels $n_C=1$, then convolving a matrix with a $1\\times1$ filter is equivalent to matrix-scalar multiplication; we just multiply every entry $M[i, j]$ by the scalar value in the filter, to get $M^*[i, j] = M[i,j] \\times f_0$. But what happens if the number of channels is not one? What happens if we have some volume as an input?\nSay that we have an input volume with dimensions $6 \\times 6 \\times 32$, and we convolve it with a filter of dimensions $1 \\times 1 \\times 32$. After applying a ReLU to the convolution, each entry in the feature map will be the ReLU applied to a dot product. Specifically, the dot product between the filter and a $1 \\times 1$ surface in the input volume. We can think of the $1 \\times 1 \\times 32$ filter as a rod or column, which is dotted with each \u0026ldquo;rod\u0026rdquo; in the input value, where the length of the rod spans the channel dimension. Because each entry in the feature map is the dot product and a non-linearity, this is equivalent to a linear function, which is equivalent to what each layer on a neural network does; hence the name: network in network.\nOkay, but why use it? Beyond having the ability to represent a hidden layer across the channel dimension of the input in a CNN context, there is another powerful application of this approach: dimensionality reduction. If we have a $28 \\times 28 \\times 192$ input, and we convolve it with $32$ different $1 \\times 1$ filters, then we will get a $28 \\times 28 \\times 32$ output. Notice that the dimensionality reduction occurs only in the channel dimension and not the height and width of the input volume. The idea of \u0026ldquo;shrinking\u0026rdquo; or \u0026ldquo;compressing\u0026rdquo; the input volume is a key idea of inception networks.\nInception Network Szegedy, et al. use the idea of $1 \\times 1$ convolutions to compose a new type of layer: the inception layer. The idea is used as a solution to a problem introduced by inception layers. Inception layers allow us to use multiple convolutional operations within a layer and stack the results. Let\u0026rsquo;s say that we have input dimensions $28 \\times 28 \\times 192$, and we want to use multiple filter sizes: $1 \\times 1, 3 \\times 3, 5 \\times 5$. Additionally, we also want to use a max-pooling layer to the input. Why do this? We don\u0026rsquo;t know which filter to use ex ante, so we\u0026rsquo;d like to try them all. Let\u0026rsquo;s say that we use:\n$64$, $1 \\times 1$ filters. Giving us an output of $28 \\times 28 \\times 64$. $128$, $3 \\times 3$ filters. Giving us an output of $28 \\times 28 \\times 128$. $32$, $5 \\times 5$ filters. Giving us an output of $28 \\times 28 \\times 32$ $32$ max pool filters with padding to get a \u0026ldquo;same\u0026rdquo; convolution. Giving us an output of $28 \\times 28 \\times 32$. Notice that the heights and widths match up, so that we can actually stack all of these outputs together into a $28 \\times 28 \\times 256$ volume.\nThe problem, of course, is computational cost. Focusing on the $5 \\times 5$ filters, when using $32$ filters, we need to do a lot of multiplications and summations. Each of the $32$ filters have dimensions $5 \\times 5 \\times 192$. The feature map has dimensions $28 \\times 28 \\times 32$. For each of these entries we need to compute $5 \\times 5 \\times 192 = 4800$ multiplications, and we have $28 \\times 28 \\times 32 = 25,088$ of these; for a total of $4800 \\times 25,088 = 120,422,400$, or around $120$ million operations. And these are just the $5 \\times 5$ filters! Here is where the $1 \\times 1$ convolutions can be helpful as dimensionality reduction.\nBefore doing the convolution operations, we \u0026ldquo;compress\u0026rdquo; the input using $1 \\times 1$ convolutions. If the input is $28 \\times 28 \\times 192$, and we use $16$, $1 \\times 1$ filters, we will get an output with dimensions $28 \\times 28 \\times 16$. The process of \u0026ldquo;compressing\u0026rdquo; is called the bottleneck layer. How many operations are we doing now? There are two steps now: the bottleneck layer and the convolutional layer.\nThe bottleneck layer takes a $28 \\times 28 \\times 192$ input and convolves it with $16$ different $1 \\times 1 \\times 192$ filters. The feature map dimensions should be $28 \\times 28 \\times 16$ and each of these numbers is the result of $1 \\times 1 \\times 192 = 192$ multiplications. For a total of $28 \\times 28 \\times 16 \\times 192 = 2,408,448$ or around $2.4$ million. The second part takes a $28 \\times 28 \\times 16$ input and convolves it with $32$ different $5 \\times 5 \\times 16$ filters. The feature map dimensions should be $28 \\times 28 \\times 32$ and each of these numbers is the result of $5 \\times 5 \\times 16 = 400$ multiplications. For a total of $28 \\times 28 \\times 32 \\times 400 = 10,035,200$ or around $10$ million. Adding both we get around $12.4$ million, which is much less than the original $120$ million. Of course, the number of filters in the bottleneck layer regulates the computational ratio between the two approaches. We now have a way to increase dimensionality reduction to reduce computational cost.\nInception Network Architecture The authors then use inception layers as building blocks of the network, using consecutive inception layers and a very deep network, interspersed with max-pooling blocks to regulate the volume dimensions. Another interesting idea is that they add more than just one softmax layer at the end of the network. They also put some before the final layer. What this does is keep the inception layers from getting too crazy and forgetting that their job should be classification. We don\u0026rsquo;t use the output from these inner softmax layers for the final classification, but we put them there solely to \u0026ldquo;ground\u0026rdquo; the optimization process.\nMobileNet As we probably know by now, CNNs offer big computational savings compared to fully connected neural networks. However, most architectures in the classic section still hover around $200$ million parameters or more. The ideas behind MobileNet were fueled by the computational constraints in mobile devices, such as cellphones or tablets, where most devices are still below double-digit RAM gigabytes. Think even of security cameras; there are clear benefits to running classification models locally on embedded systems instead of relying on networking. One of the key ideas used by Howard, et al. is that of depthwise-separable convolutions.\nLet\u0026rsquo;s start by reviewing normal convolutions. Imagine that we have a $6 \\times 6 \\times 3$ input volume, and we convolve it with a $3 \\times 3$ filter. Each filter is also a volume, a $3 \\times 3 \\times 3$ volume. The output feature map will have a dimension of $4 \\times 4 \\times n_C$ where $n_C$ is the number of filters; we get this number using our trusty formula from the first week. Imagine that we use $5$ such $3 \\times 3 \\times 3$ filters. To calculate the computational cost we multiply the number of filter parameters times the number of filter positions times the number of filters:\n$$ \\text{Computational Cost} = \\text{\\# filter parameters} \\times \\text{\\# filter positions} \\times \\text{\\# of filters} $$\nIn our case each filter has $3 \\times 3 \\times 3 = 27$ parameters, there are $4 \\times 4$ filter positions, one for each entry in the feature map output, and we have $5$ such filters. Totaling $3 \\times 3 \\times 3 \\times 4 \\times 4 \\times 5 = 27 \\times 16 \\times 5 = 2196$ multiply operations. Using depthwise-separable convolutions we can reduce this number. Depthwise-separable convolutions factor regular convolutions into two operations: depth-wise convolution, and point-wise convolution. Let\u0026rsquo;s start with depth-wise convolution.\nDepth-wise Convolution Depth-wise convolution is the same as regular convolution, but instead of convolving an input volume with a filter that is also a volume, we convolve it with as many 2D filters as there are channels in the input. If our input volume has dimensions $6 \\times 6 \\times 3$, then we will do depth-wise convolution with $3$ different $3 \\times 3$ filters separately on each channel. We use $3$ filters because $n_C = 3$. In our case, the computational cost is:\n$$ \\begin{aligned} \\text{Computational Cost} \u0026amp;= \\text{\\# filter parameters} \\times \\text{\\# filter positions} \\times \\text{\\# of filters} \\\\ \\text{Computational Cost} \u0026amp;= (3 \\times 3) \\times (4 \\times 4) \\times 3 \\\\ \\text{Computational Cost} \u0026amp;= 9 \\times 16 \\times 3 \\\\ \\text{Computational Cost} \u0026amp;= 432 \\end{aligned} $$\nAbout $20%$ of the original computational cost, but we\u0026rsquo;re not done yet. We got an output of $4 \\times 4 \\times 3$, and we want an output of $4 \\times 4 \\times 5$ since we were originally using $5$ filters. We go from a $4 \\times 4 \\times 3$ input to a $4 \\times 4 \\times 5$ output feature map via point-wise convolution.\nPoint-wise Convolution Point-wise convolution is where the $1 \\times 1$ convolution idea comes back. Our input is of dimension $4 \\times 4 \\times 3$, but we want a $4 \\times 4 \\times 5$ output; we can get that by stacking $5$ different $1 \\times 1 \\times 3$ filters in a volume. That is, convolving our $4 \\times 4 \\times 3$ input with $5$ different $1 \\times 1 \\times 3$ filters, we will get an output with dimensions $4 \\times 4 \\times 5$. Instead of projecting our features to lower-dimensional space as is the case in dimensionality reduction, we are doing the opposite; we are linearly combining the features into more of them. The computational cost of point-wise convolution is:\n$$ \\begin{aligned} \\text{Computational Cost} \u0026amp;= \\text{\\# filter parameters} \\times \\text{\\# filter positions} \\times \\text{\\# of filters} \\\\ \\text{Computational Cost} \u0026amp;= (1 \\times 1 \\times 3) \\times (4 \\times 4) \\times 5 \\\\ \\text{Computational Cost} \u0026amp;= 3 \\times 16 \\times 5 \\\\ \\text{Computational Cost} \u0026amp;= 240 \\end{aligned} $$\nAbout $10%$ of the original computation cost, and this time we are done. We went from $2196$ to $240 + 432 = 672$ multiply operations, which is a lot better. Depth-wise separable convolutions end up being around $10$ times cheaper than regular convolutions. If you\u0026rsquo;re thinking that such a computational boon does not come free of charge, then you are right. We are able to get cheaper computation by making a key assumption: the space (width and height) is statistically independent (orthogonal) of the channels (depth). That might sound like a bold assumption in the computer vision field. Intuitively, color is not orthogonal to shapes. However, in practice, it works extremely well in terms of model performance; whereby many applications are willing to pay a small accuracy hit for $10$ times cheaper compute.\nMobileNet Architecture The original MobileNet (v1) used $13$ depthwise-separable convolution layers with pooling, fully connected, and softmax layers with great success. The network offered about the same performance as networks with normal convolutional layers for about a tenth of the computational price. This, of course, was not enough; hence the second version, v2, uses a more sophisticated approach.\nMobileNet v2 composes more steps into each convolutional layer. First, we add skip-connections, the same way we did in ResNets. However, we also add two additional steps: expansion before depthwise-separable convolution, and then projection afterwards. How do we expand and project? Using $1 \\times 1$ convolutions, of course!\nSandler, et al. borrow the bottleneck approach from inception networks, which constitute the expansion and projection. Say that we start with a $n \\times n \\times 3$ input. Before doing depthwise-separable convolution, we expand this volume, using $18$ different $1 \\times 1$ filters ($18$ is just a number picked). We get an output of $n \\times n \\times 18$. This step constitutes expansion. Now, we do depthwise-separable convolution on the $n \\times n \\times 18$ volume, and we get a $n \\times n \\times 18$ output (again $18$ is just a number picked). Finally, project the volume into lower dimensional space via point-wise convolution, using $3$ different $1 \\times 1$ filters ($3$ is just a number that matches the input channels) getting a $n \\times n \\times 3$ output. This step constitutes projection.\nThis is pretty nifty: we get some volume, and we expand it, linearly combining its features to generate new ones. Afterwards, we do the cheap version of convolution, using depthwise-separable convolution, learning new features from the expanded ones. Finally, we apply dimensionality reduction via projection, to keep memory usage low. So by using expansion, we allow the network to learn a richer function; and by using projection, we keep the memory usage low. In practice, the MobileNet v2 paper uses this \u0026ldquo;bottleneck\u0026rdquo; block (expansion, depthwise-separable convolution, and projection) 17 times before running through the usual pooling, fully connected, and softmax layers.\nPractical Advice for Using ConvNets Transfer Learning As we\u0026rsquo;ve seen, networks get pretty big and computationally expensive. Many papers release their models as open-source software via public repositories in version control systems. You can use transfer learning, covered in Structuring Machine Learning Models | Week 2. The key idea is to \u0026ldquo;freeze\u0026rdquo; all the layers except the last one from a pre-trained model, and retrain the last layer using our own data. Sometimes you can freeze fewer layers, let\u0026rsquo;s say the first $90%$ of the layers, although this number is not set in stone. Think about what each layer is doing: freezing the earlier layers means reusing the lower-level features from the pre-trained model, while freezing the latter layers means freezing the higher-level features from the pre-trained model. At which point in the feature hierarchy the optimal overlap between the pre-trained model and our application occurs depends on each application and pre-trained model combination.\nData Augmentation Data augmentation means generating new training samples from our original training samples via some process. Within the context of computer vision, this is usually done via mirroring and random cropping. Color shifting is also possible via tools like PCA Color Augmentation, which was a technique used in the AlexNet paper. There are also computational approaches that implement data augmentation concurrently with training. Concurrency in scientific computing is a huge field, and it makes sense to show up in any computationally intensive applications.\nNext week\u0026rsquo;s post is here.\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week2/","summary":"\u003cp\u003eThis is the second week of the \u003ca href=\"https://www.coursera.org/learn/convolutional-neural-networks/\"\u003efourth course\u003c/a\u003e of DeepLearning.AI\u0026rsquo;s \u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep Learning Specialization\u003c/a\u003e offered on Coursera. This week is largely a literature review, going over different architectures and approaches that have made large contributions to the field.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#case-studies\"\u003eCase Studies\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#classic-networks\"\u003eClassic Networks\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#lenet5\"\u003eLeNet5\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#alexnet\"\u003eAlexNet\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#vgg16\"\u003eVGG16\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#resnets\"\u003eResNets\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#why-do-resnets-work\"\u003eWhy do ResNets Work?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#networks-in-networks--1x1-convolutions\"\u003eNetworks in Networks | 1x1 Convolutions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#inception-network\"\u003eInception Network\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#inception-network-architecture\"\u003eInception Network Architecture\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#mobilenet\"\u003eMobileNet\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#depth-wise-convolution\"\u003eDepth-wise Convolution\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#point-wise-convolution\"\u003ePoint-wise Convolution\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#mobilenet-architecture\"\u003eMobileNet Architecture\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#practical-advice-for-using-convnets\"\u003ePractical Advice for Using ConvNets\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#transfer-learning\"\u003eTransfer Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week2/#data-augmentation\"\u003eData Augmentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"case-studies\"\u003eCase Studies\u003c/h2\u003e\n\u003cp\u003eWe should obviously keep up with the computer vision literature if we are interested in implementing new ideas. However, since CNNs have so many hyperparameters and settings, it\u0026rsquo;s essential to pay attention to the empirically justified advances occurring in the field. Since so many computer vision tasks are similar, many of the core ideas of new approaches can be applied, sometimes identically and sometimes with minor editions, to new applications. Finally, in the age of big data and cheap compute, we can get away with virtually free-riding on somebody else\u0026rsquo;s compute by using their pre-trained model. Let\u0026rsquo;s start with the \u0026ldquo;classic\u0026rdquo; networks that hit the field from the late 90s through the early 10s.\u003c/p\u003e","title":"Convolutional Neural Networks: Week 2 | Case Studies"},{"content":"This is the first week of the fourth course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. This course introduces convolutional neural networks, an extremely popular architecture in the field of computer vision.\nThis week\u0026rsquo;s topics are:\nComputer Vision Convolution Convolution in continuous land Convolution in discrete land Back to Edge Detection Learning the Filters Padding Strided Convolutions Convolutions Over Volume One Layer of a CNN Defining the Notation and Dimensions Simple CNN Example Pooling Layers Full CNN Example Why Convolutions? Computer Vision If you can think of any computer vision application today: self-driving cars, medical imaging, face recognition and even visual generative AI; it\u0026rsquo;s very likely that they\u0026rsquo;re using some kind of convolutional architecture. Computer vision is a field of computer science that focuses on enabling computers to identify and understand objects and people in images and videos 1. Identification and understanding are nebulous words, but the key thing is that computer vision involves processing digital images and videos. Let\u0026rsquo;s think about how we could represent an image and use our existing knowledge about neural networks to design a cat classifier.\nSay that we want to develop a classifier that processes a digital image, and it outputs the probability that the image is a cat. How are digital images represented usually? A simple, math-friendly, software-friendly way of representing images is as matrices. An image is a grid of pixels, where each pixel has a tuple $(R, G, B)$, where each element represents the color intensity of each channel. If we have a $64\\times64$ pixel image of a cat, we would need $64\\times64\\times3 = 12288$ numerical types to represent the image. But a $64\\times64$ pixel image is incredibly tiny. A $1000\\times1000$ pixel image, 1 mega-pixel, would require $1000\\times1000\\times3 = 3,000,000 = 3M$ numerical types to be represented. That\u0026rsquo;s a lot of numbers, but it gets worse.\nIf we try to use what we already know, fully-connected feed-forward neural networks, and we set up a single hidden layer with $1,000$ hidden units, then the number of weights will explode. Because we represent each image as a $3M$ vector of features $x_1, x_2, \\dots, x_{3M}$, and we are using a fully-connected layer, each feature $x_n$ is connected to each hidden unit. We represent those weights $W^{[l]}$ as a matrix. And that matrix will have dimensions $(1000, 3M)$, which is about 3 billion parameters, and that\u0026rsquo;s just the first hidden layer.\nSure, we could settle by using smaller images, or compressing larger images - losing detail. But what if there\u0026rsquo;s a way to avoid the parameter explosion, and, also train a model that overfits less? Remember that big model complexity usually means overfitting. It turns out that convolutions do just that.\nConvolution Convolution in continuous land When someone mentions convolutions they might be referring to slightly different things. In the field of math called functional analysis, which is the field that uses functions as its units of inquiry, a convolution is an operation between two functions. What is an operation between two functions? Think of differentiation and integration as examples of functional operators.\nIn this sense, a convolution is a mathematical operation between two functions $f$ and $g$ that produces a third function $f * g$. This third function expresses how the shape of one is modified by the other. It\u0026rsquo;s defined as an integral:\n$$ (f*g) := \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau)d\\tau $$\nPerhaps more didactic, there\u0026rsquo;s a beautiful visual explanation of what a convolution is. It involves plotting $f(t)$ and $g(t - \\tau)$ along the $\\tau$ axis. As $g(t - \\tau)$ is shifted, the convolution $(f*g)(t)$ is plotted, which literally describes the normalized area overlap between the two functions at a particular $t$ value. The integral above is a way to compute that area as the sum of the overlaps over the sliding window.\nAn animation displaying the process and result of convolving a box signal with itself.\nAn animation displaying the process and result of convolving a box signal with a spiky impulse response.\nHow is this at all related to image processing? Images are not represented as continuous functions, but as discrete objects, i.e., $M \\times N \\times C$ matrices. It turns out that the discrete equivalent of the integral above is called multidimensional discrete convolution, and this is what people refer to as convolution within the signal-processing and computer vision context.\nConvolution in discrete land So, similar to how we convolve two functions, $f$ and $g$, to find a third function $f * g$, we want to convolve an image and a filter, or kernel. That is, we want to get some output that is, in essence, a filtered image. Why would we want to apply a filter to an image? It turns out that some filters, or kernels, can tell us where the edges are in an image. That seems like a reasonable place to start building a system that can process digital images.\nLet\u0026rsquo;s start by defining a 2D convolution:\n$$ g(x,y) = w * f(x,y) = \\sum_{dx=-a}^a \\sum_{dy=-b}^{b}w(dx,dy)f(x-dx, y-dy) $$\nWhere $g(x,y)$ is the filtered image, $f(x,y)$ is the original image, and $w$ is the filter kernel. Every element of the filter kernel is considered by $-a \\leq dx \\leq a$ and $-b \\leq dy \\leq b$ 2. In essence, this convolves a filter and an image by moving a filter over the image, calculating the sum of all the element-wise products, and setting that as the output for each element in the output matrix. A convolution takes in two matrices and outputs a matrix, but it\u0026rsquo;s not matrix multiplication. This animation should help clarify what\u0026rsquo;s going on:\n2D Convolution Animation.\nThe smaller $3\\times3$ matrix is the filter, or kernel, and the large matrix is our image. In this case, instead of having three values representing each pixel, it has one. How to convolve over many channels, or colors in this case, is covered later in the course.\nNotice how each of the entries in the output matrix is the result of sliding the filter around the original image, and calculating the sum of all element-wise products. This procedure is what ends up being the discrete equivalent of continuous convolution.\nBack to Edge Detection You might be thinking, how could a matrix (kernel) be used to detect edges? It turns out to be pretty intuitive.\nThink about the following $3\\times3$ filter:\n$$ f = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; -1 \\\\ 1 \u0026amp; 0 \u0026amp; -1 \\\\ 1 \u0026amp; 0 \u0026amp; -1 \\end{bmatrix} $$\nIt turns out that this filter can work as a vertical edge detector. Notice that it\u0026rsquo;s negatively symmetrical about the middle column. If we convolve this filter by a $3 \\times 3$ matrix of equal numbers, the output will be $0$ because the symmetry will cancel everything out. What happens if you convolve this filter with an image that has a vertical edge?\nVertical Edge Detection.\nConvolving an image with an edge with this filter will bring up the regions of the original image where there is a gradient of values in the horizontal dimension; thus generating an image that highlights the vertical edges. In this sense, we defined an edge as: a $3\\times3$ region in the image where there are bright pixels on the left and dark pixels on the right.\nNotice that convolving a $6\\times6$ image by a $3\\times3$ filter results in a $4\\times4$ image. This is the result of convolving without padding, which is covered later.\nIt might not surprise you that a horizontal edge filter is:\n$$ f = \\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\\\ -1 \u0026amp; -1 \u0026amp; -1 \\end{bmatrix} $$\nThat is, a horizontal edge is a $3\\times3$ region in the picture where there are brighter pixels \u0026ldquo;above\u0026rdquo; and darker pixels \u0026ldquo;below\u0026rdquo;.\nBut who came up with these numbers? Why not choose $10$ instead of $1$? It turns out that there are a lot of different filters. Think about finding diagonal edges‚Äîhow would that filter look? But most importantly, how can we know that the filters we use are going to be relevant to our problem domain, i.e., cat classification?\nIf you think that the convolution explanation is not great, it\u0026rsquo;s because I don\u0026rsquo;t really understand convolutions. A great resource is 3Blue1Brown\u0026rsquo;s beautiful But what is a convolution? video explanation of what a convolution is through the lens of statistics and geometry, and also one that directly explains Convolutions.\nLearning the Filters It turns out that we can unleash all the machine learning knowledge from the previous courses and simply learn the filters that perform best with respect to a cost function. If we define a filter as:\n$$ f = \\begin{bmatrix} w_1 \u0026amp; w_2 \u0026amp; w_3 \\\\ w_4 \u0026amp; w_5 \u0026amp; w_6 \\\\ w_7 \u0026amp; w_8 \u0026amp; w_9 \\end{bmatrix} $$\nAnd learn these weights via some optimization algorithm. Do we know which filters will work? No. Can we learn them, whatever they are? Yes!\nIf we revisit our parameter size estimates, having a $1000\\times1000\\times3$ matrix representing an image, and a fully connected hidden layer with $1000$ units, then the size of $W^{[l]}$ would be 3 billion. Because in convolution we slide the same filter over the entire image, we just need $10$ parameters (plus the bias) to start building a convolutional layer. We have broken the relationship between our input size and the parameter size of a hidden layer!\nThe idea of parameter sharing is the powerful idea behind convolutional layers. Not simply because it\u0026rsquo;s computationally efficient, but also because it helps models generalize better.\nPadding In the example above, we convolved a $6\\times6$ image with a $3\\times3$ kernel, and got back a $4\\times4$ output. This is because once you move the filter to the third column, going left to right, you get to the rightmost border of the image. It turns out that there is a handy formula that we can use to calculate the output dimensions:\n$$ \\text{output\\_dim} = n - f + 1 \\times n - f + 1 $$\nIf we have a $n\\times n$ input and convolved it with a $f \\times f$ filter, we get out a $n-f+1 \\times n-f+1$ output. In the case above: $6 - 3 + 1 = 4$.\nBut maybe we don\u0026rsquo;t want to get smaller-sized images after convolving them with our filters. What if we want to convolve them many times? It turns out that there is a solution to this issue, and it\u0026rsquo;s called padding. Padding will help us deal with the two main downsides of convolving without padding:\nShrinking output dimensions Corner and edge pixels only contribute to very few outputs in the feature map (the output). This means that the pixels around the center of the image will end up being over-represented in the output, relative to the corner and edge pixels. So what is padding? It\u0026rsquo;s very simple. Before convolving the image with a filter, you pad the input image, around its edges, with some values. Usually, people use $0$ for the padding, and it\u0026rsquo;s appropriately named zero-padding. So if we have a $6\\times6$ image, and we use a padding of $p=1$, then our padded image will be of dimensions $8 \\times 8$. This is because we pad all edges: left, right, top, and bottom. Again, there is a nice formula that allows us to calculate the feature map\u0026rsquo;s dimensions:\n$$ \\text{output\\_dim} = n+2p-f+1 \\times n+2p-f+1 $$\nPlugging our example into the formula, we get $6 + 2(1) - 3 + 1 = 6$. Notice that this is the same dimensions as our input. By using padding we were able to keep the convolution output from shrinking. This is called a same convolution, because it preserves the input dimensions. That is, for any given input, you can solve for a padding size that will preserve the dimensions. We can solve for $p$:\n$$ \\begin{aligned} n+2p-f+1 \u0026amp;= n^* \\\\ 2p-f+1 \u0026amp;= 0 \\\\ 2p \u0026amp;= f - 1 \\\\ p \u0026amp;= \\frac{f-1}{2} \\end{aligned} $$\nSo in the previous example, if we have a $3\\times3 = f$ filter, then $\\frac{3-1}{2} = 1 = p$. This is why a padding of $1$ made our convolution a same one. This is contrasted with valid convolutions, which is where we use no padding.\nIt turns out that people rarely use even-sized filters, and usually go for odd ones like $3\\times3$, $5\\times5$, etc.\nStrided Convolutions Another basic building block in CNNs is that of stride. A stride is another tool, like padding, that can be thought of as a hyperparameter of a convolution. A stride is very simple: if we move the filter by $1$ location every time, this is equivalent to using a stride of $1$, $s=1$. We could jump two spaces, or three, or however many we want. However, by using larger strides, we have fewer outputs in the feature map; therefore, it needs to be used with care, otherwise, we get back to our original problem of quickly shrinking the dimensions of our feature maps.\nWith strides, the handy-formula for the feature map output changes a little:\n$$ \\text{output\\_dim} = \\Bigl\\lfloor\\frac{n+2p-f}{s} + 1 \\Bigr\\rfloor \\times \\Bigl\\lfloor\\frac{n+2p-f}{s} + 1 \\Bigr\\rfloor $$\nNotice that we take the floor, $\\lfloor x \\rfloor$, to handle the case where the result is not an integer.\nSo to recap:\n$n \\times n$ = image dimensions $f \\times f$ = filter/kernel dimensions $p$ = padding $s$ = stride And the feature map dimensions are given to us by the handy formula:\n$$ \\text{output\\_dim} = \\Bigl\\lfloor\\frac{n+2p-f}{s} + 1 \\Bigr\\rfloor \\times \\Bigl\\lfloor\\frac{n+2p-f}{s} + 1 \\Bigr\\rfloor $$\nNote on difference between convolutions in math vs computer vision. Usually the filter is flipped over the diagonal before convolving. The process of not flipping it is called cross-correlation, while the one where we flip the filter is actually called a discrete convolution. It doesn\u0026rsquo;t matter whether we flip the filter or not in the context of CNNs.\nConvolutions Over Volume By now, we should have a pretty good grasp on 2D convolution. We have an image, and we convolve it with a filter, and we get some output. We can use padding or a stride different from one, but these are details, and we know how to calculate the dimensions of the output, which is one of the key things to keep in mind; in the same way that it\u0026rsquo;s very important to keep in mind the dimensions of any neural net. Yet a snag remains: our images are not 2D, but 3D. Our $64\\times64\\times3$ cat image is a cube. How can we do a convolution over volume?\nIt turns out that instead of using a square filter, we use a cube filter. That is, if our image is $6\\times6\\times3$, then we need the number of channels, $3$, to match the number of channels in our filter. Therefore, our filter will have dimensions $3\\times3\\times3$ if we choose a $3\\times3$ filter. Here is an illustration:\n3D Convolution | Papers With Code.\nThis means that each entry in the output feature map is the element-wise product sum of 27 pairs of numbers; 27 from the image and 27 from the filter, since $3 \\times 3 \\times 3 = 27$. An important thing to notice is that the number of channels in the output is $1$, and not $3$ as in the input or filter.\nBut why stop at one filter? We can use as many filters as we want. Think about using one filter for horizontal edge detection, another one for vertical edge detection, and so on. So if we have $n_c^$ filters, we simply stack them together, as long as they are the same dimensions. Let\u0026rsquo;s say that we have $2$ filters, and they are all dimensions $3\\times3\\times3$. Then our output will be $n - f + 1 \\times n - f + 1 \\times n_c^$, where $n_c^*$ is the number of filters used in the layer.\nLet\u0026rsquo;s run by an example using a $6\\times6\\times3$ input image and two $3\\times3\\times3$ filters. Convolving a $6\\times6\\times3$ image with a $3\\times3\\times3$ filter will result in a $4\\times4$ feature map (remember the handy function). Since we have two filters, the feature map will have dimensions $4\\times4\\times2$. The basic idea being that you can stack multiple feature maps into a cube.\nNotice that the (R, G, B) part of the image is usually called channels. It is also called the depth of the image. In the field of machine learning, a multidimensional array might be called a tensor, but this is not really a tensor in the mathematical sense. Just think about channels as the size of the third dimension in the images.\nOne Layer of a CNN Getting back to neural nets, we need to fit all these new pieces into the framework we\u0026rsquo;ve been using throughout the course. We know that ultimately we want to run some optimization algorithm to learn the filter parameters. But before we do that, we need to bring the notation home to make sure that we can apply all the nice tools we developed throughout.\nLet\u0026rsquo;s set up the problem. We have a $6\\times6\\times3$ image, and that we have two $3\\times3\\times3$ filters. We convolve each filter with the image, and we get a $4\\times4\\times2$ output, remember that the $2$ comes from using two filters, so if we had $10$ filters, the output would be $4\\times4\\times10$. So far so good. The problem is that we don\u0026rsquo;t have non-linearities! Fear not however, because we can add non-linearities the same way we did before, by using activation functions. The key idea is that we apply an activation function to the output of a convolution between the image and one filter. So after convolving each filter and getting a $4\\times4$ output, we pass it through some $a^{[l]}$, our choice of activation function. Not only that, but each filter has its own $b^{[l]}_{n_c^*}$ term, a scalar. So the whole shebang is that we grab each filter, convolve it with the image and get an output. We run that output through an activation function $a^{[l]}$, and then add some $b^{[l]}_{n_c^*}$ bias to the $4\\times4$ output, element-wise. Notice that $b^{[l]}_{n_c^*}$ is indexed by the number of filters in the layer $l$, $n_c^*$. This means that if we have a $3\\times3\\times3$ filter, we don\u0026rsquo;t have just $27$ parameters, but $28$!\nSo in this case, the $6\\times6\\times3$ image plays the role of $a^{[0]}$, our input layer. Remember that we first compute $z^{[1]} = W^{[1]}a^{[0]}+b^{[1]}$. In our case, all the filters we have on our layer play the role of $W^{[1]}$. So the $W^{[1]}a^{[0]}$ term is really the output of a convolution operation. Then we add the biases $b^{[l]}_{n_c^*}$ for each of the filters. Finally, we get $a^{[l]} = g(z^{[1]})$ by passing that output through an activation function (usually a ReLU). This is how we get our final $4\\times4$ output. If we have two filters, then the output will be $4\\times4\\times2$.\nThis is a good practice question in the course: If we have 10 filters that are $3\\times3\\times3$ in one layer of a neural network, how many parameters does that layer have? The answer should be even. And again, the truly cool thing about this is that no matter how big the input image is, the number of parameters remains the same. We are no longer in billion-parameter land.\nDefining the Notation and Dimensions If a layer $l$ is a convolutional layer:\n$f^{[l]}$ = filter size. $p^{[l]}$ = padding $s^{[l]}$ = stride $n_c^{[l]}$ = number of filters Notice that we don\u0026rsquo;t vary these settings across filters within a layer.\nSo, layer $l$ will use the output from layer $l-1$; therefore:\nDimensions of inputs: $n_H^{[l-1]} \\times n_W^{[l-1]} \\times n_c^{[l-1]}$ Dimension of outputs: $n_H^{[l]} \\times n_W^{[l]} \\times n_c^{[l]}$ Where $H$ and $W$ stand for height and width.\nOkay, but how do we actually get $n_H^{[l]}$? We use the trusty formula:\n$$ n^{[l]}_H = \\Bigl\\lfloor\\frac{n^{[l-1]}_H + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1\\Bigr\\rfloor $$\nWe can do this separately for width and height.\nLet\u0026rsquo;s go over the dimensions of the components of the layer:\nEach filter is $f^{[l]} \\times f^{[l]} \\times n_c^{[l-1]}$ (notice the channel matching to the previous layer). Activations are $a^{[l]} = n_H^{[l]} \\times n_W^{[l]} \\times n_c^{[l]}$ (the same as the output, of course). If we are using mini batch gradient descent, we can describe this with a matrix: $A^{[l]} = m \\times n_H^{[l]} \\times n_W^{[l]} \\times n_c^{[l]}$, where $m$ is the mini-batch size. Weights are $f^{[l]} \\times f^{[l]} \\times n_c^{[l-1]} \\times n_c^{[l]}$. This one is important! $n_c^{[l-1]}$ is the number of channels or filters in the previous layer, while $n_c^{[l]}$ is the number of filters in the current layer. The biases $b^{[l]}$ will have one for each filter in layer $l$, therefore it will be of dimensions $1 \\times 1 \\times 1 \\times n_c^{[l]}$, the added dimensions are for broadcasting purposes. Simple CNN Example Let\u0026rsquo;s do a simple example before we go over pooling layers; this should be good practice for calculating the input and output dimensions of the layer.\nLet\u0026rsquo;s start with a $39\\times39\\times3$ input. We want to convolve it with $10$ filters, where $f^{[1]} = 3, s^{[1]} = 1, p^{[1]} = 0$. Using the trusty formula, we should get an output of dimensions $37\\times37\\times10$. Remember $\\frac{39+2(0)-3}{1} + 1 = 37$, and we have $10$ filters, therefore the output channels are $10$.\nLet\u0026rsquo;s say that we have another convolutional layer $l=2$, and this time we use twenty $5\\times5$ filters, so that $f^{[2]} = 5, s^{[2]} = 2, p^{[2]} = 0$. Plugging these numbers into our formula, we start from the last layer\u0026rsquo;s input size: $37\\times37\\times10$ and we get this layer\u0026rsquo;s output dimensions as $17\\times17\\times20$. Finally, let\u0026rsquo;s do another layer, $l=3$, with $40$ filters where $f^{[3]} = 5, s^{[3]}=2, p^{[3]} = 0$. Then we end up with an output size of $7\\times7\\times40$.\nNotice how we started with an input of $39\\times39\\times3$ and end up with a feature map of dimensions $7\\times7\\times40$; we took a cube and reshaped it into a rectangle. In the case of doing classification, we can run the $7\\times7\\times40$ output into a $1960$ hidden-unit fully connected layer that runs its output through a softmax or sigmoid.\nIn summary, the layer types in a CNN are:\nConvolutional Layer | CONV Pooling Layer | POOL Fully connected Layer | FC Pooling layers are discussed next and remain a key part of CNNs.\nPooling Layers Pooling layers reduce some layer\u0026rsquo;s output dimension by applying an aggregating procedure, usually taking the max or average over some region in the output. This is good for two main reasons: smaller outputs are more computationally efficient. However, unlike the output shrinking from filters, the aggregating procedures can boost certain features down the network.\nA pooling layer works similar to a convolutional layer. An example of $2\\times2$ max pooling is shown here:\nMax-pooling with a $2\\times2$ shape\nEach of the outputs in the feature map is simply the maximum value in the input\u0026rsquo;s region overlaid by the filter. We can still use our trusty formula to calculate the output sizes.\nA key thing to notice is that pooling layers have no parameters. That is, they only have hyperparameters, $f$ and $s$‚Äîthat is, their size and the stride. Additionally, another hyperparameter is whether the layer is a max-pooling layer or average-pooling layer. Max-pooling is a lot more popular in the CNN literature.\nFull CNN Example This example goes over a CNN architecture similar to that of LeNet-5, a legendary architecture proposed by Yan LeCun in 1998. The network was implemented to do character recognition originally.\nThe architecture is as follows:\nInput: $32\\times32\\times3$ CONV 1: $f=5, s=1$. Outputs: $28\\times28\\times6$ MAXPOOL 1: $f=2, s=2$. Outputs: $14\\times14\\times6$ CONV 2: $f=5, s=1$. Outputs: $10\\times10\\times16$ MAXPOOL 2: $f=2, s=2$. Outputs: $5\\times5\\times16$ FC3: $120$ units. The weights $W^{[FC3]}$ has dimensions $(120, 400)$. Where $400 = 5\\times5\\times16$. FC4: $84$ units. The weights $W^{[FC4]}$ has dimensions $(120, 84)$ Softmax: The final output layer with $C=9$ classes, one for each digit. A couple of details:\nIn the literature, a convolutional layer is usually referred to as a CONV layer followed by a POOL layer. So CONV 1 and MAXPOOL 1 can be referred to as a single layer. The dimensions of the feature maps tend to decrease. That is, $n_H, n_W$ go down as we go deeper in the network. The number of channels tends to increase; that is, $n_C$ goes up as we go deeper in the network. Here is a table with the dimensions for each layer and also the number of parameters. Verifying the number is a great exercise to test our understanding:\nActivation shape Activation size # Parameters Input (32, 32, 3) 3,072 0 CONV1 $(f=5, s=1)$ (28, 28, 6) 4,704 456 POOL1 $(f=2, s=2)$ (14, 14, 6) 1,176 0 CONV2 $(f=5, s=1)$ (10, 10, 16) 1,600 2416 POOL2 $(f=2, s=2)$ (5, 5, 16) 400 0 FC3 (120, 1) 120 48,120 FC4 (84, 1) 84 10,164 Softmax (10, 1) 10 850 Notice how the activation sizes decrease through the layers. Also, notice how around 94% of all the parameters in the network are part of the fully connected layers. This network has a grand total of $62,006$ parameters, which is a lot less than hundreds of millions.\nWhy Convolutions? Unfortunately, there is no single reason as to why convolutions work so well. However, there are two probable reasons:\nParameter sharing: Since you use the same filter to convolve it across different parts of the image, you can use the same filter many times. This is related to translational invariance, the idea that a CNN is robust to shifted or distorted images. Sparsity of connections: Each output value depends only on a few inputs. This is a mark of low complexity, and it might have an effect similar to regularization; therefore, helping avoid overfitting. Next week\u0026rsquo;s post is here.\nWhat is Computer Vision? | Microsoft\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKernel (image processing) | Wikipedia\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week1/","summary":"\u003cp\u003eThis is the first week of the \u003ca href=\"https://www.coursera.org/learn/convolutional-neural-networks/\"\u003efourth course\u003c/a\u003e of DeepLearning.AI\u0026rsquo;s \u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep Learning Specialization\u003c/a\u003e offered on Coursera. This course introduces convolutional neural networks, an extremely popular architecture in the field of computer vision.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#computer-vision\"\u003eComputer Vision\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#convolution\"\u003eConvolution\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#convolution-in-continuous-land\"\u003eConvolution in continuous land\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#convolution-in-discrete-land\"\u003eConvolution in discrete land\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#back-to-edge-detection\"\u003eBack to Edge Detection\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#learning-the-filters\"\u003eLearning the Filters\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#padding\"\u003ePadding\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#strided-convolutions\"\u003eStrided Convolutions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#convolutions-over-volume\"\u003eConvolutions Over Volume\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#one-layer-of-a-cnn\"\u003eOne Layer of a CNN\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#defining-the-notation-and-dimensions\"\u003eDefining the Notation and Dimensions\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#simple-cnn-example\"\u003eSimple CNN Example\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#pooling-layers\"\u003ePooling Layers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#full-cnn-example\"\u003eFull CNN Example\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/cnns/week1/#why-convolutions\"\u003eWhy Convolutions?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"computer-vision\"\u003eComputer Vision\u003c/h2\u003e\n\u003cp\u003eIf you can think of any computer vision application today: self-driving cars, medical imaging, face recognition and even visual generative AI; it\u0026rsquo;s very likely that they\u0026rsquo;re using some kind of convolutional architecture. Computer vision is a field of computer science that focuses on enabling computers to identify and understand objects and people in images and videos \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e. Identification and understanding are nebulous words, but the key thing is that computer vision involves processing digital images and videos. Let\u0026rsquo;s think about how we could represent an image and use our existing knowledge about neural networks to design a cat classifier.\u003c/p\u003e","title":"Convolutional Neural Networks: Week 1 | Foundations of CNNs"},{"content":"This is the second week of the third course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. This course is less technical than the previous two and focuses instead on general principles and intuition related to machine learning projects.\nThis week\u0026rsquo;s topics are:\nError Analysis Carrying out Error Analysis Cleaning up Incorrectly Labeled Data Build our First System Quickly, then Iterate Mismatched Training and Dev/Test Sets Training and Testing on Different Distributions Bias and Variance with Mismatched Data Distributions Addressing Data Mismatch Learning from Multiple Tasks Transfer Learning Multitask Learning End-to-end Deep Learning What is End-to-end Deep Learning? Whether to use End-to-end Deep Learning Error Analysis Carrying out Error Analysis One of the things that we can do when our model is performing worse than human-level performance is to carry out error analysis. Error analysis is just a fancy name to trying to ascertain what the sources of errors are. This is critical because if we can quickly come up with a \u0026ldquo;ceiling\u0026rdquo; or upper-bound on the improvement of a particular strategy.\nLet\u0026rsquo;s say that our metric is accuracy and that our classifier is achieving $90\\%$ accuracy on the dev set. One thing that can help is to take a look at the samples where the algorithm is not performing correctly. Say for example that our classifier is a cat classifier; and that for some breeds of dogs it\u0026rsquo;s not performing that well. Whether to focus on this problem is key to iterate fast.\nThe main idea is to look at what proportion of the errors are coming from mislabeled dogs. If we grab $\\approx 100$ mislabeled examples from the dev set, we need to count up how many are dogs. If only $5\\%$ of the $100$ mislabeled examples are dogs, then even it we get them all wrong we would reduce our error down to $9.5\\%$. Maybe this is not worth it. Of course this might be different if $50\\%$ or more of our mislabeled examples are dogs.\nWe can extend this idea and evaluate multiple sources of errors in parallel. We simply grab our $100$ mislabeled examples from the dev set and tag each example as belonging to either one or more categories of issues. Then we calculate which of the categories is the highest and try to focus on that.\nThe key takeaway from error analysis is to figure out what approach has the biggest bang for our buck; and it\u0026rsquo;s usually the most prevalent one in our errors.\nCleaning up Incorrectly Labeled Data If we suspect, or can even confirm, that our training set has mislabeled samples, then it might because for concern. There is a lot of literature in econometrics that investigates measurement or observational error and the effect this has on causal inference. Since we are not that interested in causal inference but on prediction instead, we won\u0026rsquo;t go too much into the details. However, a key concept from the field is characterizing the \u0026ldquo;issues\u0026rdquo; in our data. The most important thing is whether the issues are random or systematic. For example, if mislabeled examples are random with respect to other features, then it\u0026rsquo;s as good as we can hope for. However, if all the mislabeled examples are related to black cats, then it\u0026rsquo;s definitely an issue.\nThe key is that if our mislabeled examples are in our training set, and if the mislabeling is occurring at random, then we can rely on our model being robust to the mislabeling.\nWe can implement the same approach described in the previous section and add a tag where the source of error is incorrectly labeled data. We might consider engaging in the usually costly process of relabeling if a majority of the errors come from mislabeled data. A key thing is to make sure that both the dev and test set go through the same changes, i.e. we don\u0026rsquo;t want to fix labels on the dev set but not on the test set. Whether the training set comes from a different distribution than the dev/test set is a topic covered in the following section.\nBuild our First System Quickly, then Iterate This should be pretty intuitive: following the same guidelines from agile development, we should get a baseline running as fast as possible; usually meaning a simple baseline. Being able to diagnose the first round of errors quickly, and then engaging in bias/variance and error analysis allows us to iterate quickly. Many projects fail to even leave the ground due to unjustifiable (sometimes theoretical) complexity before even trying the first ideas.\nMismatched Training and Dev/Test Sets Training and Testing on Different Distributions We have repeatedly worried about having the dev and test set come from different distributions. How bad is it that our training set comes from a different distribution from our dev and test set? Let\u0026rsquo;s go over the example shown in the course.\nWhy would we want to have different distributions between training and dev/test sets? Because doing this might allow we to use a lot more data.\nSay that we have our cat app, where users can upload pictures, and we can classify them as a cat or not. Say that we scraped the web for cat images, and we have $200,000$ such images, called $D_{old}$; all of which are pretty high quality. These images were used for our original train/dev/test splits. However, now our users upload their own images of cats, which are usually blurry and of lesser quality; we have $10,000$ of these user-generated images, called $D_{new}$. How can we incorporate this new data, which comes from a different distribution than our original $200,000$ samples, into our pipeline? Let\u0026rsquo;s go over the different options:\nMix $D_{old}$ and $D_{new}$ into a single dataset $(210,000)$ samples. Shuffle them and split again: Good: This is good because now thrice our train/dev/test splits come from the same distribution. Bad: This is bad (more bad than good) because our $10,000$ are very rare across our splits. On expectation about $4.7\\%$ the data in each split contain the new data. Put $50\\%$ of the new data into the training set $(205,000)$ and keep the other half $(5,000)$ as dev/test sets $2,500$ each. Good: This better reflects where we want to \u0026ldquo;aim\u0026rdquo; with our model. We want to do good on the images that our users upload. Bad: Our training set comes from a different distribution than the dev/test sets. But this is not as bad as not \u0026ldquo;aiming\u0026rdquo; where we actually want to. The key takeaway here is three-fold:\nAlways make sure that our test/dev splits come from the same distribution. Our dev/test set should reflect the main application of our model. In our case it\u0026rsquo;s doing well on images that our users upload and not on random images from the web. Having a train set that comes from a different distribution than our dev/test sets can be justified in cases like the example above. The last point has a particular caveat, which is discussed next.\nBias and Variance with Mismatched Data Distributions If our training set distribution is different from our dev/test distribution, then our the bias/variance analysis we have been discussed will not be correct anymore. Let\u0026rsquo;s revisit the bias/variance analysis.\nAssume that human-level error $\\approx 0$, and that our cat classifier has the following performance:\nWhen train/dev come from the same distribution: Training error: $1\\%$ Dev error: $10\\%$ In the case that our training and dev sets come from the same distribution, we might diagnose this with a variance problem. We are overfitting the training data, and we are not able to generalize to unseen data. However, we cannot apply the same reasoning when the training and dev sets come from different distributions. We cannot say if the error is coming from a variance problem, i.e. not being able to generalize, or from the fact that the dev set comes from a different distribution than the training set. These two things are no longer orthogonal when our training and dev sets come from different distributions.\nIt turns out that further splitting our data can help us determine which one of the two issues is driving our dev error. We can generate a new split, called the training-dev split, which is separate from the train, dev and test sets. The key is that the training-dev set must come from the same distribution as the training set. This means that both the training and training-dev set come from the same distribution, which can be different from the distribution from which the dev/test sets come from. We can extend our bias/variance analysis over all these splits to better understand our model\u0026rsquo;s performance.\nLet\u0026rsquo;s revisit the example above but with a made up train-dev error:\nWhen train/train-dev come from same distribution and dev comes from another distribution: Training error: $1\\%$ Train-dev error: $9\\%$ Dev error: $10\\%$ What does it mean that our model is performing well on the training set but not that well on the train-dev set? The only difference is that our model has not seen the train-dev set. Remember that the train and train-dev sets come from the same distribution. This must mean that we have a variance problem, since the model is not able to generalize to unseen data. Why is it not a bias problem? Because our training error is very close to human-level performance. What about the difference between the train-dev error and the dev error?\nLet\u0026rsquo;s look at a similar example:\nWhen train/train-dev come from same distribution and dev comes from another distribution: Training error: $1\\%$ Train-dev error: $1.5\\%$ Dev error: $10\\%$ It doesn\u0026rsquo;t look like we either a bias nor variance problem. Not a bias problem because our model is performing very close to human-level performance on the training set. Not a variance problem because the gap between our training and train-dev error is very close. What about the gap in performance between the train-dev and the dev set? This is error is a data mismatch error, because the difference between the train-dev set and the dev set is that they come from different distributions. It\u0026rsquo;s not even that our classifier cannot generalize, but that we are simply evaluating its performance on a task that it wasn\u0026rsquo;t trained to perform good on.\nSo in general, the difference in performance between the data splits will give we us sense of where the issue lies:\nHuman-level error: We are assuming that this is approximately equal to Bayes error rate. Training error: The difference between training error and human-level error is the amount of avoidable bias. If this is high, then we have a bias problem. Train-dev error: The difference between the train-dev error and the training error is the amount measuring the inability of our model to generalize to unseen data. If this is high, then we have a variance problem. Dev error: The difference between the dev error and the train-dev error is the error attributable to data mismatch. Remember, the main difference between the dev set and the train-dev set is that they come from different distributions. Which means that this is beyond its ability to generalize. We will see ways of tackling this in the next section. Test error: The difference between test error and dev error is the amount to which our model is overfitting the dev set. Remember that both our dev and test sets should come from the same distribution, so the difference between the two is equivalent to the difference between the train and training-dev set errors. It\u0026rsquo;s a lack of generalization, or a variance problem. The key takeaway is that there are cases when we might desire having a mismatched training and dev/test set distributions. Usually, having more data will help our model perform better; especially if not doing means throwing away a lot of data. However, by doing this we are introducing a new kind of error: data mismatch error. When our training set comes from a different distribution than our dev/test sets, our traditional bias/variance problem will be biased or \u0026ldquo;off\u0026rdquo; by some amount. This \u0026ldquo;off\u0026rdquo; amount will come from the data mismatch; that is the degree to which our training set and dev/test sets come from different distributions. This error should be totally expected. What can we expect by training a model to classify cats using pictures of lions and then evaluating them taxonomic drawings of lions?\nAddressing Data Mismatch If our training and dev/test sets come from different distributions and our improved bias/variance analysis indicates that we have a data mismatch issue, what can we do?\nUnfortunately there are no systematic ways of addressing this problem. At the end of the day, we have data that comes from two different distributions. However, there are a couple of things to do that might help us describe the difference in distributions.\nThe first thing to do is to carry out error analysis and try to understand how the two distributions are different. Notice that this is not an empirical approach, and it might be problematic for high-dimensional data, as more things can be different.\nAnother thing to do is to make training data \u0026ldquo;more similar\u0026rdquo; or to get more data that comes from the distribution of the dev/test sets.\nThere are some data synthesis techniques that are discussed in the course in a very shallow manner, but I think that these come with a lot of issues if we are not experts.\nLearning from Multiple Tasks Transfer Learning Time for transfer learning. It\u0026rsquo;s possible that we might have heard this in the past since it has become very popular in the generative AI field. The concept is pretty simple: imagine that we trained a cat classifier. It might be the case that the model that we learned, or even parts of it, could be used successfully for another computer vision task; such as generating medical diagnosis from medical images.\nThe main idea is that if we trained some model on data $X_{cats}, Y_{cats}$, which in our case is images and labels of cat pictures, then we can use this model to perform classification on another domain, with another data $X_{medicine}, Y_{medicine}$. The first step, training on the cat classification task is called pre-training. The second stage, training on medical images, is called fine-tuning.\nPractically, we carry out pre-training the same way we would do any other application. However, for fine-tuning, we have two approaches. The approach we take depends on the amount of data that we have for fine-tuning, relative to pre-training.\nIf we have a lot of medical images, we might retrain the entire network. We wouldn\u0026rsquo;t start with random weights but from the starting point of our pre-trained model. If we don\u0026rsquo;t have that much data, we can simply retrain the last layer of our network, the output layer, during the fine-tuning. In either case, the output layer will be retrained from scratch, that is: we initialize the output layer to random weights and retrain it via fine-tuning. The difference in approaches is simply whether we only train the output layer, or retrain the entire network.\nWhy and how can this work? We might imagine that within a domain, say image recognition, any neural network that we train for different applications, such as cat classification or medical diagnosis, have a huge overlap. Remember that neural networks amount of feature generation, and the abstract features between two similar applications will have a lot of overlap. This is why a model trained on cat classification might work well for medical diagnosis, because a lot of the low-level features, such as recognizing shapes, borders, etc., will carry over from one domain to the other.\nSo when does transfer learning make sense? An obvious reason we might think of is when we have don\u0026rsquo;t have a lot of data for a particular task. In our example, we might have a lot of data for cat recognition, but not so much for medical imaging. Transfer learning makes sense when we can pre-train on a lot of data, and fine-tune for a specific application, where we have less data than that which is available for pre-training.\nMore specifically, transfer learning makes sense when:\nBoth tasks have the same inputs, e.g. images. We have a lot more data for pre-training than fine-tuning. Abstract/low level features learned during pre-training could be helpful for fine-tuning. The use of many large language models (LLMs) use transfer learning today. Huge models are trained with vast data scraped and curated from the internet. These models are gigantic and therefore contain a lot of the generalizable information that\u0026rsquo;s needed to parse and represent language. These models can then be fine-tuned for different applications such as finance, customer support or making memes.\nMultitask Learning Multitask learning is a similar approach to transfer learning. Similar in the sense where two different tasks might benefit from having shared low-level features. The difference between transfer learning and multitask learning is that instead of having two models: pre-training and fine-tuning; multitask learning has one model that does many things.\nThe example in the course is that of object recognition in an autonomous driving system. In this scenario we want a system that recognizes stop signs, pedestrians, and many other objects related to driving. It turns out that we can approach this problem similarly to a softmax classifier. However, the key difference is that our loss function will sum across the tasks, instead of only the label for that sample, as in the case of a classifier. There are more technical details in the course, but they are very shallow.\nThe key takeaway is that instead of having $N$ models when we have $N$ tasks, we can approach the $N$ tasks with a single model. The core idea is that all the tasks share a lot of the feature representations. If this is not the case, then multitask learning is not a sound way to go. We also, perhaps intuitively by now, need to have about the same amount of data for each task if we expect the model to perform equally across each task. In practice, transfer learning is used a lot more than multitask learning. This is due to a combination of the availability of data for a given task, and the fact that different parties can do the pre-training and fine-tuning separately.\nEnd-to-end Deep Learning What is End-to-end Deep Learning? End-to-end deep learning is in a way a commodification of deep learning applications. If we take a deep learning application that is composed of several steps (models) and replace it with a single step (model), then this would be called end-to-end deep learning. It came as a response to complicated and hand-crafted processes. If we think of speech recognition as an example, then an end-to-end approach would be to go directly from audio to a transcript. This is in juxtaposition to a process where we go from audio, to features, to phonemes, etc.\nThe key takeaway is that end-to-end requires a lot of data in general, depending on how efficient the non-end-to-end process is. An example of success is machine translation. Originally the process of machine translation was composed of many steps. Today, however, larger and larger transformer models can be trained directly on the task of translation.\nWhether to use End-to-end Deep Learning The good thing about end-to-end learning is that we don\u0026rsquo;t need to rely that much on manual feature engineering. In a way, this process is more transparent when compared to a heavily engineered process. This is why we need more data. On the other hand, many hand-designed components are the result of intense research; potentially giving us large efficiency gains. Whether to use end-to-end deep learning depends on if you can keep the baby and throw away the bathwater, which is not always possible. Finally, end-to-end approaches are a lot more data hungry. Therefore, depending on the amount of data available to us, it might not even be a feasible approach.\nNext week\u0026rsquo;s post is here.\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/","summary":"\u003cp\u003eThis is the second week of the \u003ca href=\"https://www.coursera.org/learn/machine-learning-projects?specialization=deep-learning\"\u003ethird course\u003c/a\u003e of DeepLearning.AI\u0026rsquo;s \u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep Learning Specialization\u003c/a\u003e offered on Coursera. This course is less technical than the previous two and focuses instead on general principles and intuition related to machine learning projects.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/#error-analysis\"\u003eError Analysis\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/#carrying-out-error-analysis\"\u003eCarrying out Error Analysis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/#cleaning-up-incorrectly-labeled-data\"\u003eCleaning up Incorrectly Labeled Data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/#build-our-first-system-quickly-then-iterate\"\u003eBuild our First System Quickly, then Iterate\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/#mismatched-training-and-devtest-sets\"\u003eMismatched Training and Dev/Test Sets\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/#training-and-testing-on-different-distributions\"\u003eTraining and Testing on Different Distributions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/#bias-and-variance-with-mismatched-data-distributions\"\u003eBias and Variance with Mismatched Data Distributions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/#addressing-data-mismatch\"\u003eAddressing Data Mismatch\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/#learning-from-multiple-tasks\"\u003eLearning from Multiple Tasks\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/#transfer-learning\"\u003eTransfer Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/#multitask-learning\"\u003eMultitask Learning\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/#end-to-end-deep-learning\"\u003eEnd-to-end Deep Learning\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/#what-is-end-to-end-deep-learning\"\u003eWhat is End-to-end Deep Learning?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/#whether-to-use-end-to-end-deep-learning\"\u003eWhether to use End-to-end Deep Learning\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"error-analysis\"\u003eError Analysis\u003c/h2\u003e\n\u003ch3 id=\"carrying-out-error-analysis\"\u003eCarrying out Error Analysis\u003c/h3\u003e\n\u003cp\u003eOne of the things that we can do when our model is performing worse than human-level performance is to carry out error analysis. Error analysis is just a fancy name to trying to ascertain what the sources of errors are. This is critical because if we can quickly come up with a \u0026ldquo;ceiling\u0026rdquo; or upper-bound on the improvement of a particular strategy.\u003c/p\u003e","title":"Structuring ML Projects: Week 2 | ML Strategy"},{"content":"This is the first week of the third course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. This course is less technical than the previous two and focuses instead on general principles and intuition related to machine learning projects.\nThis week\u0026rsquo;s topics are:\nIntroduction to ML Strategy Why ML Strategy Orthogonalization Setting Up our Goal Single Number Evaluation Metric Satisficing and Optimizing Metrics Train/Dev/Test Distributions Size of Dev and Test Sets When to Change Dev/Test Sets and Metrics? Comparing to Human-Level Performance Why Human-level performance? Avoidable Bias Understanding Human-level Performance Surpassing Human-level Performance Improving your Model Performance Introduction to ML Strategy Why ML Strategy Whenever we are working on a machine learning project, and after we have completed our first iteration on the approach, there might be many things to try next. Should we get more data? Try regularization? Try a bigger network? So many things to choose from. This is why we need high-level heuristics to guide our strategy.\nOrthogonalization Because there are so many things that could be changed simultaneously, a key idea is that of orthogonalization. In the context of computer science, orthogonality refers to a system design property whereby changing one thing changes that one thing only and nothing else. For example, changing the brightness setting on your phone changes only the brightness, and not whether your phone is on silent or not. We can say that these two components are orthogonal.\nIn our machine learning setting, we want to define some things that we can change in an orthogonal way. For example, we want to reduce bias, and reduce bias only; without any side effects. Some of the goals that we want to engage in during a project can be (in order):\nFit training set well on cost function. Fit dev set well on cost function. Fit test set well on cost function. Performs well on the real world. We can make progress toward each of these goals in an orthogonal way via different processes.\nAs a counter-example to orthogonalization think about early stopping. With early stopping you will fit your training set less well but also fit your dev set better; that is changing two things at once. This is why early stopping is usually not a suggested approach in the course.\nSetting Up our Goal Single Number Evaluation Metric The key idea to making progress in our project is to be able to quickly compare different approaches. We can do this by defining a single real-valued evaluation metric.\nAs an example, think that you have two classifiers: $A, B$, and the following performance metrics:\n$A$ - Precision: $95\\%$, Recall: $90\\%$ $B$ - Precision: $98\\%$, Recall: $85\\%$ Remember that precision tells you the proportion of true positives out of true positives and false positives, while recall tells you the proportion of true positives out of true positives and false negatives.\nYou might have an application where recall is more important than precision: classifying flight-risk when setting bails. In this case your single metric would be recall. However, if you\u0026rsquo;re not sure, how can you compare classifiers $A$ and $B$?\nIn this case we can combine precision and recall into the F1-score which is the harmonic mean of both precision and recall.\nAnother example is wanting to implement a model that performs good over different geographical regions. If you have $K$ geographical regions you might have $K$ different evaluation metrics, one for each region. The solution here is to take the average and compare the average of each classifier across all regions and pick the best one.\nThey key take away here is to have a single evaluation metric, and also to have a dev set. With these two things you can quickly iterate over different approaches and pick the best performing one.\nSatisficing and Optimizing Metrics Satisficing is a decision-making strategy introduced by the legendary Herbert A. Simon in 1947. It differs from optimizing metrics by the fact that satisficing metrics don\u0026rsquo;t have to be the best but just good enough instead.\nIn our setting, an optimizing metric might be the performance of our classifier. Whereas latency or training time might be a satisficing metric. Being able to clearly think of which goals are which will help us to iterate faster.\nWe could also have our model performance to be a satisficing metric: we are okay with some number of bad predictions over some period of time.\nThe main takeaway is that if you have $N$ metrics, you should have $1$ optimizing metric and $N-1$ satisficing metrics. This is because optimizing for two metrics is complicated and also not guaranteed to be aligned.\nTrain/Dev/Test Distributions Very simple: make sure our dev and test set come from the same distribution. If we train a classifier to predict defaulting on a loan, having high-income zip codes in our dev set and low-income zip codes in our test sets is a terrible idea. The dev and test sets should reflect the data we expect to get in the future, and is relative to the main application of our machine learning model. In practice this usually means randomly shuffling your data before splitting it.\nSize of Dev and Test Sets This was covered before as well.\nThe key takeaway is to make sure that our test set is big enough to reduce the probability that the result obtained is not due to random chance. This could be a lot less than $30\\%$ of your data if you have a lot of data.\nIn some settings not having a test set might be okay, but it will never be better than having a test set.\nWhen to Change Dev/Test Sets and Metrics? If we find that our algorithm is performing \u0026ldquo;well\u0026rdquo; but when being used has side effects, we should definitely change our splits and/or metrics.\nThe example in the course goes over a cat classifier, where classifier $A$ performs better than classifier $B$. However, $A$ also shows some illicit content to some users, while $B$ does not. Clearly you should use $B$. But how can we incorporate this new preference, not showing illicit content, into our metric?\nThe idea is to first think about how to express the new preference, i.e. a penalization for illicit content. Afterwards we can separately (orthogonalization) think about how to do well on this new metric.\nThe key takeaway here is to not marry a metric and/or data split. We should be able to rapidly pivot if we gain new information about our problem and how our model is performing in relationship to our problem.\nComparing to Human-Level Performance Why Human-level performance? Humans are very skilled at some tasks. Remember there are three performance levels for any classification task:\nBayes error rate: The theoretical limit that a perfect classifier would achieve when using random data. It\u0026rsquo;s not $0$. Human-level error: The performance of the best human(s) at the task. It will be higher than Bayes error rate. Training error: The performance of our model on the training set. It might be lower than human-level performance or higher, but never lower than Bayes error rate. Knowing the human-level performance is extremely useful, because it allows us to reason about our training error. If our model is doing better than human-level error, then we might be close to Bayes error rate and improving training set performance might result in overfitting. On the other hand, if we are below human-level error, then it means that we should keep trying to improve our model\u0026rsquo;s performance on the training set. Depending on whether our model is above or below human-level performance can tell us which thing we should focus on next.\nAvoidable Bias The idea of comparing our model\u0026rsquo;s performance to that of humans is very powerful. In general, human-level performance is very close to Bayes error rate; which means that we can, in general, treat human-level performance as the best we can do. The difference between our model\u0026rsquo;s performance and human-level performance (assuming it\u0026rsquo;s a proxy for Bayes error rate) is called avoidable bias. It\u0026rsquo;s called avoidable because, in general, we should be able to perform as good as human-level performance.\nTo cement this idea, and how the comparison can affect the direction of your project, let\u0026rsquo;s go over a simple example. Imagine that we have some classification task with the following performance numbers:\nHuman-level performance: $1\\%$, Training Error: $8\\%$, Dev error: $10\\%$ Because there is still about $8\\% - 1\\% = 7\\%$ avoidable bias, we should focus on reducing bias. Train a larger model, train for longer, reduce regularization, etc. Human-level performance: $7.5\\%$, Training Error: $8\\%$, Dev error: $10\\%$ Because the avoidable bias is smaller than the difference between the dev error and the training error, we should focus more on reducing variance. That is the difference between the dev error and the training error. So how come we reached different conclusions on two classifiers that have the same performance? The only thing that is different is the human-level performance. Because we are thinking of human-level performance as a proxy for Bayes error rate, then if the human-level performance changes, then everything changes. If we don\u0026rsquo;t know human performance, we must make some educated guess about it.\nThe key thing here is to look at the difference between training error and human-level error, which we call avoidable bias. On the other hand, the difference between the dev error and the training error is a measure of variance. We should tackle whichever is greater first.\nUnderstanding Human-level Performance When talking about human-level performance, we usually mean the best performing individual human or group of humans. We don\u0026rsquo;t take whether it takes a village, we just want to know what\u0026rsquo;s the best that the human species can do. We care about this for two reasons:\nIt will inform us about how much we should expect our training error to be. It will give us an upper bound of the Bayes error rate. If you have a bias problem (you haven\u0026rsquo;t reached human-level performance yet) focus on this first. If we are relatively close to human-level performance but have a variance problem, focus on that instead.\nSurpassing Human-level Performance We have been talking about human-level performance for a while now, and you might be thinking: haven\u0026rsquo;t some models beaten humans? Yes, but only in certain problems. Of course the set of problems that have been solved with better-than-human performance is changing every day.\nWithin our context, the thing to keep in mind is that once our model is doing better than human-level performance, we might be very close to Bayes error rate; therefore any improvements in the training error might be very costly, and might actually make our model overfit.\nImproving your Model Performance Okay, so we introduced all these topics above; now how do we put them together?\nWhen doing supervised learning, there are two fundamental assumptions, and how good those assumptions are being met will tell us what do next:\nWe can fit the training set pretty well: This means that we have eliminated all or almost all avoidable bias. The training set performance generalizes pretty well to the dev/test set. This means that we have found a good amount of variance in our model. So if we still have an avoidable bias problem we can try the following:\nTraining a bigger model Train longer and/or with better optimization algorithms Try a specialized NN architecture and/or improve hyperparameters If we solved the bias problem, but we still have a variance problem we can try:\nTry getting more data Add more regularization Try a specialized NN architecture and/or improve hyperparameters Next week\u0026rsquo;s post is here.\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/","summary":"\u003cp\u003eThis is the first week of the \u003ca href=\"https://www.coursera.org/learn/machine-learning-projects?specialization=deep-learning\"\u003ethird course\u003c/a\u003e of DeepLearning.AI\u0026rsquo;s \u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep Learning Specialization\u003c/a\u003e offered on Coursera. This course is less technical than the previous two and focuses instead on general principles and intuition related to machine learning projects.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#introduction-to-ml-strategy\"\u003eIntroduction to ML Strategy\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#why-ml-strategy\"\u003eWhy ML Strategy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#orthogonalization\"\u003eOrthogonalization\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#setting-up-our-goal\"\u003eSetting Up our Goal\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#single-number-evaluation-metric\"\u003eSingle Number Evaluation Metric\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#satisficing-and-optimizing-metrics\"\u003eSatisficing and Optimizing Metrics\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#traindevtest-distributions\"\u003eTrain/Dev/Test Distributions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#size-of-dev-and-test-sets\"\u003eSize of Dev and Test Sets\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#when-to-change-devtest-sets-and-metrics\"\u003eWhen to Change Dev/Test Sets and Metrics?\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#comparing-to-human-level-performance\"\u003eComparing to Human-Level Performance\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#why-human-level-performance\"\u003eWhy Human-level performance?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#avoidable-bias\"\u003eAvoidable Bias\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#understanding-human-level-performance\"\u003eUnderstanding Human-level Performance\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#surpassing-human-level-performance\"\u003eSurpassing Human-level Performance\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/#improving-your-model-performance\"\u003eImproving your Model Performance\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"introduction-to-ml-strategy\"\u003eIntroduction to ML Strategy\u003c/h2\u003e\n\u003ch3 id=\"why-ml-strategy\"\u003eWhy ML Strategy\u003c/h3\u003e\n\u003cp\u003eWhenever we are working on a machine learning project, and after we have completed our first iteration on the approach, there might be many things to try next. Should we get more data? Try regularization? Try a bigger network? So many things to choose from. This is why we need high-level heuristics to guide our strategy.\u003c/p\u003e","title":"Structuring ML Projects: Week 1 | ML Strategy"},{"content":"This is the third and final week of the second course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera.\nThis week\u0026rsquo;s topics are:\nHyperparameter Tuning Tuning Process Random Search Coarse-to-fine Grained Search Using an Appropriate Scale when Searching Python Implementation Hyperparameter Tuning in Practice: Pandas vs. Caviar Batch Normalization Normalizing Activations in a Network Fitting Batch Norm into a Neural Network Why does Batch Norm work? Batch Norm at Test Time Multi-class Classification Softmax Regression Training a Softmax Classifier Programming Frameworks Hyperparameter Tuning We have seen by now that neural networks have a lot of hyperparameters. Remember that hyperparameters remain fixed during training. This means that the process of finding reasonable hyperparameters, called hyperparameter tuning, is a process that is separate from training your model.\nTuning Process As mentioned above, neural networks can have a lot of parameters:\n$\\alpha$: the learning rate. $\\beta$: the EWMA term in momentum. $\\beta_1, \\beta_2, \\epsilon$: the EWMA parameters for Adam. The number of layers $L$. The number of hidden units $n^{[l]}$. Learning rate decay rate if using learning rate decay. The size of mini batches. If it seems daunting, it\u0026rsquo;s because it is. Hyperparameter tuning is necessary most of the time because good hyperparameters from one problem not always translate to other problems. However, there are some hyperparameters that are usually more important than others. This can help you guide your tuning to focus on the most important ones first.\nIn the course, Andrew defines three tiers of importance for hyperparameter tuning, the first ones being more important than the latter ones:\n$\\alpha$ $\\beta$ if using momentum. Using $\\beta = 0.9$ is a good default. The number of hidden layers and the mini-batch size. Learning rate decay, and $L$. If using Adam the defaults of $\\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon=10^{-8}$ usually work fine.\nNow we know where to focus first. But how do we actually search and evaluate values?\nIn the earlier generation of machine learning algorithms, researchers would usually use grid search. Grid search is an exhaustive search over a manually specified subset of the hyperparameter space, usually using cross validation. It\u0026rsquo;s called a grid because if you imagine having two hyperparameters $h_1, h_2$, and some range of values and a step-size for each of the ranges $r_1, r_2$, then you can imagine a grid or matrix where each cell is a combination of $h_1, h_2$ over each of their ranges. For example, you might say, I want to search a value for $\\alpha$ in the interval $[0.001, 0.1]$, and I will draw some equidistant samples in that range and evaluate all the values. Notice that the \u0026ldquo;grid\u0026rdquo; becomes a matrix, or $n$-dimensional tensor as you add more hyperparameters; this is terrible because the volume of the search space increases exponentially with each additional dimension or hyperparameter.\nRandom Search Staying within the simple example of using two hyperparameters $h_1, h_2$, the problem with grid search might be apparent: you evaluate the same value for each hyperparameter multiple times. If set a grid of $9$ cells, then each value of both $h_1, h_2$ will be tested three times while holding the other constant. This is why the recommendation is to use random search: where each combination of $h_1, h_2$ is unique (the probability of repetition being low if you sample appropriately at random). The following image illustrates this point:\nBergstra and Bengio | Hyperparameter Search\nCoarse-to-fine Grained Search Another, optional, recommendation is to first find a promising range for search and then zoom in within the range. This is called coarse-to-fine grained search.\nUsing an Appropriate Scale when Searching So we know that we should sample the hyperparameter space at random. However, this doesn\u0026rsquo;t necessarily mean to do it uniformly at random. The reason for this is that the search scale for each hyperparameter matters a lot. Being able to communicate the desired scale to the random sampling process is crucial for efficient search.\nFor some hyperparameters, such as the size of a hidden-layer $l$, $n^{[l]}$, it might be reasonable to sample uniformly at random. Say that you want to search for $n^{[l]}$ over the range $[50, 100]$. Then values like $89, 57, 62, 89, 74$ might be reasonable (actually sampled at random).\nRemember that a continuous uniform distribution has a PDF of $\\frac{1}{b-a}$ where $a,b$ are the minimum and maximum values. Therefore, every value in the range $[a, b]$ has the same probability of being realized.\nThis will not work for a hyperparameter like $\\alpha$, however. Say that we suspect that a good $\\alpha$ value is in the range $[0.0001, 1]$, and we sample uniformly at random. Since every value has the same likelihood of being drawn, this means that values between $[0.1, 1]$ will be sampled with $90\\%$ probability. What about the interval $[0.0001, 0.1)$? These values have only a $10\\%$ probability of being drawn. This is terrible because we are equally interested in values in both ranges.\nThis is why it makes more sense to sample for $\\alpha$ values on the log scale as opposed to the linear scale. When we search over the linear scale and sample uniformly at random, we will spend $10\\%$ of our effort searching over $[0.0001, 0.1)$ and $90\\%$ of our effort searching over $[0.1, 1]$. On the other hand, when using a log-scale and sampling uniformly at random, we will spend an equal amount of effort searching over $[0.0001, 0.001), [0.001, 0.01), [0.01, 0.1), [0.1, 1]$.\nPython Implementation The way to implement the example above is as follows on Python:\nnp.random.seed(1337) r = -4 * np.random.rand() print(10 ** r) 0.0895161303335359 Let\u0026rsquo;s break this down. Remember that np.random.rand() generates random samples from a uniform distribution over the interval $[0, 1]$. Now, when we want to sample uniformly at random on a log scale, we want to sample uniformly in the exponents. What exponents? In our case, we want to sample uniformly in the range of $[0.0001, 1]$ which is the same as $[10^{-4}, 10^{0}]$. So if we can draw random samples $r$ from a uniform random distribution in the range $[-4, 0]$, then we can plug those into $10^{-r}$ and thus generate the samples in the log scale. How do we get $-4$? It\u0026rsquo;s as simple as $-4 = \\log_{10}0.0001$.\nLet\u0026rsquo;s generate $10$ such values for $\\alpha$ in a vectorized way:\nnp.random.seed(1337) rs = -4 * np.random.rand(10) # we draw 10 samples print(10 ** rs) # this is vectorized over `rs` array([2.31880438e-01, 7.71780714e-02, 1.45456272e-02, 5.19993408e-02, 8.44167674e-03, 8.95835560e-02, 1.24640408e-04, 1.17149864e-03, 3.45862195e-01, 2.85036007e-02]) Hyperparameter Tuning in Practice: Pandas vs. Caviar Andrew mentions that there are two ways that hyperparameter tuning happens in practice: pandas vs. caviar. No, it\u0026rsquo;s not related to the pandas Python package. It\u0026rsquo;s related to how the animal, the giant panda, has offspring. Pandas usually have very few offspring and therefore put a lot of effort into the upbringing of each (don\u0026rsquo;t think too heavily on the veracity of this statement). This is contrasted to other species that lay thousands of eggs, where each egg is almost left to chance (again no offense to the regal Giant Pacific Octopus mothers whom sometimes give their lives for their precious 120,000 to 400,000 eggs).\nThe idea is that sometimes your model is too big, and you cannot afford to train multiple instances of your model, so must babysit it like a little panda. That is, adjusting the hyperparameters over longer periods of time. On the other hand, the caviar approach is where you can afford to train multiple models in parallel with different hyperparameters and then pick the winner.\nBatch Normalization You might remember how we mentioned that normalizing your inputs could help our optimization run faster. The issue with this is that even though our input layer is getting normalized values, the outputs of each layer $l$, $A^{[l]}$ are no longer normalized. Batch normalization is applying the same reasoning but on each layer.\nBatch normalization doesn\u0026rsquo;t have anything to do with batch or mini-batch gradient descent and can be implemented under both approaches.\nA key thing is that we won\u0026rsquo;t normalize $A^{[l]}$ but $Z^{[l]}$ instead. Let\u0026rsquo;s go over the steps.\nNormalizing Activations in a Network For a given layer $l$ in our network, under the batch gradient descent approach, we compute:\n$$ \\begin{equation} Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\end{equation} $$\nNow, we will normalize $Z^{[l]}$ via standardization:\n$$ \\begin{equation} Z^{[l]}_{norm} = \\frac{Z^{[l]} - \\bar{Z}^{[l]}}{S^{[l]} + \\epsilon} \\end{equation} $$\nWhere $\\bar{Z}^{[l]}$ is the sample mean of $Z^{[l]}$ and $S^{[l]}$ is the sample standard deviation of $Z^{[l]}$.\nFinally, we compute $\\tilde{Z}^{[l]}$ by:\n$$ \\begin{equation} \\tilde{Z}^{[l]} = \\gamma^{[l]} Z^{[l]}_{norm} + \\beta^{[l]} \\end{equation} $$\nThere should be nothing new until the last step; we just standardized $Z^{[l]}$. But what is this new $\\tilde{Z}^{[l]}$? It\u0026rsquo;s simply a rescaled and re-shifted version of $Z_{norm}^{[l]}$. Remember that standardizing samples of random variable will make them have approximately mean $0$ and variance $1$ if the random variable is normally distributed. Maybe this particular center and spread is not what\u0026rsquo;s best for our model. Remember what happens in regularization when $Z_i^{[l]} \\approx 0, \\forall i$ and $g^{[l]} = \\sigma(x)$; maybe centering our $Z^{[l]}$ around $0$ makes our neural network approximately linear.\nHaving $\\gamma^{[l]}, \\beta^{[l]}$ allows us to shift and scale $Z^{[l]}_{norm}$ in a way that improves the performance of our model. Yes, this means that $\\gamma^{[l]}, \\beta^{[l]}$ are new parameters that we can learn via our garden variety gradient descent.\nThink about what happens when $\\gamma^{[l]} = S^{[l]} + \\epsilon$ and $\\beta^{[l]} = \\bar{Z}^{[l]}$. In this case the last step undoes the normalization! This is not relevant to the actual implementation but simply to highlight that the learnable parameters are as powerful as standardizing.\nFitting Batch Norm into a Neural Network So now we have $\\gamma^{[l]}, \\beta^{[l]}$ for each of our hidden layers $l$. Remember that $\\beta^{[l]}$ is different from $b^{[l]}$ and also different from the $\\beta$ parameter used in gradient descent with momentum. We can learn these parameters in the same way we have been learning $W^{[l]}, b^{[l]}$; that is updating them on each step of gradient descent. We can even use EWMA methods such as momentum, RMSProp or Adam. One of the amazing things about these approaches is that they\u0026rsquo;re generalizable.\nWhen using mini-batch gradient descent, we use the mean and standard deviation of each mini-batch to standardize. However, the $\\gamma^{[l]}, \\beta^{[l]}$ is the same for all mini-batches within a layer $l$.\nA final detail mentioned in the course is that, when using batch normalization, the parameters $b^{[l]}$ are redundant because they are subtracted when standardizing. Therefore, you can drop $b^{[l]}, \\forall l$ from the set of learnable parameters and simply focus on $\\beta^{[l]}$.\nWhy does Batch Norm work? Earlier we discussed how normalization helps the optimization by undoing weird scales in our data. Batch norm works on the exact principle, but not just on the input layer; it applies this idea to all the layers. Remember that the hidden-layers are the \u0026ldquo;feature generation\u0026rdquo; layers, so this also means that batch norm undoes any weird scaling issues produced by layer $l-1$ which affect layer $l$, but also layer $l+1$ and so on.\nAnother way to think about this is to think about what happens in the following scenario. Imagine that we train a cat classifier using only cat pictures of cats with black fur. Assuming that our classifier performs well, it might not perform well on tasks where cats are allowed to have different fur colors than black. This is called covariate shift, and it simply means that our training data comes from a different distribution than the testing data. What batch norm does it to weaken the coupling between layer $l$ and layers $1, 2, \\dots, l$, which can be thought of as internal covariate shift.\nFinally, batch norm can act as a slight regularization. This occurs because when using batch norm with mini-batches, there is sampling error in the estimates of the mean and variance of each mini-batch. This noise ends up having a similar effect to dropout, which can result in some slight regularization. The noise added is inversely proportional to the mini-batch size by $O\\left(\\frac{1}{\\sqrt{n}}\\right)$, so that the larger the mini-batch size the less the regularization effect.\nBatch Norm at Test Time When we train our network with mini-batch batch norm, we calculate the mean and variance within each mini-batch. What happens when we want to make predictions or test our model? How can we calculate the mean and variance for a single test example?\nWe don\u0026rsquo;t. Instead, we keep a running average of the mean and variance for each layer using our favorite EWMA method during training. The last value is the one we use for testing, that is, the latest EWMA estimates for each layer\u0026rsquo;s mean and variance.\nAndrew also mentions that batch norm is pretty robust to the particular approach you use to estimate the mean and variance when testing time. If you estimate it using your entire training set or if you use an EWMA approach, the results should be very similar.\nMulti-class Classification Softmax Regression So far, all classification examples have been for binary classifiers, i.e. we only have two classes to predict: is this cat or not? What happens when we want to classify many classes, such as cats, dogs, baby chicks and anything else?\nWe denote the number of classes as $C$. In the case above $C = 4$. Ideally we want to go from an input image, to a vector $A^{[L]} = \\hat{y}$, where $\\hat{y}_1 = P(C_1 \\mid x), \\hat{y}_2 = P(C_2 | x), \\dots, \\hat{y}_k = P(C_k | x)$. That is, each entry in $\\hat{y}$ describes the probability that an input $x$ belongs to each class $k \\in C$. Since each class should be independent, it would be nice if:\n$$\\sum_{i=1}^k \\hat{y}_k = 1$$\nThat is, the probabilities for each class should add up to one, that being one of the requirements of a valid sample space. We solved this in the binary case using our trusty old friend the sigmoid $\\sigma(x)$, which maps $\\mathbb{R} \\rightarrow (0, 1)$. But the sigmoid $\\sigma(x)$ takes a single scalar, what can we do if $x$ is a vector? That is, what can we do when we have more than 2 classes? It turns out that there is a generalization of the sigmoid $\\sigma(x)$ called the softmax function.\nThe standard (unit) softmax function $\\sigma: \\mathbb{R}^K \\mapsto (0, 1)^K$. This means that the softmax function $\\sigma$ maps a $K$ dimensional real-valued vector, to a $K$ dimensional vector where each element is in the interval $(0 ,1)$, and it\u0026rsquo;s defined when $K \\geq 1$ by:\n$$ \\begin{equation} \\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} \\end{equation} $$\nNotice that in our case, $K = C$ where $C$ is the number of classes in our approach. Notice that because of the denominator, which is normalizing, all of our values will add up to $1$, which is exactly what we want.\nLet\u0026rsquo;s walk over one example. Assume that:\n$$ \\begin{equation} Z^{[l]} = \\begin{bmatrix} 5 \\\\ 2 \\\\ -1 \\\\ 3 \\end{bmatrix} \\end{equation} $$\nNow let\u0026rsquo;s calculate $A^{[l]} = \\sigma(Z^{[l]})$ using the softmax. We will do this in two steps, first calculate the numerator, $t$:\n$$ \\begin{equation} t^{[l]} = \\exp(Z^{[l]}) = \\begin{bmatrix} e^5 \\\\ e^2 \\\\ e^{-1} \\\\ e^3 \\end{bmatrix} = \\begin{bmatrix} 148.4 \\\\ 7.4 \\\\ 0.4 \\\\ 20.1 \\end{bmatrix} \\end{equation} $$\nNow we normalize $t^{[l]}$ by the sum $\\sum_{j=1}^4 e^{t_j} = 176.3$ to get $A^{[l]}$\n$$ \\begin{equation} A^{[l]} = \\hat{y} = \\frac{t^{[l]}}{176.3} = \\begin{bmatrix} \\frac{148.4}{176.3} \\\\ \\frac{7.4}{176.3} \\\\ \\frac{0.4}{176.3} \\\\ \\frac{20.1}{176.3} \\end{bmatrix} = \\begin{bmatrix} 0.842 \\\\ 0.042 \\\\ 0.002 \\\\ 0.114 \\end{bmatrix} \\end{equation} $$\nNow we can interpret these as probabilities! For example $P(x | C_1) = 0.842, P(x | C_2) = 0.042$ and so on. Also notice that the sum of these is $1$ because of the normalization in the denominator. Of course, we can think of the softmax as just another activation function, albeit a multi-dimensional one.\nThe name softmax comes from the comparison against the \u0026ldquo;hard max\u0026rdquo; function, which used in our case would return the vector $[1, 0, 0, 0]$, that is a boolean mask of the maximum value in the vector. The softmax is a \u0026ldquo;continuous\u0026rdquo; or \u0026ldquo;soft\u0026rdquo; version of that so that the vector we get as output is $[0.842, 0.042, 0.002, 0.114]$.\nYou might be thinking, what if $C=2$? In this case we are back to binary classification. If we compare softmax regression to logistic regression, then when $C=2$ softmax reduces to logistic regression. However, softmax can generalize logistic regression to $C=K$ dimensions or classes.\nTraining a Softmax Classifier It turns out that using negative log loss as your loss function generalizes nicely to many classes, i.e. $C \u0026gt; 2$.\nAnother thing to keep in mind if we are using the vectorized implementation, where:\n$$ Y = [y^{(1)}, y^{(2)}, \\dots, y^{(m)}] $$\nThen both $Y$ and $\\hat{Y}$\u0026rsquo;s dimensions will be $(C, m)$ now instead. That is each sample $m$ will have it\u0026rsquo;s own softmax vector describing the probability that it belongs to each class.\nProgramming Frameworks Okay, never program your deep learning framework from scratch unless you are an expert trying to squeeze the latest bits of performance in some kind of specialized hardware. Just go with your favorite deep learning framework package. Hopefully go with one that has a vibrant, active community where you can get support and learn how to use the framework.\nMost of the frameworks differ in their approaches and they ultimately end up being equivalent. Like many other packages, measuring the \u0026ldquo;best\u0026rdquo; is hard because the \u0026ldquo;best\u0026rdquo; package is not just the fastest, but the easiest to use, the one with the best community and support, etc.\nThe key thing that all frameworks share is that they solve the problem of automatic differentiation. That is you can define some data, some kind of optimizer, some kind of cost function, and then automatically differentiate the cost function with respect to some parameters. For the purposes of the course, all frameworks should be thought of as equivalent. The course goes over a very simple TensorFlow example, but I decided to ignore it since TensorFlow\u0026rsquo;s documentation has better introductory examples in my opinion.\nNext week\u0026rsquo;s post is here.\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/","summary":"\u003cp\u003eThis is the third and final week of the \u003ca href=\"https://www.coursera.org/learn/deep-neural-network\"\u003esecond course\u003c/a\u003e of DeepLearning.AI\u0026rsquo;s \u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep Learning Specialization\u003c/a\u003e offered on Coursera.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#hyperparameter-tuning\"\u003eHyperparameter Tuning\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#tuning-process\"\u003eTuning Process\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#random-search\"\u003eRandom Search\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#coarse-to-fine-grained-search\"\u003eCoarse-to-fine Grained Search\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#using-an-appropriate-scale-when-searching\"\u003eUsing an Appropriate Scale when Searching\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#python-implementation\"\u003ePython Implementation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#hyperparameter-tuning-in-practice-pandas-vs-caviar\"\u003eHyperparameter Tuning in Practice: Pandas vs. Caviar\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#batch-normalization\"\u003eBatch Normalization\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#normalizing-activations-in-a-network\"\u003eNormalizing Activations in a Network\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#fitting-batch-norm-into-a-neural-network\"\u003eFitting Batch Norm into a Neural Network\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#why-does-batch-norm-work\"\u003eWhy does Batch Norm work?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#batch-norm-at-test-time\"\u003eBatch Norm at Test Time\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#multi-class-classification\"\u003eMulti-class Classification\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#softmax-regression\"\u003eSoftmax Regression\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#training-a-softmax-classifier\"\u003eTraining a Softmax Classifier\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/#programming-frameworks\"\u003eProgramming Frameworks\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"hyperparameter-tuning\"\u003eHyperparameter Tuning\u003c/h2\u003e\n\u003cp\u003eWe have seen by now that neural networks have a lot of hyperparameters. Remember that hyperparameters remain fixed during training. This means that the process of finding reasonable hyperparameters, called hyperparameter tuning, is a process that is separate from training your model.\u003c/p\u003e","title":"Improving Deep Learning Networks: Week 3 | Hyperparameter Tuning, Batch Optimization, Programming Frameworks"},{"content":"This week is focused on the optimization and training process. In particular, this week covers ways to make the training process faster and more efficient, allowing us to iterate more quickly when trying different approaches.\nThis week\u0026rsquo;s topics are:\nMini-batch Gradient Descent Exponentially Weighted Moving Averages (EWMA) Bias Correction Gradient Descent with Momentum RMSProp Adam Learning Rate Decay Mini-batch Gradient Descent We discussed gradient descent briefly on the second week of the first course. A key takeaway is that vanilla gradient descent is vectorized across all our training samples $x^{(m)}$. That is, every gradient step is taken after doing a forward and back propagation over our entire training data. This is relatively efficient when our data is small, but as our data grows it becomes a very big challenge. This is especially true with large neural networks or complex architectures.\nThe process described above is called batch gradient descent, that is a batch is our entire training set $M_{train}$. The name batch gradient descent is an artifact of history, so just take it as it is. Remember that batch gradient descent is the gradient descent version that uses all of our training samples.\nAs we might imagine, mini-batch gradient descent is running gradient descent on smaller batches of our data. Formally a mini-batch is a subset of our training samples and labels, $X, Y$. We refer to batch $t$ by the tuple $(X^{\\{t\\}}, Y^{\\{t\\}})`. For example if we have $m = 5,000,000$ training samples. We might generate $5,000$ mini-batches of $1,000$ samples each. If the dimensions of $X$ are $(N_x, m)$, then the dimensions of $X^{\\{t\\}}$ should be $(N_x, 1000)$, and those of $Y^{\\{t\\}}$ should be $(1, 1000)$.\nThe whole idea is that instead of using $X, Y$ when running gradient descent, we loop over every batch $X^{\\{t\\}}, Y^{\\{t\\}}$. On each iteration we run the vectorized implementation of our network\u0026rsquo;s forward propagation over each batch, in this case $1,000$ samples. After this, we run the back propagation and update the parameters based on the gradients with respect to the cost function over each batch. One pass over all our batches is called an epoch. So far, this is nothing new, just chunking gradient descent to have finer-grained chunks.\nBut why does this work? Imagine that we set our batch to be $m_b = 1$, that is, on each iteration we will simply look at one training sample and run gradient descent. If the sample we use on each iteration is random, then this process is called stochastic gradient descent. It turns out that on every iteration, gradient descent will take a step independent of other samples in our data, so the \u0026ldquo;path down the mountain\u0026rdquo; of gradient descent will look like a blind-wandering, kinda like Brownian motion. Think of a drunk version of gradient descent. It will also be slower because we loose the optimization that arises from vectorization, but still faster than batch gradient descent. On the other hand, imagine that $m_b = m$ so that we have a single batch, our entire data set. Now we are back to batch gradient descent, and each iteration step of gradient descent is taken by averaging each of the samples\u0026rsquo; contribution to the gradient. The \u0026ldquo;path down the mountain\u0026rdquo; looks much more purposeful and directed, because we have a larger sample. The tradeoff is in the amount of noise that results from smaller samples sizes as we decrease our batch size. Somewhere in between $m_b = 1$ and $m_b = m$ there is a sweet spot that allows us to iterative fast but without much random meandering.\nIf our data set is small $m \\leq 2000$ then running batch gradient descent should be close to optimal in terms of speed. Otherwise, using mini-batch gradient descent will usually be faster. Batch sizes is really a hyperparameter that we might search for, however, usually it\u0026rsquo;s some multiple of $2^n$, such as $64, 128, 256, 512$. Usually we select this based on amount of memory on our machine in the case that we cannot fit all our data in memory at once.\nAn important consideration is that the data should be randomly shuffled before generating the batches. So that they really are random samples of $X, Y$ without replacement.\nExponentially Weighted Moving Averages (EWMA) EWMA is a central component of other optimization algorithms that build upon gradient descent. If we have ever tried to code up a moving average, we might have noticed that it is very memory inefficient, especially if we want to do it in a vectorized fashion. EWMA is a solution with a memory complexity of $O(1)$ that approximates a moving average over an arbitrary number of periods. We will later see how this is applied within the context of gradient descent. Let\u0026rsquo;s first look at what exponentially weighted averages are.\nSay that we have a sequence of observations over time from the same source; that is a time-series measurement. Let $\\theta_t$ be the measurement in period $t$. A very common thing to do with time-series is to smoothen the time-series. A common approach is to use a moving average: at every period $t$ we look behind some number of periods, and calculate the average and assign that to a new time-series $V_t$, and so on. If we use a 7-period moving average, we will lose first 7 periods. There is another way that approximates this, and it\u0026rsquo;s defined as this:\n$$ \\begin{equation} V_t = \\beta V_{t-1} + (1-\\beta)\\theta_t \\end{equation} $$\nThat is, each smoothed value $V_t$ is some combination of the previous one $V_{t-1}$ and the current one $\\theta_t$ scaled by $\\beta$ and $(1-\\beta)$, thinking about $\\beta$ as the mix between the past and today. The nice thing is that we can go from values of $\\beta$ to the number of periods pretty easily: when $\\beta = 0.9$ we are approximately averaging over the last 10 periods. If we set $\\beta = 0.98$ then this is approximately averaging over the last 50 periods. If we set $\\beta = 0.5$ then this is approximately averaging over the last 2 periods. Think about what happens when $\\beta=1$.\nThere is a nice explanation of why this approximates a regular moving average on the course, but I think the details are fine to be left out for now.\nIn practice, we usually initialize $V_t = 0$ and loop over our periods to calculate all $V_t$ for each period $t$.\nBias Correction If we initialize $V_t = 0$, then we might imagine that our initial $V_t$s will be heavily biased downwards. The bias will eventually disappear as we move through our time periods. It turns out that there is a way to correct the bias with a simple scaling procedure:\n$$ \\begin{equation} V_t = \\frac{\\beta V_{t-1} + (1-\\beta)\\theta_t}{1 - \\beta^t} \\end{equation} $$\nNotice the term $\\beta^t$ in the denominator. When $t$ is small, i.e. our initial periods, then $V_t$ will be scaled upwards. As we move down our time-series, the bias correction will be much smaller since $\\beta^t$ will become much smaller since $0 \u0026lt; \\beta \\leq 1$.\nGradient Descent with Momentum Let\u0026rsquo;s come back from the time-series digression. Why did we go over moving averages on time-series? Remember that when we introduced mini-batch gradient descent we mentioned that the \u0026ldquo;path down the mountain\u0026rdquo; will be more \u0026ldquo;noisy\u0026rdquo;. From the extreme of using a batch of size $1$, which is the most \u0026ldquo;noisy\u0026rdquo; up to using our entire data, which is the least noisy. It turns out that we can use the concept of EWMA to give \u0026ldquo;inertia\u0026rdquo; to the steps \u0026ldquo;down the mountain\u0026rdquo; that mini-batch gradient descent takes. This is called momentum, and it uses EWMA of the gradients to take steps, instead of each gradient being independently evaluated.\nRemember that in physics, momentum is a vector: $\\mathbf{p} = m\\mathbf{v}$ since $\\mathbf{v}$ is the velocity vector. While inertia is a scalar!\nThe idea is exactly the same as when we want to smooth time-series. In this case we want to smooth the oscillations of the cost function as we iterate over each mini-batch. Think that if the oscillations along one dimensions are higher, those will be dampened more than a dimension with fewer oscillations.\nTechnically, this means that when we update $W^{[l]}$, we not only need to calculate the gradient $dW^{[l]}$, we need to also calculate the EWMA of the gradients:\n$$ \\begin{equation} v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1-\\beta) dW^{[l]} \\end{equation} $$\nAnd the parameter update becomes:\n$$ \\begin{equation} W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}} \\end{equation} $$\nNotice that in practice many people don\u0026rsquo;t use bias correction, and that the example above is only for the $W^{[l]}$ parameters. This should also be repeated for the $b^{[l]}$ parameters. Finally, the choice of $\\beta$ is now part of hyperparameter tuning, but almost everyone uses $\\beta = 0.9$, which approximates averaging over the last 10 gradient iterations. In a nutshell, momentum accelerates our search in the direction of the minima (and away from oscillations) using EWMA. Finally, notice that we are updating our parameters directly with the EWMA of the gradients. We will do something slightly different next.\nRMSProp RMSProp (root-mean-square propagation) is a cousin of the momentum idea. However, instead of using the EWMA of the gradients for the update, we scale the gradient with the EWMA of the squares of the gradients. 1 If this is confusing, looking at the equation below might help:\n$$ \\begin{equation} S_{dW^{[l]}} = \\beta S_{dW^{[l]}} + (1-\\beta) dW^{[l]2} \\end{equation} $$\nAnd the parameter update:\n$$ \\begin{equation} W^{[l]} = W^{[l]} - \\alpha \\frac{dW^{[l]}}{\\sqrt{S_{dW^{[l]}}}+ \\epsilon} \\end{equation} $$\nWhere $\\epsilon = 10^{-8}$ is a constant that guarantees numerical stability.\nThe idea is similar to momentum, only that with the case of RMSProp we are shrinking the gradient by the square root of the squared EWMA of the gradients. This in turn means that the gradient dimension with the largest oscillation will be dampened proportionally to its size. In a nutshell, RMSProp constrains the oscillations away from the minima, also using EWMA. Intuitively, this is what\u0026rsquo;s happening when we scale down our gradient by the squared EWMA of the gradient.\nAdam Adam is usually introduced as a combination of momentum and RMSProp. It turns out that Adam has been shown to be more flexible and generalizable than momentum. 2\nThe update step is literally combining momentum and RMSProp, and again we will only go over the $W^{[l]}$ update, ignoring the $b^{[l]}$ update for conciseness. We initialize $V_{dW^{[l]}}, S_{dW^{[l]}} = 0$:\n$$ \\begin{align} V_{dW^{[l]}} \u0026amp;= \\beta_1 V_{dW^{[l]}} + (1-\\beta_1)dW^{[l]} \\\\ S_{dW^{[l]}} \u0026amp;= \\beta_2 S_{dW^{[l]}} + (1-\\beta_2)dW^{[l]2} \\end{align} $$\nWe also apply bias-correction for the EWMA:\n$$ \\begin{align} V_{dW^{[l]}}^{corrected} = \\frac{V_{dW^{[l]}}}{1 - \\beta_1^t} \\\\ S_{dW^{[l]}}^{corrected} = \\frac{S_{dW^{[l]}}}{1 - \\beta_2^t} \\\\ \\end{align} $$\nFinally, the weight update looks like:\n$$ \\begin{equation} W^{[l]} = W^{[l]} - \\alpha\\frac{V_{dW^{[l]}}^{corrected}}{\\sqrt{S_{dW^{[l]}}^{corrected}}+\\epsilon} \\end{equation} $$\nIn a nutshell, Adam combines momentum and RMSProp: it adds inertia towards the minima, and it also dampens out the oscillations by using the second moment (squares) of the gradient.\nAn important thing to notice is that since Adam combines momentum and RMSProp, we need two hyperparameters now: $\\beta_1, \\beta_2$. In practice most people don\u0026rsquo;t search over these hyperparameters and use default values:\n$\\beta_1 = 0.9$ $\\beta_2 = 0.999$ $\\epsilon = 10^{-8}$ Learning Rate Decay You might hear that RMSProp or Adam have adaptive learning rates. However, we should think of these as two separate things. On the descriptions above, the learning rate $\\alpha$ never changes. However, we can combine the approaches described above with learning rate decay for better performance.\nLearning rate decay is a pretty simple idea: as we get closer to the minima, we want to take smaller steps in gradient descent; that is, start with some $alpha$ and keep reducing that by some amount every time to we make a step. The reason why we want to do this is that we might be running circles around the minima if the cost function has a degenerate minimum, and therefore take longer to converge.\nThere are many ways to do this, but the one described on the course is that we will decrease $\\alpha$, the learning rate, by some fixed amount every epoch of training:\n$$ \\begin{equation} \\alpha^* = \\frac{1}{1 + \\text{decay\\_rate} \\times \\text{epoch}} \\alpha \\end{equation} $$\nNotice that $\\text{decay\\_rate}$ is another hyperparameter to tune via hyperparameter tuning.\nThere are other learning rate decay approaches, such as exponential decay, that work well in certain settings.\nAndrew mentions that tuning the learning rate decay should be pretty low in the list of priorities. Getting a good value of $\\alpha$ is much more important.\nThe topic of non-convex optimization is huge, wide-ranging and beautiful. A lot of the ideas presented in the course are part of ongoing research in the field. The course does a great job at describing modern approaches superficially, but there\u0026rsquo;s many rabbit holes to go down in. To motivate this, checkout this pretty animation of how different optimizers navigate a complicated cost function:\nVisualizing Optimization algorithm comparing convergence with similar algorithm\nAlso, in the case of a long valley:\nVisualizing Optimization algorithm comparing convergence with similar algorithm\nNext week\u0026rsquo;s post is here.\nRMSProp | Cornell University\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAdam | Cornell University\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/","summary":"\u003cp\u003eThis week is focused on the optimization and training process. In particular, this week covers ways to make the training process faster and more efficient, allowing us to iterate more quickly when trying different approaches.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/#mini-batch-gradient-descent\"\u003eMini-batch Gradient Descent\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/#exponentially-weighted-moving-averages-ewma\"\u003eExponentially Weighted Moving Averages (EWMA)\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/#bias-correction\"\u003eBias Correction\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/#gradient-descent-with-momentum\"\u003eGradient Descent with Momentum\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/#rmsprop\"\u003eRMSProp\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/#adam\"\u003eAdam\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/#learning-rate-decay\"\u003eLearning Rate Decay\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"mini-batch-gradient-descent\"\u003eMini-batch Gradient Descent\u003c/h2\u003e\n\u003cp\u003eWe discussed \u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week2/#gradient-descent\"\u003egradient descent\u003c/a\u003e briefly on the second week of the first course. A key takeaway is that vanilla gradient descent is vectorized across all our training samples $x^{(m)}$. That is, every gradient step is taken after doing a forward and back propagation over our entire training data. This is relatively efficient when our data is small, but as our data grows it becomes a very big challenge. This is especially true with large neural networks or complex architectures.\u003c/p\u003e","title":"Improving Deep Learning Networks: Week 2 | Optimization Algorithms"},{"content":"This is the first week in the second course of DeepLearning.AI\u0026rsquo;s Deep Learning Specialization offered on Coursera. The course deals with hyperparameter tuning, regularization and the optimization of the training process. The optimization ranges from computational complexity to performance. This is again a pretty technical week, where you will benefit a lot from doing the programming exercises.\nThis week\u0026rsquo;s topics are:\nSetting up our Machine Learning problem Train / Dev / Test sets Bias-Variance Tradeoff Basic Recipe for Machine Learning Regularizing our Neural Network Regularization Why Does Regularization Reduce Overfitting? Dropout Regularization Understanding Dropout Other Regularization Methods Setting up our Optimization Problem Normalizing Inputs Vanishing and Exploding Gradients Weight Initialization Setting up our Machine Learning problem Train / Dev / Test sets Machine learning projects are highly iterative. That is, you try something new, see how it does and then adjust; very much like gradient descent. Therefore, you want the iteration time to be quick so that you can try as many things as quickly as possible, without affecting the final performance of the model. Part of this is setting up your datasets correctly so that you can efficiently iterate over different approaches.\nIf you have a data set $M$ of $|m|$ samples then you usually want to split it into three parts.\nTraining set: $M_{train} \\subseteq M$. The set of samples that will be used to learn the parameters of our chosen hypothesis $h(X, \\theta)$, i.e. your model parameters via SGD. Holdout/Cross-validation/Development set: $M_{dev} \\subseteq M$. The set of samples that will be used to evaluate a hypothesis class $H$, i.e. your model hyperparameters via hyperparameter tuning. Test set: $M_{test} \\subseteq M$. The set of samples that will be used the best hypothesis you\u0026rsquo;ve found so far on completely unseen data that belongs to the same distribution as the dev set. Note that these three subsets are disjoint, i.e. $M_{train} \\cap M_{test} \\cap M_{dev} = \\emptyset$\nHistorically most researchers would split the set $M$ using a 70%/30% training and testing respectively. If using a dev set, a 60%/20%/20% split for training, dev and test. This was reasonable for datasets that have sizes in the order of 10,000. If you have a dataset $M$ with 1,000,000 training samples, then it can be reasonable to use a 98%/1%/1% training, dev and test split. You can read more about how big a sample has to be for certain statistical properties to kick in, but a number around 10,000 is considered relatively safe.\nOne assumption is that all the training samples $m \\in M$ come from the same distribution, which implies that each of the splits also belong to the same distribution. This is critical because you don\u0026rsquo;t want to develop a model with pictures of cats and then test it on picture of dogs. A key thing is to make sure that your dev and test sets come from the same distribution. Having the training set come from a different distribution is more lax, as long as the training distribution is a superset of the dev and test sets. In some cases it might be okay to not have a test set, but measuring variance will be harder.\nBias-Variance Tradeoff The bias/variance tradeoff is a topic in machine learning and statistics. The whole idea of supervised learning is that, if you do it correctly, your model can generalize beyond the training set, i.e. samples it has not seen during training and still do a good job. We think that there are some parameters $\\theta$ that minimize our cost function. We don\u0026rsquo;t know what that is, and the only way to know that is to have infinite training samples. But we need to make due with our estimate $\\hat{\\theta}$, which implies that $\\theta \\neq \\hat{\\theta}$. When we talk about the bias-variance tradeoff we are describing the errors, i.e. $\\epsilon = \\theta - \\hat{\\theta}$ that are produced by our model that uses our estimates of the parameters. In particular, we are describing the mean and variance of the error distribution. The bias is a part of how far $\\hat{\\theta}$ is from $\\theta$ on average, while the variance measures the spread of the errors.\nIt turns out that we can characterize our expected test errors even further by decomposing them into three components (skipping some math):\nVariance: $E_{x, D}\\left[\\left(h_D(x) - \\bar{h}(x)\\right)^2\\right]$ $\\text{Bias}^2$: $E_x\\left[\\left(\\bar{h}(x) - \\bar{y}(x)\\right)^2\\right]$ Noise: $E_{x,y}\\left[\\left(\\bar{y}(x) - y\\right)^2\\right]$ Keep in mind:\nVariance measures how much our classifier $h_D$ changes when we train it on a different training set $D$. How \u0026ldquo;over-specialized\u0026rdquo; is our classifier to a particular training set $D$? 1 Notice that $h$ and $h_D$ are random variables, and so this measures the spread of a classifier trained on each possible sample $D$ drawn from $P^n$ versus the average classifier $\\bar{h}(x)$. Bias measures the inherent error from our classifier if we had infinite training data. 1 How far is our estimator\u0026rsquo;s expected value $\\bar{y}(x)$ from the expected classifier? Noise measures some irreducible error, which is an intrinsic property of the data. This related to Bayes error rate, which is the lowest possible error rate for any classifier of random outcomes. Graphical Illustration of bias and variance\nWe know that bias and variance are the components of our model\u0026rsquo;s expected test error. But why is it a tradeoff? It turns out that model complexity is related to both bias and variance. A model with low complexity (always guess the same) will have high error, and the majority of the error will come from the bias term. This means that as we decrease the complexity of a model, our error will increase, but most of the increase will come from the bias term. Conversely, a model with high complexity (guessing randomly) will have high error, and the majority of the error will come from the variance term. This means that as we increase model complexity, our error will also increase, but most the increase will come from the variance term. Somewhere in the middle, there is a sweet spot, a combination of bias and variance that has the minimum total error.\nWhen we say that a model has high bias, we say that it\u0026rsquo;s underfitting. It\u0026rsquo;s too basic and not being complex enough to learn the training data, underfitting it. On the other hand, when we say that a model has high variance we say that a model is overfitting the training data, not being able to generalize beyond it, overfitting it. This is where we come back to our dataset splits.\nBefore moving on, we have to re-introduce the irreducible error term from the previous section. For every classification task, there is some amount of error that cannot be done away with. This is also called Bayes error rate, and it\u0026rsquo;s a theoretical limit that no classifier can surpass. Similarly, there is a human-level error achieved by the best a human can do, whether a group or individual, and this is called the human-level error. The best classifier can be better or worse than human-level error but never better than Bayes error rate. For the following section we assume that the human-level error is approximately $0$ but more than the Bayes error rate.\nEvaluating the performance of your model on the training and dev set can help you diagnose whether your model has high variance or high bias or both very quickly. Let $\\epsilon_{train}, \\epsilon_{dev}$ the error of your model on the training and dev sets respectively.\n$e_{train} = 1\\%, e_{dev} = 11\\%$: Our model is doing really well on the training set but much poorer on the development set. It smells of high variance since the model is not able to generalize from the training set to the dev set. It\u0026rsquo;s therefore overfitting the training set. $e_{train} = 15\\%, e_{dev} = 16\\%$: Assuming that human-level error is approximately $0$, then this looks like high bias. The model is very far from human-level error on the training set, therefore it\u0026rsquo;s not fitting our training data enough. It\u0026rsquo;s therefore underfitting the training set. However, it\u0026rsquo;s generalizing quite well to the dev set, therefore it might have low variance. $e_{train} = 15\\%, e_{dev} = 30\\%$: Assuming that human-level error is approximately $0$, then this looks like high bias again. But because it\u0026rsquo;s not generalizing well, it also smells of high variance because of the discrepancy in performance between the test and dev sets. $e_{train} = 0.5\\%, e_{dev} = 1\\%$: Assuming that human-level error is approximately $0$, then this looks like both low bias and low variance. It\u0026rsquo;s very close to the Bayes\u0026rsquo; error rate, and also generalizes well to the dev set. The key assumption is that human-level error is approximately $0$, which implies that Bayes error has to be between $0$ and the human-level error. If for example the Bayes error rate is $15\\%$ then the second case would not be high variance, because we are actually close to the best possible classifier.\nIf you\u0026rsquo;re wondering about how you can simultaneously underfit and overfit your training data, think about this:\nHigh Bias and High Variance\nBasic Recipe for Machine Learning Ideally we want a low-bias and low-variance model. But how do we get there?\nDo we have high bias? How is our performance on the training set relative to Bayes error rate? If high bias: Increase the size/depth of your network, adding model complexity, increasing variance but lowering bias. Train it longer, better estimates of the parameters, lower bias and variance. Do we have high variance? How is our performance on the dev set relative to the training set and Bayes error rate? Get more data, better estimates of the parameters, lower bias and variance. Regularization, decreasing model complexity, increasing bias but lowering variance. Following these steps, i.e. fixing high bias before high variance is the basic recipe for machine learning.\nIt\u0026rsquo;s mentioned in the course that in the deep learning era, there is less of a tradeoff. The ability to have bigger models and get more data are both tools that don\u0026rsquo;t expose the bias-variance tradeoff, so usually these are the first things to check when encountering high bias or high variance. In general training a bigger network almost never hurts, as long as you add proper regularization, keeping the model complexity from exploding.\nRegularizing our Neural Network Regularization Regularization is a tool you employ when you suspect your model is suffering from high variance or overfitting. Regularization will reduce your model complexity by penalizing model complexity when evaluating its performance and therefore generating a \u0026ldquo;simpler\u0026rdquo; model. It turns out that you can add a term to your cost function, which is a measure of model complexity, so that more complex models result in a higher cost, therefore driving down complexity. But how do we measure model complexity? Let\u0026rsquo;s answer this using the logistic regression case:\nLet\u0026rsquo;s recall our cost function:\n$$ \\begin{equation} \\mathcal{J}(w, b) = \\frac{1}{m} \\sum_{i = 1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) \\end{equation} $$\nWe would like to add something to the expression above that measures model complexity. One way of measuring model complexity is literally to look at the magnitude of the parameters, literally how big they are. Remember that in logistic regression we have one theta $\\theta_i$ for each feature in our data. This implies that if coefficients blow up in magnitude, that is, get very big or very small, they are having a lot of effect on the output. This is simply because the relationship between parameters and features is multiplicative and additive, i.e. it\u0026rsquo;s linear.\nWe can check how big the parameters are by summing over each of them, let\u0026rsquo;s call that $|w|$:\n$$ \\begin{equation} |w| = \\sum_{j=1}^{N_x} w_j \\end{equation} $$\nThe issue with the above is that some $w_j$ might be positive and others might be negative, undoing each of their effects in the summation. The solution to this problem is to use a norm. A norm is a function that maps the real or complex numbers into the non-negative real numbers. An important geometrical idea related to linear algebra is that the norm is a generalization of distance from the origin: it\u0026rsquo;s proportional to scaling, it doesn\u0026rsquo;t violate a form of the triangle inequality, and is zero only at the origin. 2 We would also like to generalize this to more dimensions, which is handy when our parameters is not just a vector but matrices that describe some vector space that has some \u0026ldquo;size\u0026rdquo;. When talking about the norm of a matrix, the term Matrix norm is used, and the euclidean norm equivalent is called the Frobenius norm. It has a different name because you have to do some work before generalizing from one to many dimensions and some brilliant mathematicians figured it out.\nOne of the most common norms is to take the squares to only get positive numbers. This is called the L2 Norm, or Euclidean norm for being related the definition of distance in Euclidean geometry, remember the guy Pythagoras?. It\u0026rsquo;s denoted $||w||_2$. When working with the square of the L2 norm, its denoted $||w||_2^2$. It\u0026rsquo;s defined by:\n$$ ||w||_2 = \\sqrt{\\sum_{j=1}^{N_x} w_j^2} = \\sqrt{w^Tw} $$\nAlso:\n$$ ||w||^2_2 = w^Tw $$\nAdding this to our cost function makes it looks like this now:\n$$ \\begin{equation} \\mathcal{J}(w, b) = \\frac{1}{m} \\sum_{i = 1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + ||w||_2^2 \\end{equation} $$\nWhy are we ignoring the other parameter $b$? Because it\u0026rsquo;s a scalar and not a vector. $w \\in \\mathbb{R}^{N_x}$, which means $w$ has $N_x$ elements. While $b \\in \\mathbb{R}$, which means that $b$ has only one element. This means that if we have $N_x$ features, then $b$ has to be $N_x$ times bigger than the $w$ norm to influence the same amount. You can also think about how the space or volume of a $N_x$ dimensional hypercube grows as opposed to a $1$ dimensional line. So we usually ignore $b$.\nWe now have a way of penalizing model complexity by assigning a higher cost to a more complex model. What would also be great is to have a way of scaling the effect of the regularization. For example, we don\u0026rsquo;t want a model with a very small $w$ because the smallest is where $w_i = 0$, we want to retain some complexity but not too much.\nWe do this by introducing a hyperparameter $\\lambda$ called the regularization parameter, which is a scalar that scales the regularization effect:\n$$ \\begin{equation} \\mathcal{J}(w, b) = \\frac{1}{m} \\sum_{i = 1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\lambda||w||_2^2 \\end{equation} $$\nWe can find a good $\\lambda$ by hyperparameter tuning using the dev set. By comparing the loss between the training set and the dev set, we can find a good regularization value that balances simplicity and complexity, or bias and variance.\nWe also want to scale the effect of the regularization by the size of our training data $m$. Remember, we have a high variance problem we are trying to solve by penalizing a complex model. However, our model will be more complex as we add more training data. So we want our regularization to be stronger when our training data is small, but decrease as our training data is larger. We can just divide $\\lambda$ by $m$ to have a linearly decreasing effect:\n$$ \\begin{equation} \\mathcal{J}(w, b) = \\frac{1}{m} \\sum_{i = 1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{m}||w||_2^2 \\end{equation} $$\nFinally, for differentiation reasons, we add an easy way to cancel some terms:\n$$ \\begin{equation} \\mathcal{J}(w, b) = \\frac{1}{m} \\sum_{i = 1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}||w||_2^2 \\end{equation} $$\nThink about what happens to $w$ after we introduce this term to our cost function. It means it will prefer smaller parameters than bigger ones, and if it has to chose between two big ones, it will choose the one that is the least costly. In other words the model will let some parameters be large as long as they earn their keep (in terms of model performance) and decrease the features that are not that relevant. In this sense, regularization can be thought of as a type of feature selection. With the L2 norm the parameters that don\u0026rsquo;t earn their keep approach zero, but never quite get to be zero.\nBut the L2 norm is not the only norm. What if you use some other norm? Another popular norm is the L1-Norm:\n$$ ||x||_1 = \\sum_{j=1}^{N_x} |x_i| $$\nJust like the L2-norm, it maps the real or complex numbers into the non-negative real numbers. We can also use this in our cost function:\n$$ \\begin{equation} \\mathcal{J}(w, b) = \\frac{1}{m} \\sum_{i = 1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}||w||_1 \\end{equation} $$\nUnlike the L2-norm, the L1-norm does actually send the parameters that don\u0026rsquo;t earn their keep to zero. This means that $w$ will end up being sparser. Which implies that the L1-norm works as a sort of model compression. In practice however, the L2 norm is used more commonly so that should be your first go to.\nWe mentioned the Frobenius norm as the generalization of the L2-norm to higher dimensions. The L2-norm works in logistic regression precisely because $w$ is a vector. When we have matrices of parameters, $W^{[l]}$ as is the case for a neural network, we need to use the grown-up\u0026rsquo;s norm. The Frobenius norm is a reasonable extension, instead of summing over a vector\u0026rsquo;s single dimension, you sum over both dimensions in a matrix:\n$$ ||W^{[l]}||_F = \\sqrt{\\sum_{i=1}^{n^{[l - 1]}}\\sum_{j=1}^{n^{[l]}}w_{ij}^{[l]}} $$\nAlso:\n$$ ||W^{[l]}||^2_F = \\sum_{i=1}^{n^{[l - 1]}}\\sum_{j=1}^{n^{[l]}}w_{ij}^{[l]} $$\nSo for a $L$-layered neural network our cost function will look like:\n$$ \\begin{equation} \\mathcal{J}(W^{[1]}, b^{[l]}, \\dots, W^{[l]}, b{[l]}) = \\frac{1}{m} \\sum_{i = 1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m} \\sum_{l=1}^L ||W^{[l]}||^2_F \\end{equation} $$\nA couple of things to mention:\nNotice that now the cost function is a function of all layers of our network, $W^{[l]}, b^{[l]}$. This is in contrast to logistic regression where we only have one pair of $W, b$. Also notice that in the regularization term we need to sum over all layers of our network. Again we are ignoring the $b^{[l]}$ and only focusing on the $W^{[l]}$ parameters. The L2-norm is also usually called weight decay. This is because when you differentiate the loss function, which now includes the regularization term, with respect to the parameters, it will scale down $W^{[l]}$ by $1 - \\frac{\\alpha\\lambda}{m}$ on every step. Making those weights smaller, or decaying the weights if you please.\nIf you\u0026rsquo;re coming from econ-land, you might have heard of Lasso and Ridge regressions. Lasso is a linear regression that uses the L1-norm, while Ridge is a linear regression that uses the L2-norm.\nWhy Does Regularization Reduce Overfitting? We went over how regularization is equivalent to penalizing model complexity, and how model complexity can be measured by the magnitude of the parameters of your model, which is the mechanism through which we implement regularization. The intuition here is that parameters with high values assign high sensitivity to the features being multiplied by those parameters. The high sensitivity is what results in the high variance, and by penalizing the magnitude of the parameters is how we reduce overfitting.\nAnother great way of thinking about it is to think about what happens in the activation functions when the values are big or small.\nRemember that $Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$. This, we pass through our activation function $g^{[l]}$ so that we get $A^{[l]} = g^{[l]}(Z^{[l]})$. Taking as an example the sigmoid activation function $\\sigma(x)$, think about what happens when the inputs $Z^{[l]}$ are small. How can $Z^{[l]}$ be small? If we set $\\lambda$ to be huge, then most of our $W^{[l]}$ will be small, therefore making $Z^{[l]}$ small as well. When we pass small values, that is values close to $0$ to the sigmoid function, the outputs are approximately linear. It is only when $|x| \\geq 2$ that $\\sigma(x)$ (approximately) becomes non-linear. So in a way, setting $W{[l]}$ to be small undoes all the non-linear magic that you got from adding more layers to your network.\nDropout Regularization It turns out that measuring the \u0026ldquo;size\u0026rdquo; of your parameters is not the only way to measure model complexity. Conversely, minimizing the size of your parameters is also not the only way to punish model complexity. Dropout achieves a similar result by doing something more intuitive to some.\nIn a nutshell, dropout randomly deletes some fraction of nodes in each hidden layer on each forward-backward pass. Since on each forward-backwards pass you are \u0026ldquo;turning off\u0026rdquo; some hidden units, the model trained on each iteration is literally smaller. The random component of the dropout guarantees that all parts of the model are affected equally on expectation. So what do we mean exactly by \u0026ldquo;deleting\u0026rdquo; nodes? We mean setting them to 0. It really is that simple.\nMore formally, there is some probability $p$ that represents probability for a single hidden unit to be kept untouched. $p$ is referred to as the keep probability. Conversely, $1 - p$ is the \u0026ldquo;dropout\u0026rdquo; probability. Notice that the events are independent of each other. That is each hidden unit, i.e. each node is dropped independently relative to all the other units in that layer, and also from other layers.\nSay that you have a layer with $50$ hidden units, and that $p = 0.8$, so that you will be dropping out $1 - 0.8 = 0.2$ of the units in a hidden layer. So on expectation you will be dropping $50 * 0.2 = 10$ nodes. Let\u0026rsquo;s also think that this layer is $l = 3$. Let\u0026rsquo;s think about how this will affect the next layer.\nThe next layer\u0026rsquo;s computation will be:\n$$ \\begin{equation} Z^{[4]} = W^{[4]}A^{[3]} + b^{[4]} \\end{equation} $$\nWhere $A^{[3]}$ is the output of the layer with $50$ hidden units, and we shut down $20\\%$ of them. This means that $A^{[3]}$ is reduced by $20\\%$, which also implies that $Z^{[4]}$ is reduced by $20\\%$ because $Z^{[4]}$ is just a linear combination of $A^{[3]}$. In order to avoid this problem propagating throughout our network making everything small, we need to roughly \u0026ldquo;refund\u0026rdquo; the lost value to $A^{[3]}$ before passing it to the next layer. We do this simply by:\n$$ \\begin{equation} A^{[3*]} = \\frac{A^{[3]}}{p} \\end{equation} $$\nRemember that dividing anything by a number less than $1$ will make it bigger. Therefore:\n$$ \\begin{equation} Z^{[4]} = W^{[4]}A^{[3*]} + b^{[4]} \\end{equation} $$\nWill make $Z^{[4]}$ be roughly the same as it was originally. This scaling technique is called inverted dropout.\nThe intuition behind this rescaling is that when you set some hidden-units to zero, you are shifting the expected value of $A^{[3]}$, which is an issue when you do testing. Because when you evaluate your network with the test set, you do not do any dropout, the difference in expected values becomes a problem. Therefore, the rescaling ameliorates the scaling issue between training and testing.\nFinally, you can apply a different value of $p$ to different layers, so that you have a vector $p$ where $p^{[l]}$ is the probability of keeping units in hidden layer $l$. This faces you with a tradeoff of having more hyperparameters, so it should be used wisely.\nUnderstanding Dropout So why does dropout behave like regularization? It seems crazy to \u0026ldquo;kill\u0026rdquo; some nodes at random, it\u0026rsquo;s the very definition of chaotic evil. It turns out that by doing this, the model will learn not to rely on any specific feature, because it might not be there the next time. The model will respond to dropout by spreading out the weights across the network, effectively reducing their magnitude. This is why dropout can be thought of as regularization.\nAn interesting approach to building resilience in engineering systems is that of Chaos Engineering. It\u0026rsquo;s similar in spirit to dropout, whereby randomly disabling some parts of a system, and forcing designers to deal with random failures will produce a more robust system.\nOther Regularization Methods Another regularization method is that of data augmentation. For example if you have images as your training data, you might think about applying some geometric distortions (flipping, scaling, etc.). This works as regularization by injecting more variance into your training set, making your model smarter by making it harder for it to put all its eggs in one basket (focusing too much on some parameters).\nAnother method is early stopping. This amounts to regularization because early stopping might amount to stopping training before $W{[l]}$ gets \u0026ldquo;too big\u0026rdquo;. What is too big? You won\u0026rsquo;t know. The problem with early stopping is that it conflates two processes that should be orthogonal (independent): doing well on your training samples and not overfitting. There\u0026rsquo;s more information about orthogonalizing your goals in the next course.\nSetting up our Optimization Problem One thing is to make sure that your model has the right combination of bias and variance, which amounts to learning the problem at hand well enough and also being able to generalize to unseen samples. Now, we turn our attention to making the optimization process (learning the problem) easier and more efficient.\nNormalizing Inputs Normalization is one of those topics that means different things to different people. 3 Let\u0026rsquo;s disambiguate the term and also talk about why normalizing our inputs might be helpful for our optimization process.\nAs the name implies, normalization is the process of making something normal. Of course, there are many ways of making something \u0026ldquo;normal\u0026rdquo;, and we haven\u0026rsquo;t even defined what \u0026ldquo;normal\u0026rdquo; is. Let\u0026rsquo;s focus on one of the ways to normalize things.\nIn the context of statistics, normalization usually refers to standardizing, which is where we calculate a standard score for a random variable $X$. It\u0026rsquo;s called the standard score because its units are standard deviations.\nStandardizing a random variable $X$ is pretty easy and intuitive:\n$$ \\begin{equation} z = \\frac{x - \\mu}{\\sigma} \\end{equation} $$\nWhere $\\mu$ is the mean of the population and $\\sigma$ is the standard deviation of the population. Because we don\u0026rsquo;t know those values, and we usually estimate them, we usually work with z-scores, which are the sample analogues:\n$$ \\begin{equation} z = \\frac{x - \\bar{x}}{S} \\end{equation} $$\nWhere $\\bar{x}$ is the sample mean and $S$ is the sample standard deviation. If your features are normally distributed, then the transformed variables will approximately mean zero and standard deviation of one.\nThe process is intuitive, for each observation we first ask: how far is this observation from the sample mean. Whatever that distance is, positive or negative, we scale it by the standard deviation. So that the result is: an observation is $z$ standard deviations from the mean.\nBut why do this? The purpose of this process is to convert different random variables that have different scales and shifts, into the same units. The units will be standard deviations for that particular random variable. So that $z_i$ will tell you how many standard deviations observation $i$ is from the mean. Positive values will be above the mean and negative values will be below the mean. Using this approach we can negate the effects of having different features with wildly different unit scales and shapes.\nWhen you normalize your training set, you want to use standardization on each of your features separately. However, when you are testing, you want to use the same $\\bar{x}$ and $S$ that you calculated during training. You don\u0026rsquo;t want to scale and shift your training data separately from your testing data. You want the opposite, you want your training and testing data to go through the same transformations.\nHow is this related to optimization? How does normalizing scales and shifts in our features affect our optimization? It all has to do with gradient descent and how the updates are made.\nGradient Descent with and without features scaling\nThe contour plot above shows an imaginary two-dimensional cost function $J(F_1, F_2)$. Notice that if we have a pair of unscaled features and one of the features\u0026rsquo; scale is larger than the other, then the cost function will look squished. Gradient descent will take many tiny steps because it\u0026rsquo;s considering both features at the same time. Since one feature is big and the other one small, the same step size is big in one but small on the other. This means that the larger feature will dominate the update. However, if you scale your features with standardization for example, then the cost function might look like the one on the right. Since all features are in a standard scale, no feature will dominate the update, allowing for less oscillation, and therefore faster convergence.\nVanishing and Exploding Gradients Vanishing and exploding gradients sound magical, if not outright dangerous. What this means is that when we train very deep networks, and we take the derivatives for gradient descent, the magnitude of these derivatives either \u0026ldquo;vanish\u0026rdquo; to $0$ or \u0026ldquo;explode\u0026rdquo; towards $\\infty$. How does this disaster come about?\nThink about a very deep $L$-layered neural network. Imagine also that all $W^{[l]}$ are the same:\n$$ \\begin{equation} W^{[l]} = \\begin{bmatrix} 1.5 \u0026amp; 0 \\\\ 0 \u0026amp; 1.5 \\end{bmatrix} \\end{equation} $$\nAlso imagine that all $b^{[l]} = 0$.\nSince all $W^{[l]}$ are the same, then whatever our features $x$ is, we get that:\n$$ \\hat{y} = W^{[l]L}x $$\nThat is the $W^{[l]}$ is multiplied by itself $L$ times. In our case this amounts to $1.5^L$, which literally grows exponentially with the number of layers. Imagine now that instead of $1.5$ on the diagonal of $W^{[l]}$ we have a number less than 0, i.e. $0.5$. With the same problem setup, now the values will be $0.5^L$ which is an expression that shrinks exponentially in the number of layers.\nIn either case, this is a huge problem. With exploding gradients, the gradient descent steps are gigantic, and with vanishing gradients the gradient descent steps are tiny. Either way, converging the minimum of our cost function will take a long, long time. We tackle this issue by being more careful in the way that we initialize our parameters.\nWeight Initialization In order to avoid exploding or vanishing gradients, we need to be more careful as to how we initialize our parameters. The main idea here is that the more hidden units in a hidden-layer, the smaller each $w^{[l]}_j$ you want. This is to keep large hidden layers and small hidden layers in around the same output scale.\nWe will still initialize our parameters to random values, but we will scale them by a term, $s$, that is proportional to the size of the inputs coming into a layer $l$. The whole point of this is to adjust the variance of $W^{[l]}$ so that it\u0026rsquo;s proportional to inputs, $n^{[l-1]}$:\n$$ \\begin{equation} W^{[l]} = \\texttt{np.random.randn(shape)} \\times s \\end{equation} $$\nThe choice is $s$ depends on $g^{[l]}(x)$, the activation function for layer $l$:\nIn the case that $g^{[l]}(x) = \\text{ReLU}(x)$, then $s = \\sqrt{\\frac{2}{n^{[l-1]}}}$. In the case that $g^{[l]}(x) = \\text{tanh}(x)$, then $s = \\sqrt{\\frac{1}{n^{[l-1]}}}$, which is also called Xavier initialization. Another variant for $\\text{tanh}(x)$ is $s = \\sqrt{\\frac{2}{n^{[l-1]} + n^{[l]}}}$ Next week\u0026rsquo;s post is here.\nBias-Variance Tradeoff | Cornell University\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNorm | Wikipedia\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNormalization | Wikipedia\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/","summary":"\u003cp\u003eThis is the first week in the \u003ca href=\"https://www.coursera.org/learn/deep-neural-network\"\u003esecond course\u003c/a\u003e of DeepLearning.AI\u0026rsquo;s \u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep Learning Specialization\u003c/a\u003e offered on Coursera. The course deals with hyperparameter tuning, regularization and the optimization of the training process. The optimization ranges from computational complexity to performance. This is again a pretty technical week, where you will benefit a lot from doing the programming exercises.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#setting-up-our-machine-learning-problem\"\u003eSetting up our Machine Learning problem\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#train--dev--test-sets\"\u003eTrain / Dev / Test sets\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#bias-variance-tradeoff\"\u003eBias-Variance Tradeoff\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#basic-recipe-for-machine-learning\"\u003eBasic Recipe for Machine Learning\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#regularizing-our-neural-network\"\u003eRegularizing our Neural Network\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#regularization\"\u003eRegularization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#why-does-regularization-reduce-overfitting\"\u003eWhy Does Regularization Reduce Overfitting?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#dropout-regularization\"\u003eDropout Regularization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#understanding-dropout\"\u003eUnderstanding Dropout\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#other-regularization-methods\"\u003eOther Regularization Methods\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#setting-up-our-optimization-problem\"\u003eSetting up our Optimization Problem\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#normalizing-inputs\"\u003eNormalizing Inputs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#vanishing-and-exploding-gradients\"\u003eVanishing and Exploding Gradients\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/#weight-initialization\"\u003eWeight Initialization\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"setting-up-our-machine-learning-problem\"\u003eSetting up our Machine Learning problem\u003c/h2\u003e\n\u003ch3 id=\"train--dev--test-sets\"\u003eTrain / Dev / Test sets\u003c/h3\u003e\n\u003cp\u003eMachine learning projects are highly iterative. That is, you try something new, see how it does and then adjust; very much like gradient descent. Therefore, you want the iteration time to be quick so that you can try as many things as quickly as possible, without affecting the final performance of the model. Part of this is setting up your datasets correctly so that you can efficiently iterate over different approaches.\u003c/p\u003e","title":"Improving Deep Learning Networks: Week 1 | Practical Aspects of Deep Learning"},{"content":"Final week of this course. Again, this week is pretty technical and a lot of the learning is done while coding up your own examples via the weekly programming assignments. The purpose of this week is to extend previous weeks\u0026rsquo; ideas into $L$-layered networks.\nThis week\u0026rsquo;s topics are:\nDeep L-Layer neural network Getting your matrix dimensions right Why deep representations? Parameters and Hyperparameters Deep L-Layer neural network The number of hidden layers in a neural network determine whether it is \u0026ldquo;shallow\u0026rdquo; or \u0026ldquo;deep\u0026rdquo;. Exactly how many layers is deep or shallow is not set in stone.\nMore notation is introduced to have an explicit way of communicating two things:\nThe number of layers The \u0026ldquo;height\u0026rdquo; of each layer, i.e. how many hidden units are in each layer. Therefore, the following notation is introduced:\n$L$ is the number of layers in your network. This includes the output layer, but it does not include the input layer (i.e. your features). The simple reason is that the input layer is usually layer $0$, so that your output layer is layer $L$. $n^{[l]}$ denotes the number of hidden units or nodes in layer $l$. This is the \u0026ldquo;height\u0026rdquo; of your hidden layer. $a^{[l]}$ is the corresponding activation for layer $l$. Remember that layers can have different activation functions so that $A^{[l]} = g^{[l]}(Z^{[l]})$, where $g^{[l]}$ is the activation function used in layer $l$. A key thing to remember is that for any layer, you calculate $A^{[l]}$ as: $$ \\begin{equation} Z^{[l]} = g^{[l]}(W^{[l]}A^{[l - 1]} + b^{[l]}) \\end{equation} $$\nRemembering that the input layer is usually denoted $A^{[0]} = X$.\nThis is pretty much all there is to deeper networks. The caveat however, is that this applies to fully-connected layers or feed-forward neural networks. In the fourth course, we will go over different architectures which try to represent information in more novel ways.\nGetting your matrix dimensions right If you\u0026rsquo;re implementing this from scratch, making sure that your dimensions are right is usually the first step in debugging. In other architectures such as CNNs it\u0026rsquo;s more involved, and therefore it becomes more important to keep track of the dimensions, at least in your head. Here are the key things to keep in mind:\nThe dimensions of arrays in vector land:\n$W^{[l]} = (n^{[l]}, n^{[l - 1]})$ $b^{[l]} = (n^{[l]}, 1)$ $z^{[l]} = a^{[l]} = (n^{[l]}, 1)$ In a single equation:\n$$ \\begin{equation} \\underset{(n^{[l]}, 1)}{z^{[l]}} = \\underset{(n^{[l]}, n^{[l -1]})}{W^{[l]}}\\underset{(n^{[l - 1]}, 1)}{a^{[l - 1]}} + \\underset{(n^{[l]}, 1)}{b^{[l]}} \\end{equation} $$\nNow, in matrix land:\n$W^{[l]} = (n^{[l]}, n^{[l - 1]})$ (remains the same) $b^{[l]} = (n^{[l]}, 1)$ (remains the same) $Z^{[l]} = A^{[l]} = (n^{[l]}, m)$, where $m$ is the number of training samples. Again, in a single equation:\n$$ \\begin{equation} \\underset{(n^{[l]}, m)}{Z^{[l]}} = \\underset{(n^{[l]}, n^{[l - 1]})}{W^{[l]}}\\underset{(n^{[l - 1]}, m)}{A^{[l-1]}} + \\underset{(n^{[l]}, 1)}{b^{[l]}} \\end{equation} $$\nNotice that adding $b^{[l]}$ to the product $W^{[l]}A^{[l-1]}$ is done via broadcasting with NumPy!\nA note about taking derivatives is that the derivatives should be the same dimensions as the arrays.\nWhy deep representations? We have been mentioning how deep neural networks amount to automatic feature generation, and how this is what really set deep neural networks apart from other contemporaneous models. Therefore, a key thing is that they do not just need to be \u0026ldquo;big\u0026rdquo;, but they need to be deep, i.e. have hidden layers. How many? Keep reading.\nThe basic idea is that as you run the features through your network, they are combined into more abstract features. The example used in the course is going from raw audio to phonemes, from phonemes to words, from words to sentences; and of course the magic is that you, the programmer, don\u0026rsquo;t to know what these are, but the machinery figures them out by itself by optimization.\nAn important idea mentioned in the course is that of the relationship between depth and height, that is the number of hidden units. First think of neural networks as trying to learn a function $f(x)$ that maps a vector $x \\mapsto y$. Now, there are some functions that you can estimate using \u0026ldquo;small\u0026rdquo; $L$-layer neural networks. However, if you want to use a shallower network and keep the same level of performance in the estimation, you will need to use exponentially more hidden units.\nMore precisely, if you have $n$ features, and you want to compute the exclusive or (XOR) of all the features, you will need a network which has a depth in the order of $O(\\log n)$, where $n$ is also the number of nodes in your network. On the other hand, if you\u0026rsquo;re forced to use a shallow network, i.e. logistic regression, you will need $O(2^n)$ nodes. In summary, adding layers to your network is much more computationally efficient than growing the size of a hidden layer. Authors Mhaskar, Liao and Poggio show this at much more detail in their paper.\nParameters and Hyperparameters The parameters $W^{[l]}, b^{[l]}$ are the things you derive via training. On the other hand hyperparameters are fixed during training and must be set before training.\nA good way to think about the difference between parameters and hyperparameters comes from Tamara Broderick\u0026rsquo;s slides. In machine learning we usually want to evaluate a hypothesis $h(x)$ that maps $x \\mapsto y$. A hypothesis can belong to a hypothesis class. A hypothesis class $H$ used by a learning algorithm is the set of all classifiers considered by it. 1 For example a linear classifier considers all classifiers whose decision boundary is linear. During training the algorithm will search within the hypothesis class for a particular hypothesis that minimizes the cost. Hyperparameters are related to the hypothesis class, while parameters are related to a particular hypothesis.\nHyperparameters are usually selected via hyperparameter tuning, which is the topic of the next course.\nSome hyperparameters in neural networks are:\nLearning rate $\\alpha$: the step-size of gradient descent. Number of iterations: how many times will the model observe the entire training set. Number of hidden layers. Size of hidden layers. Which activation functions and on which layers. As you can probably see, there are a lot of hyperparameters to search over, especially considering the size of the set of all combinations.\nThe first week of the next course\u0026rsquo;s post can be found here.\nHypothesis Class | Carnegie Mellon University\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/","summary":"\u003cp\u003eFinal week of this course. Again, this week is pretty technical and a lot of the learning is done while coding up your own examples via the weekly programming assignments. The purpose of this week is to extend previous weeks\u0026rsquo; ideas into $L$-layered networks.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week4/#deep-l-layer-neural-network\"\u003eDeep L-Layer neural network\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week4/#getting-your-matrix-dimensions-right\"\u003eGetting your matrix dimensions right\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week4/#why-deep-representations\"\u003eWhy deep representations?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week4/#parameters-and-hyperparameters\"\u003eParameters and Hyperparameters\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"deep-l-layer-neural-network\"\u003eDeep L-Layer neural network\u003c/h2\u003e\n\u003cp\u003eThe number of hidden layers in a neural network determine whether it is \u0026ldquo;shallow\u0026rdquo; or \u0026ldquo;deep\u0026rdquo;. Exactly how many layers is deep or shallow is not set in stone.\u003c/p\u003e","title":"Neural Networks and Deep Learning: Week 4 | Deep Neural Networks"},{"content":"This week\u0026rsquo;s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from a single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.\nThis week\u0026rsquo;s topics are:\nOverview Neural Network Representation Computing a Neural Network\u0026rsquo;s Output Vectorizing across multiple examples Activation functions Random Initialization Overview It\u0026rsquo;s time to refine our notation and to disambiguate some concepts introduced in week 2. Let\u0026rsquo;s start with the notation used in the course.\nOur inputs are represented by a vector $x^{(i)}$ where $i$ represents the $i$th training sample. The weights for layer $l$ are represented by a matrix $W^{[l]}$ The bias term for layer $l$ is represented by a vector $b^{[l]}$ The linear combination of layers $l$\u0026rsquo;s inputs is $z^{[l]} = W^{[l]}x + b^{[l]}$ Layer $l$\u0026rsquo;s output, after using an activation function (in this case the sigmoid) is $a^{[l]} = \\sigma(z^{[l]})$ $a^{[l]}$ is the input to layer $l + 1$ so that $z^{[l + 1]} = W^{[l + 1]}a^{[l]} + b^{[l + 1]}$ At the end, your loss is $\\mathcal{L}(a^{[L]}, y)$ Neural Network Representation The input layer corresponds to your training samples $x^{(1)}, x^{(2)}, \\dots, x^{(m)}$. You can also think of the input layer as $a^{[0]}$. This means that the first hidden layer is $a^{[1]}$, and in a neural network with a single hidden layer, the output layer would be $a^{[2]}$.\nGraduating from logistic regression to neural networks requires us to differentiate which $a$ we are taking about. This is because any hidden layer\u0026rsquo;s inputs, including the output layer, is the output of a previous layer.\nAn important comment in the course is that a network with a single hidden layer is usually referred to as a two layer network.\nComputing a Neural Network\u0026rsquo;s Output This section goes over how to graduate from vectors into matrices to improve our notation. It might look intimidating, but it\u0026rsquo;s just notation, and it\u0026rsquo;s very important to become comfortable with the dimensions of the layers.\nImagine that you have a single training example $x$ with three features: $x_1, x_2, x_3$. Imagine also that you have a two layer neural network, that is a neural network with a single hidden unit. Finally also imagine that the hidden layer has $4$ hidden units $a^{[1]}_1, a^{[1]}_2, a^{[1]}_3, a^{[1]}_4$. 1\nA two layer neural network\nNow let\u0026rsquo;s focus on calculating $z^{[1]} = W^{[1]}x + b^{[1]}$. Notice that $W^{[1]}$ is a matrix and this is how it is built: remember that we have four hidden units in our hidden layer. This means that $z^{[1]}$ will have four elements, $z_1^{[1]}, z_2^{[1]}, z_3^{[1]}, z_4^{[1]}$, and this is how each of them is calculated:\n$z_1^{[1]} = w_1^{[1]T}x + b_1^{[1]}$ $z_2^{[1]} = w_2^{[1]T}x + b_2^{[1]}$ $z_3^{[1]} = w_3^{[1]T}x + b_3^{[1]}$ $z_4^{[1]} = w_4^{[1]T}x + b_4^{[1]}$ Notice that $w_i^{[1]T}$ is actually a vector! Its size is the size of the previous layer, your features, so each $w_i^{[1]T}$ has three elements, each of which is multiplied by each of your input features. The main idea is to stack $w_i^{[1]T}$ into a matrix $W^{[1]}$, like this:\n$$ \\begin{equation} W^{[1]} = \\begin{bmatrix} \u0026mdash; \u0026amp; w_1^{[1]T} \u0026amp; \u0026mdash; \\\\ \u0026mdash; \u0026amp; w_2^{[1]T} \u0026amp; \u0026mdash; \\\\ \u0026mdash; \u0026amp; w_3^{[1]T} \u0026amp; \u0026mdash; \\\\ \u0026mdash; \u0026amp; w_4^{[1]T} \u0026amp; \u0026mdash; \\\\ \\end{bmatrix} \\end{equation} $$\nRemember that each $w_i^{[1]T}$ was of size 3, so our matrix $W^{[1]}$ is of dimensions $(4, 3)$. Because of how matrix-vector multiplication works when you define a vector as a column matrix, now we can do the whole thing in a single step. So that:\n$$ \\begin{equation} z^{[1]} = \\begin{bmatrix} \u0026mdash; \u0026amp; w_1^{[1]T} \u0026amp; \u0026mdash; \\\\ \u0026mdash; \u0026amp; w_2^{[1]T} \u0026amp; \u0026mdash; \\\\ \u0026mdash; \u0026amp; w_3^{[1]T} \u0026amp; \u0026mdash; \\\\ \u0026mdash; \u0026amp; w_4^{[1]T} \u0026amp; \u0026mdash; \\\\ \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} + \\begin{bmatrix} b_1^{[1]} \\\\ b_2^{[1]} \\\\ b_3^{[1]} \\\\ b_4^{[1]} \\end{bmatrix} = \\begin{bmatrix} w_1^{[1]T}x + b_1^{[1]} \\\\ w_2^{[1]T}x + b_2^{[1]} \\\\ w_3^{[1]T}x + b_3^{[1]} \\\\ w_4^{[1]T}x + b_4^{[1]} \\end{bmatrix} = \\begin{bmatrix} z_1^{[1]} \\\\ z_2^{[1]} \\\\ z_3^{[1]} \\\\ z_4^{[1]} \\end{bmatrix} \\end{equation} $$\nWhich you can simply rewrite as:\n$$ \\begin{equation} z^{[1]} = W^{[1]}x + b^{[1]} \\end{equation} $$\nWhich is a lot better!\nNow let\u0026rsquo;s not forget about $a^{[1]} = \\sigma(z^{[1]})$. This means that the sigmoid function $\\sigma(x)$ is applied element-wise to $z^{[1]}$. So that:\n$$ \\begin{equation} a^{[1]} = \\begin{bmatrix} \\sigma(z_1^{[1]}) \\\\ \\sigma(z_2^{[1]}) \\\\ \\sigma(z_3^{[1]}) \\\\ \\sigma(z_4^{[1]}) \\\\ \\end{bmatrix} \\end{equation} $$\nNow let\u0026rsquo;s keep track of the dimensions. Remember that we have $1$ training example with $3$ features and our single hidden layer has $4$ nodes:\n$\\underset{(4, 1)}{z^{[1]}} = \\underset{(4, 3)}{W^{[1]}}\\underset{(3, 1)}{x} + \\underset{(4, 1)}{b^{[1]}}$ $\\underset{(4, 1)}{a^{[1]}} = \\underset{(4, 1)}{\\sigma(z^{[1]})}$ $\\underset{(1, 1)}{z^{[2]}} = \\underset{(1, 4)}{W^{[2]}}\\underset{(4, 1)}{a^{[1]}} + \\underset{(1, 1)}{b^{[2]}}$ $\\underset{(1, 1)}{a^{[2]}} = \\underset{(1, 1)}{z^{[2]}}$ Notice that the dimensions of the arrays are below them. Remember that the product $AB$ of two matrices $A, B$ is only defined if the number of columns in $A$ equals the number of rows in $B$. So that you can multiply a $m \\times n$ matrix $A$ by a $n \\times p$ matrix $B$, and the result $AB$ will be a $m \\times p$ matrix. This is exactly why the dimensions have to line up. 2\nVectorizing across multiple examples Previously, $x$ was a single training sample. It had three features so that $x \\in \\mathbb{R}^3$ (this means that $x$ belongs to the set of all vectors that live in three-dimensional space).\nWe could run a for loop for each of our training samples and do the calculation in the previous section for each of the elements, but it turns out that linear algebra is the gift that keeps on givin'.\nIf we stack our training samples $x_1, \\dots x_m$ as columns of a matrix $X$ we can make our life infinitely easier. If our data has $n$ feature and $m$ training examples, then:\n$$ \\begin{equation} \\underset{(n, m)}{X} = \\begin{bmatrix} \\mid \u0026amp; \\mid \u0026amp; \u0026amp; \\mid \\\\ x^{(1)} \u0026amp; x^{(2)} \u0026amp; \\dots \u0026amp; x^{(m)} \\\\ \\mid \u0026amp; \\mid \u0026amp; \u0026amp; \\mid \\\\ \\end{bmatrix} \\end{equation} $$\nNotice that $x_i$ refers to the $i$th feature of a training example, while $x^{(i)}$ refers to the $i$th training example.\nNow we can rewrite our neural net with matrices for $Z^{[l]}, A^{[l]}$:\n$Z^{[1]} = W^{[1]}X + b^{[1]}$ $A^{[1]} = \\sigma(Z^{[1]})$ $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$ $A^{[2]} = \\sigma(Z^{[2]})$ Similar to how the training examples are stacked in the columns of $X$, each training example is a column of $A^{[l]}, Z^{[l]}$. In the case of $A^{[k]}$, the entry $i, j$ corresponds to the $i$th hidden-unit\u0026rsquo;s activation of the $k$th hidden layer on the $j$th training example.\nOn a personal note, I think that Andrew\u0026rsquo;s explanation of this is exceptional, and highlights the importance of being able to abstract away from one-training-example scale to neural-network scale.\nActivation functions So far we\u0026rsquo;ve picked the sigmoid function, $\\sigma(x)$, and slapped it at the end of our linear regression. But why did we pick it? We picked it to add non-linearities, which are important to deal with datasets that are non-linearly separable. Also think that the composition of two linear functions is itself a linear function. Without any non-linearities your network would not be able to learn more \u0026ldquo;interesting\u0026rdquo; (non-linear) features as you go deeper in the layers. It turns out however, that this is not our only choice, and in fact the choice matters in many ways.\nTamara Broderick is an Associate Professor at MIT, where she teaches machine learning and statistics. Being an all-around amazing person, she has published an entire course of machine learning on YouTube, and the slides made public on her website. The reason I mention this, besides the fact that she is an amazing teacher, is that her slides are beautiful; geometrically so. In her slides, you can see why how the choice of activation function changes the decision plane in graphical detail. I 100% suggest you check out these slides and marvel at the geometric beauty of neural nets!\nIt turns out that there are three widely used (some more than others) activation functions. We denote the activation function of layer $l$ as $g^{[l]}$. The functions are:\nHyperbolic Tangent Function: $g^{[l]}(z^{[l]}) = \\text{tanh}(z^{[l]}) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ Standard Logistic Function: $g^{[l]}(z^{[l]}) = \\sigma(z^{[l]}) = \\frac{1}{1 + e^{-z}}$ Rectified Linear Unit (ReLU): $g^{[l]}(z^{[l]}) = \\text{ReLu}(z^{[l]}) = \\max(0, z)$ Now when do you use which? Only use the $\\sigma(z)$ as your output function. If your output is strictly greater than $0$, you could also use a $\\text{ReLU}(z)$ here as well, even for regression. Sometimes you don\u0026rsquo;t need a non-linear activation function in your output layer for regression. The $\\text{tanh}(z)$ function is strictly superior to $\\sigma(z)$ in hidden layers. Most commonly use the $\\text{ReLU}(z)$ in hidden layers. Why? Because of differentiation! The derivatives of these functions have to be well-behaved across the domain in order for our fancy gradient descent to not get bogged by numerical precision issues. This is later covered in the course under the name of exploding or vanishing gradients.\nRandom Initialization How do we initialize our parameters? The choice of how we initialize our $W^{[l]}$ actually matters. There is a property known as symmetry. If you choose all your parameters as $0$, then you are not \u0026ldquo;breaking\u0026rdquo; symmetry. What this means is that every iteration of gradient descent will result in the same weights, therefore it will never improve your cost. You might be asking, hey what about $b^{[l]}$? It turns out that $b^{[l]}$ doesn\u0026rsquo;t have the symmetry problem, so you don\u0026rsquo;t need to worry too much about it, and can in fact initialize it to $0$.\nFine, but what do we initialize it to? You can use random numbers (actually not just any random numbers as we will see in the next course), with the only caveat that you need to scale your randomly chosen parameters by a constant, usually $0.01$ or $10^{-2}$. This is done to keep the numbers close to $0$, where the derivatives of your activation functions are better defined than at the extremes. Remember that $\\sigma(z^{[l]})$ is basically flat for values $|x| \\approx 6$. There are more sophisticated approaches to parameter initialization that will are covered in the next course.\nNext week\u0026rsquo;s post is here.\nDeep Learning Specialization | Coursera, Andrew Ng\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMultiplying matrices and vectors | Math Insight\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/","summary":"\u003cp\u003eThis week\u0026rsquo;s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from a single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week3/#overview\"\u003eOverview\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week3/#neural-network-representation\"\u003eNeural Network Representation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week3/#computing-a-neural-networks-output\"\u003eComputing a Neural Network\u0026rsquo;s Output\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week3/#vectorizing-across-multiple-examples\"\u003eVectorizing across multiple examples\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week3/#activation-functions\"\u003eActivation functions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week3/#random-initialization\"\u003eRandom Initialization\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eIt\u0026rsquo;s time to refine our notation and to disambiguate some concepts introduced in week 2. Let\u0026rsquo;s start with the notation used in the course.\u003c/p\u003e","title":"Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks"},{"content":"Here we kick off the second week of the first course in the specialization. This week is very technical, and many of the details shown in the course will be lost in the summarization. Also, a lot of the content is based on you doing programming assignments. There is simply no substitute for getting your hands dirty.\nThis week\u0026rsquo;s topics are:\nBinary Classification Logistic Regression Logistic Function Gradient Descent Computation Graph Python and Vectorization Broadcasting Binary Classification Binary classification is a supervised learning approach where you train what\u0026rsquo;s called a classifier. The binary classifier is a model that learns how to discriminate between two classes from the features, think about cats and dogs. A key concept is that of linearly separability:\nThe existence of a line separating the two types of points means that the data is linearly separable\nThis is a property of your training data, some datasets are linearly separable and others are not. You can a have a linear binary classifier that works perfectly if the data is linearly separable, and this is what a perceptron can do, the building blocks of neural nets. However, if you imagine a dataset where there is no line that separates the classes then you need a non-linear classifier, which is extremely common when you work with data in the wild. There are two main ways of adding non-linearities to a linear classifier: use a kernel classifier, or you can add a single hidden layer to a perceptron, making it into a neural network. One of the best demonstrations of this principle is in creating a classifier that works on the XOR problem. It turns out that the latter is much more versatile.\nLogistic Regression A basic perceptron can be a linear classifier, but by itself it cannot tell you how uncertain it is about a particular prediction. If dogs are labeled as $0$ and cats as $1$, then you don\u0026rsquo;t want ones and zeros, but actually a probability that a training sample is a cat (or a dog depending on the positive class). We can define what we want our prediction to be:\n$$ \\begin{equation} \\hat{y} = P(y = 1 \\mid x) \\end{equation} $$\nWhere $\\hat{y}$ is the probability that a picture of a dog or cat $x$ is actually a cat, $y=1$, given a particular picture $x$. We will never know the exact probability $y$ (a parameter), so we denote our estimate with a hat $\\hat{y}$ (a statistic). Logistic regression achieves this by composing two things:\nLinear regression Logistic function Linear regression is a way of fitting a line (or plane, or something as you go up higher dimensions) to some data. The line (or plane, etc.) will be a line that minimizes some measurement of error, usually the euclidean distance between a point, and it\u0026rsquo;s prediction. You can describe a line with two parameters, it\u0026rsquo;s intercept $b$, and it\u0026rsquo;s slope $w$. Linear regression finds $\\hat{b}$ and $\\hat{w}$ such that they minimize some loss or error. You can describe linear regression as a linear combination of your features and the parameters:\n$$ \\begin{equation} y_i = \\mathbf{x}_i^Tw + b \\end{equation} $$\nIt turns out that under some assumptions, linear regression has a closed form solution, which simply means that you can take out a pen and do algebra and solve for a set of linear equations using least squares. The next step will undo our ability to have a closed-form solution. If you\u0026rsquo;re into economics you might now this and much more by heart, but to the extent of this course, it\u0026rsquo;s enough to know that we have a way of fitting a line (linear classifier) to our data, and that it has a closed-form solution.\nLogistic Function We now have a way of fitting a line to our data. However, that line\u0026rsquo;s domain, it\u0026rsquo;s range of values, ranges over all the real numbers $\\mathbb{R}$. Our linear regression could output any number, but we want probabilities. It turns out that there is a nice way to map the real numbers $\\mathbb{R}$, or the interval $[-\\infty, \\infty]$ into the interval $(0, 1)$. You can do this with a type of sigmoid function, called the standard logistic function:\n$$ \\begin{equation} \\sigma(x) = f(x) = \\frac{1}{1+e^{-x}} \\end{equation} $$\nStandard Logistic Function\nIf $x$ is large, then $\\sigma(x) \\approx 1$, if $x$ is small, then $\\sigma(x) \\approx 0$, and everything in between in a continuous fashion. This is how we get our probabilities. Remember that any number $x^0 = 1$ so when $x = 0$ then $\\sigma(0) = 0.5$\nLogistic regression combines the two in a very literal way:\n$$ \\begin{equation} P(y = 1 \\mid x) = \\hat{y} = \\sigma(\\mathbf{x}_i^Tw + b) \\end{equation} $$\nWe literally just pass our linear regression estimate through a sigmoid function. It turns out that we no longer have access to a nice closed-form solution. This means that we will need to find the parameters $w, b$ via numerical optimization. Now, how do we know this amounts to estimating the probabilities? There is a lot of work done on this, under the name of maximum-likelihood estimation (MLE). For now assume that what you get out are estimates of the probabilities.\nLet\u0026rsquo;s pack our model\u0026rsquo;s parameters $w, b$ into a single vector $\\theta$, so that $\\theta_0 = b, \\theta_1 = w_0$, etc. Now we can have many parameters, one for each feature in our input, so that feature $i$ maps to $\\theta_{i+1}$ since $b$ is an additive constant. Now our logistic regression looks like this:\n$$ \\begin{equation} P(y = 1 \\mid x) = \\hat{y} = \\sigma(\\theta^TX) \\end{equation} $$\nWhere $X$ is a matrix representing our data, each sample in a row, and each feature as a column.\nGradient Descent To recap, we have combined a linear regression with a sigmoid function with the purpose of getting an estimate of the probability that an image is a cat. We know that there is no closed-solution to the equation, and that we need to find the parameters via optimization. Gradient descent does this for us. There is a ton of amazing gradient descent content online, so I will skip a lot of details.\nIn a nutshell, gradient descent is what fuels the \u0026ldquo;learning\u0026rdquo;. This is how the model is able to estimate the parameters from the training data, in our case $\\hat{\\theta}$. It starts with $\\hat{\\theta}$ chosen at random (more on this later), and then it runs a training sample through our logistic regression function. Then it compares the predicted value with the actual value, $(\\hat{y_i}, y_i)$ using a loss function, $\\mathcal{L}(\\hat{y_i}, y_i)$. The loss function tells us how bad a single prediction is. We get to be very creative with our loss functions, although many loss functions are well established for certain applications. On top of this, we have a cost function $\\mathcal{J}(\\theta)$, which computes the loss for our entire training set using a specific set of parameters.\nFinally, gradient descent does its magic. Gradient descent will differentiate $J(\\theta) = \\sum_{i = 1}^m\\frac{1}{m}\\mathcal{L}(\\hat{y_i}, y_i)$, our cost function averaged across our training set, with respect to $\\hat{\\theta}$, our current estimate of $\\theta$ (some random numbers currently). Remember that differentiation describes the rate of change between some function parameter and its output. In this case, we want to know how perturbing or changing $\\hat{\\theta}$, our parameters, changes $J(\\theta)$, how bad our prediction is. If $\\theta$ is a knob in our machine, we want to know which way to turn the knob to make our predictions better. This is what gradient descent does for us. It figures out in which direction our predictions become better with respect to $\\theta$, and then it takes a step (moves the knob) in the direction that minimizes our loss (how we want the machine to work). Then it repeats this steps many times, called iterations, until we converge to a minimum. The basic version of gradient descent uses the entire dataset for each iteration, and it calculates a cost function $J(\\theta)$ which is the loss of our predictions averaged across the entire dataset.\nThe name gradient descent comes from the fact that a gradient is simply the vector version of a derivative. Whereas the derivative is usually a scalar (single number), the gradient $\\nabla f(\\mathbf{x})$ is a vector describing how $f$ changes with respect to every element $x_i$ of $\\mathbf{x}$. Since at every iteration we are \u0026ldquo;taking a step\u0026rdquo; towards a lower (better) cost, it\u0026rsquo;s a descent.\nAll of this is done via basic calculus, using derivative rules. In the case of logistic regression, this is relatively easy to do. But in the case of neural networks, where we stack these logistic regression on top of each other, the calculus gets very messy.\nComputation Graph Since the calculus gets very messy, we might as well get a machine to do it for us. We describe this process to a machine via a computational graph; this is simply an ordering of the mathematical operations in our neural network. This particular ordering can be differentiated step by step using the basic rules of differentiation. This is nice because as our neural networks grow deeper, the computational graph also grows bigger and so more computation is required. The process described just now is called automatic differentiation, and it\u0026rsquo;s the core of modern deep learning software like PyTorch or TensorFlow.\nWith a computation graph and a way to calculate the derivatives, we can unleash gradient descent on our data. The basic steps are:\nForward propagation: Calculate the output of our model with the current $\\hat{\\theta}$ Backward propagation: Calculate the gradient of our cost function $J(\\theta)$ with respect to our current estimates $\\hat{\\theta}$ and then update $\\hat{\\theta}$ (via calculus) towards the minimum cost by some amount (the learning rate). Rinse and repeat Python and Vectorization This section shows you how to actually code these things from scratch in a relatively efficient manner. I say relatively, because in practice (or production to borrow a fancy term), you will almost never code these things up from scratch. Don\u0026rsquo;t reinvent the wheel (unless you think the wheel is obsolete). You will most likely implement a neural network using a mature software package such as PyTorch or Tensorflow. There are still key ideas that are important to discuss.\nVectorization or array programming, simply means applying a single operation to many pieces of data, it\u0026rsquo;s a $1:M$ operation where $M$ is the size of an array. For example, this is a $1 : 1$ operation:\n\u0026gt;\u0026gt;\u0026gt; \u0026#34;w\u0026#34;.upper() \u0026#34;W\u0026#34; A $1:M$ operation:\n\u0026gt;\u0026gt;\u0026gt; \u0026#34;\u0026#34;.join(map(str.upper, \u0026#34;Hello, World!\u0026#34;)) \u0026#34;HELLO, WORLD!\u0026#34; In Python, strings are sequences of characters, therefore they are iterable. What the code above does is it applies the str.upper method to all the elements of a string.\nThis is technically vectorization, but in general, the term vectorization usually refers to math and linear algebra operations. If we have two vectors $x, y$ both of size $3$, and we want to calculate the dot product between the two, we would have to write a loop:\nx = [1, 2, 3] y = [1, 2, 3] total = 0 for left, right in zip(x, y): total += left * right print(total) 14 Compare this with the NumPy version:\nx = np.array([1, 2, 3]) y = np.array([1, 2, 3]) total = x @ y # @ is the dot operator print(total) 14 You might think, oh well, it\u0026rsquo;s just syntactic sugar hiding the loop from us. But it\u0026rsquo;s much more than that. I suggest you time your code using much larger vectors and see how they measure up. Without getting bogged down into the details of how NumPy actually works, this is related to how NumPy uses pre-compiled C libraries to perform arithmetic and linear algebra operations. There\u0026rsquo;s a conversion cost to go from Python objects to NumPy (which uses C under the hood), but once you converted them, the speed-ups are tremendous. Here is more information about why is NumPy fast.\nThis concept even goes down to the hardware level and is present generally in computer architecture and design. Older machines usually subscribed to a single-instruction single-data (SISD) architecture. Where one instruction is applied to one piece of data in a CPU cycle. However, with single-instruction multiple-data (SIMD), you can apply a single instruction to multiple pieces of data in parallel. In a very general sense, this is why GPUs are so powerful in performing the same task (dot product) to arrays of things (images).\nBroadcasting The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is ‚Äúbroadcast‚Äù across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations, so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. There are, however, cases where broadcasting is a bad idea because it leads to inefficient use of memory that slows computation. 1\nI copied and pasted the definition above from the NumPy documentation because I\u0026rsquo;ve seen the term broadcasting be misunderstood by people. The idea is very basic: if you have a vectorized operation with arrays of different sizes in NumPy, it will \u0026ldquo;pad\u0026rdquo; the smaller one to match the bigger one.\nFor example:\nx = np.array([1, 2, 3, 4, 5]) y = np.array([1]) print(x + y) [2, 3, 4, 5, 6] Here we have that $x$ is a $5$ dimensional vector, while $y$ is a $1$ dimensional vector. We used the vectorized sum operator, which should perform an element-wise addition. That is, adding $y_0$ to every element $x_i$. But $y$ is smaller than $x$! So actually the $y$ vector is broadcasted (or padded) over the $x$ vector so that under the hood this is actually happening:\nx = np.array([1, 2, 3, 4, 5]) y = np.array([1, 1, 1, 1, 1]) print(x + y) [2 3 4 5 6] Which brings us back to vectorization. This is the most basic example of broadcasting, and there are some times when the dimensions of the arrays are incompatible, such as matrix multiplication. You can rely on NumPy complaining when this occurs.\nNext week\u0026rsquo;s post is here.\nBroadcasting\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week2/","summary":"\u003cp\u003eHere we kick off the second week of the first course in the specialization. This week is very technical, and many of the details shown in the course will be lost in the summarization. Also, a lot of the content is based on you doing programming assignments. There is simply no substitute for getting your hands dirty.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week2/#binary-classification\"\u003eBinary Classification\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week2/#logistic-regression\"\u003eLogistic Regression\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week2/#logistic-function\"\u003eLogistic Function\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week2/#gradient-descent\"\u003eGradient Descent\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week2/#computation-graph\"\u003eComputation Graph\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week2/#python-and-vectorization\"\u003ePython and Vectorization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/posts/coursera/deep-learning-specialization/nn-dl/week2/#broadcasting\"\u003eBroadcasting\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"binary-classification\"\u003eBinary Classification\u003c/h2\u003e\n\u003cp\u003eBinary classification is a supervised learning approach where you train what\u0026rsquo;s called a \u003cem\u003eclassifier\u003c/em\u003e. The binary classifier is a model that learns how to discriminate between two classes from the features, think about cats and dogs. A key concept is that of \u003ca href=\"https://en.wikipedia.org/wiki/Linear_separability\"\u003elinearly separability\u003c/a\u003e:\u003c/p\u003e","title":"Neural Networks and Deep Learning: Week 2 | Neural Network Basics"},{"content":"Introduction to Deep Learning This is the first course in Coursera\u0026rsquo;s Deep Learning Specialization. I will try to summarize the major topics presented in each week of each course in a series of posts. The purpose of this is to both deepen my own understanding by explaining, but also to help people who have not taken the specialization. Hopefully, these posts will inspire you to do so.\nThis week\u0026rsquo;s topics are:\nIntroduction to Deep Learning In the most basic sense, neural networks are a particular approach to machine learning, a process in which a computer learns to perform some task by analyzing training examples. They have their not so humble origins in people trying to understand and model the human brain, particularly the way in which humans learn and store information. Learning and information representation are one of the things that make deep learning powerful. But before we get to deep learning, we need to at least define learning.\nGlossing over the definition for what learning is, let\u0026rsquo;s say for now that learning amounts to being able to \u0026ldquo;reconstruct\u0026rdquo; some part of our data from the other parts (like supervised learning). Learning this reconstruction is what learning is within the context of machine learning.\nSo we said that neural networks are a particular approach to the machine learning problem. This approach uses a daisy-chain of building blocks called the perceptron. Think of the perceptron as a single-layer neural network.1 Deep neural networks have many such layers of perceptrons. Usually the input and output are called the input and output layers, while everything in between are called hidden layers.\nSimplified view of a feedforward artificial neural network\nIt turns out a single perceptron is not that flexible or generalizable, just like a single neuron is not that great at writing literature. However, and this is the important part, if you daisy-chain a bunch of them, and add some magic sauce (non-linearities) they are extremely generalizable. The magic of deep neural networks comes from the hidden layers. The hidden layers do something very important, which amounts to feature generation. For example if you have a dataset with some data, then the algorithm will learn how to combine the existing features into new features. And not just any features, but features that are relevant to learning the particular task. This is one of the key differences between deep learning and previous machine learning approaches. It does this by linearly combining the outputs from previous layers, down the layers, until the output layer.\nGoing back to learning, generally there are two main paradigms for learning (and a couple of others):\nSupervised Learning Unsupervised Learning At this point it\u0026rsquo;s not terribly important to know the exact difference, other than two things. Supervised learning is so called because you use labeled data. Labeled data are pairs of features (covariates) and their associated label. Features could be the picture of a cat and the label is whether it\u0026rsquo;s a cat or not, which is called classification. You could also predict the price of a house based on a house\u0026rsquo;s features, and this is called regression. In either case explaining the performance of the algorithm is very straightforward: we are close/far from perfectly predicting (recombining) our target from the training samples. On the other hand, with unsupervised learning, there are no labels in the data, so you cannot objectively measure how the algorithm is doing based on labels. Unsupervised learning does use cost functions, but they are not usually related to the labels in the data. These are commonly clustering or partitioning algorithms. The course focuses on supervised learning.\nNext week\u0026rsquo;s post is here.\nWikipedia | Perceptron\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week1/","summary":"\u003ch2 id=\"introduction-to-deep-learning\"\u003eIntroduction to Deep Learning\u003c/h2\u003e\n\u003cp\u003eThis is the first course in Coursera\u0026rsquo;s \u003ca href=\"https://www.coursera.org/specializations/deep-learning\"\u003eDeep Learning Specialization\u003c/a\u003e. I will try to summarize the major topics presented in each week of each course in a series of posts. The purpose of this is to both deepen my own understanding by explaining, but also to help people who have not taken the specialization. Hopefully, these posts will inspire you to do so.\u003c/p\u003e\n\u003cp\u003eThis week\u0026rsquo;s topics are:\u003c/p\u003e","title":"Neural Networks and Deep Learning: Week 1 | Introduction to Deep Learning"},{"content":"VSCode is very popular A lot of people like VSCode, 74% of respondents in StackOverflow\u0026rsquo;s 2022 Developer Survey said that VSCode is the IDE that they\u0026rsquo;ve used in the past year and also plan to keep using it. Why is VSCode liked so, and what do people like about it is not the focus of the post; but for sure it has to do with its extendable functionality, much of which comes from extensions.\nThe issue is that, as you might imagine, all that functionality doesn\u0026rsquo;t come for free. Making things work perfectly all the time is hard. Even harder is to make it efficient and lightweight. To work around performance, some functionalities take advantage of the fact that many consumer machines ship out with a very healthy CPU and an equally healthy amount of RAM. A sensible choice since most people use their work/personal machines to develop. But what about if you\u0026rsquo;re toying around on a cloud provider and don\u0026rsquo;t want to spend the money, or you\u0026rsquo;re taking a cloud-based programming class where they won\u0026rsquo;t let you run anything that costs more than the free-tier?\nWorking on itty-bitty machines AWS\u0026rsquo; t3.nano comes with 0.5 GB of memory; and if you Remote-SSH into your instance, it will most likely be okay, and you will be able to edit files and do a lot of things. Until you install some extensions to make your life considerably easier.\nFor me, it\u0026rsquo;s Microsoft\u0026rsquo;s Pylance, which offers a feature-rich support for Python in VSCode. Add to that some on-save linter or static checker, and now your machine will be either hanging or close to hanging.\nThe solution Thankfully, you can give your remote instance a well-deserved break and offload some cost of running extensions to your local machine. The distinction is that there are two kinds of extensions; UI Extensions and Workspace Extensions, and you can change an extension\u0026rsquo;s kind in your settings. Found this on the repo vscode-remote-release.\nBasically in your settings.json, you can set these settings:\n{ \u0026#34;remote.extensionKind\u0026#34;: { \u0026#34;ms-azuretools.vscode-cosmosdb\u0026#34;: [\u0026#34;ui\u0026#34;], \u0026#34;ms-vscode-remote.remote-ssh-edit\u0026#34;: [\u0026#34;workspace\u0026#34;] } } In my case, for Pylance it was changing these settings:\n{ \u0026#34;remote.extensionKind\u0026#34;: { \u0026#34;ms-python.python\u0026#34;: [\u0026#34;ui\u0026#34;], \u0026#34;ms-python.vscode-pylance\u0026#34;: [\u0026#34;ui\u0026#34;], } } Another very good comment by deltemp is that you can see exactly which Workspace Extensions are being run in the remote machine:\n$ ls ~/.vscode-server/extensions extensions.json golang.go-0.38.0 ms-python.python-2023.6.0 vscjava.vscode-java-debug-0.49.1 ms-python.vscode-pylance-2023.3.40 vscjava.vscode-java-dependency-0.21.2 ... You can use these same names in your settings.json, i.e. ms-python.vscode-pylance, just make sure it\u0026rsquo;s set to [ui].\nDownside The issue now is that by making the Python language service run locally, well, it runs locally. It won\u0026rsquo;t know anything about any Python environment that you might have in the remote instance. So now you have to choose your favorite virtual environment manager and replicate the environment locally. This is annoying if you want language features from a custom package that you\u0026rsquo;re writing currently; but why not do that entirely local and push the changes via a source-control tool?\nPersonally, I am happy with the setup. Between not using VSCode at all (or using it and dreading the next crash), and using it and waving my hands around package imports, I\u0026rsquo;d rather use it.\n","permalink":"https://manmartgarc.github.io/posts/tech-support/vscode-ec2/","summary":"\u003ch2 id=\"vscode-is-very-popular\"\u003eVSCode is very popular\u003c/h2\u003e\n\u003cp\u003eA lot of people like VSCode, 74% of respondents in \u003ca href=\"https://survey.stackoverflow.co/2022/#section-most-popular-technologies-integrated-development-environment\"\u003eStackOverflow\u0026rsquo;s 2022 Developer Survey\u003c/a\u003e said that VSCode is the IDE that they\u0026rsquo;ve used in the past year and also plan to keep using it. Why is VSCode liked so, and what do people like about it is not the focus of the post; but for sure it has to do with its extendable functionality, much of which comes from extensions.\u003c/p\u003e","title":"VSCode and Tiny Instances over Remote SSH"},{"content":"This is a working paper with Ana√Øs Fabre, Tomas Larroucau, Christopher Neilson and Ignacio Rios. The paper explores how information and beliefs have an effect on outcomes within the tertiary education market in Chile, which is a centralized college admissions system. The paper includes two waves of surveys done in 2019 and 2020 and an RCT done in 2021 with both government and NGOs. The abstract is below, and you can find the latest version of the working paper here.\nWe analyze the prevalence and relevance of application mistakes in a seemingly strategy-proof centralized college admissions system. We use data from Chile and exploit institutional features to identify a common type of application mistake: applying to programs without meeting all requirements (admissibility mistakes). We find that the growth of admissibility mistakes over time is driven primarily by growth on active score requirements. However, this effect fades out over time, suggesting that students might adapt to the new set of requirements but not immediately. To analyze application mistakes that are not observed in the data, we design nationwide surveys and collect information about students‚Äô true preferences, their subjective beliefs about admission probabilities, and their level of knowledge about admission requirements and admissibility mistakes. We find that between 2% - 4% of students do not list their true most preferred program, even though they face a strictly positive admission probability, and only a fraction of this skipping behavior can be rationalized by biases on students‚Äô subjective beliefs. In addition, we find a pull-to-center effect on beliefs, i.e., students tend to attenuate the probability of extreme events and under-predict the risk of not being assigned to the system. We use these insights to design and implement a large-scale information policy to reduce application mistakes. We find that showing personalized information about admission probabilities has a causal effect on improving students‚Äô outcomes, significantly reducing the risk of not being assigned to the centralized system and the incidence of admissibility mistakes. Our results suggest that information frictions play a significant role in affecting the performance of centralized college admissions systems, even when students do not face clear strategic incentives to misreport their preferences.\n","permalink":"https://manmartgarc.github.io/posts/mistakes/","summary":"\u003cp\u003eThis is a working paper with \u003ca href=\"https://sites.google.com/view/anais-fabre\"\u003eAna√Øs Fabre\u003c/a\u003e, \u003ca href=\"https://tlarroucau.github.io/\"\u003eTomas Larroucau\u003c/a\u003e, \u003ca href=\"https://christopherneilson.github.io/\"\u003eChristopher Neilson\u003c/a\u003e and \u003ca href=\"https://iriosu.github.io/\"\u003eIgnacio Rios\u003c/a\u003e. The paper explores how information and beliefs have an effect on outcomes within the tertiary education market in Chile, which is a centralized college admissions system. The paper includes two waves of surveys done in 2019 and 2020 and an RCT done in 2021 with both government and NGOs. The abstract is below, and you can find the latest version of the working paper \u003ca href=\"https://tlarroucau.github.io/Mistakes_College.pdf\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"Application Mistakes and Information frictions in College Admissions"},{"content":"TLDR $\\rightarrow$ You can view my CV here.\nI am currently working as a Software Engineer at Amazon Web Services in the EC2 organization. My focus is on building and maintaining large-scale data pipelines that focus on telemetry with downstream use cases in observability, analytics and reporting.\nFrom 2023 to 2025, I worked in the same team as I am now, but as a Data Engineer where I focused on modernizing our entire Apache Spark-based tech stack and the creation of a Redshift-based data warehouse to support cross-team analytics and promote data-driven decision-making.\nBack in June 2023 I graduated from the MSCAPP program, a graduate degree offered by the University of Chicago Harris School of Public Policy and the University of Chicago Department of Computer Science. The curriculum offers a high degree of flexibility, allowing students to specialize; I chose to focus on computer science, machine learning and software engineering. While a student, I worked as a Research Engineer at the Mansueto Institute for Urban Innovation at the University of Chicago helping researchers do their work more efficiently.\nBefore graduate school I worked as a data scientist and occasionally as a data engineer for ConsiliumBots, an NGO that leverages economic theory and technology to make navigation in education markets easier for families and students. I was also a Predoctoral Fellow (a synonym for research assistant) at Yale\u0026rsquo;s Tobin Center for Economic Policy for Christopher Neilson, Seth Zimmerman, and John Eric Humphries where I cut my teeth in the trenches of applied microeconomics.\nI completed a B.A. in Economics from Southern Connecticut State University, graduating in December 2016.\nThis website is hosted on GitHub Pages, with its source code in a public GitHub repository. More importantly, this website is powered by Hugo, using the beautiful Coder theme.\n","permalink":"https://manmartgarc.github.io/about/","summary":"\u003cp\u003e\u003cstrong\u003eTLDR\u003c/strong\u003e $\\rightarrow$ \u003ca href=\"/cv/\"\u003eYou can view my CV here\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eI am currently working as a Software Engineer at \u003ca href=\"https://aws.amazon.com/\"\u003eAmazon Web Services\u003c/a\u003e in the EC2 organization. My focus is on building and maintaining large-scale data pipelines that focus on telemetry with downstream use cases in observability, analytics and reporting.\u003c/p\u003e\n\u003cp\u003eFrom 2023 to 2025, I worked in the same team as I am now, but as a Data Engineer where I focused on modernizing our entire Apache Spark-based tech stack and the creation of a Redshift-based data warehouse to support cross-team analytics and promote data-driven decision-making.\u003c/p\u003e","title":"About"},{"content":" If the PDF doesn't display, download it here.\n","permalink":"https://manmartgarc.github.io/cv/","summary":"\u003cdiv class=\"pdf-embed\" style=\"margin: 1rem 0;\"\u003e\n  \u003cembed src=\"https://www.dropbox.com/s/oif6726jvsb3j15/martinez_cv.pdf?raw=1\" \n         type=\"application/pdf\"\n         width=\"100%\" \n         height=\"800px\" \n         style=\"border: 1px solid #ccc; border-radius: 4px;\"\u003e\n  \u003cp style=\"margin-top: 1rem;\"\u003eIf the PDF doesn't display, \u003ca href=\"https://www.dropbox.com/s/oif6726jvsb3j15/martinez_cv.pdf?raw=1\"\u003edownload it here\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e","title":"CV"}]