<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on Manuel Martinez</title>
    <link>https://manmartgarc.github.io/categories/deep-learning/</link>
    <description>Recent content in Deep Learning on Manuel Martinez</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 31 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://manmartgarc.github.io/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Convolutional Neural Networks: Week 3 | Detection Algorithms</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week3/</link>
      <pubDate>Mon, 31 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week3/</guid>
      <description>This is the third week of the fourth course of DeepLarning.AI&amp;rsquo;s Deep Learning Specialization offered on Coursera. The week focuses on object detection and localization, important applications of computer vision where CNNs serve as a building block to more specialized applications.
This week&amp;rsquo;s topics are:
Object Localization Landmark Detection Object Detection Sliding Windows Detection Convolutional Implementation of Sliding Windows Turning fully connected layers into convolutional layers Implementing sliding windows convolutionally Bounding Box Predictions Intersection Over Union Non-max Suppression Anchor Boxes Semantic Segmentation Transpose Convolutions U-Net Architecture Summary Object LocalizationLink to headingObject localization is, intuitively, not just detecting an object in an image, but also being able to describe its position in the image.</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks: Week 2 | Case Studies</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week2/</link>
      <pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week2/</guid>
      <description>This is the second week of the fourth course of DeepLarning.AI&amp;rsquo;s Deep Learning Specialization offered on Coursera. This week is largely a literature review, going over different architectures and approaches that have made large contributions to the field.
This week&amp;rsquo;s topics are:
Case Studies Classic Networks LeNet5 AlexNet VGG16 ResNets Why do ResNets Work? Networks in Networks | 1x1 Convolutions Inception Network Inception Network Architecture MobileNet Depth-wise Convolution Point-wise Convolution MobileNet Architecture Practical Advice for Using ConvNets Transfer Learning Data Augmentation Summary Case StudiesLink to headingWe should obviously keep up with the computer vision literature if we are interested in implementing new ideas.</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks: Week 1 | Foundations of CNNs</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week1/</link>
      <pubDate>Tue, 11 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week1/</guid>
      <description>This is the first week of the fourth course of DeepLarning.AI&amp;rsquo;s Deep Learning Specialization offered on Coursera. This course introduces convolutional neural networks, an extremely popular architecture in the field of computer vision.
This week&amp;rsquo;s topics are:
Computer Vision Convolution Convolution in continuous land Convolution in discrete land Back to Edge Detection Learning the Filters Padding Strided Convolutions Convolutions Over Volume One Layer of a CNN Defining the Notation and Dimensions Simple CNN Example Pooling Layers Full CNN Example Why Convolutions?</description>
    </item>
    
    <item>
      <title>Structuring ML Projects: Week 2 | ML Strategy</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/</guid>
      <description>This is the second week of the third course of DeepLearning.AI&amp;rsquo;s Deep Learning Specialization offered on Coursera. This course is less technical than the previous two, and focuses instead on general principles and intuition related to machine learning projects.
This week&amp;rsquo;s topics are:
Error Analysis Carrying out Error Analysis Cleaning up Incorrectly Labeled Data Build our First System Quickly, then Iterate Mismatched Training and Dev/Test Sets Training and Testing on Different Distributions Bias and Variance with Mismatched Data Distributions Addressing Data Mismatch Learning from Multiple Tasks Transfer Learning Multitask Learning End-to-end Deep Learning What is End-to-end Deep Learning?</description>
    </item>
    
    <item>
      <title>Structuring ML Projects: Week 1 | ML Strategy</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/</link>
      <pubDate>Fri, 23 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/</guid>
      <description>This is the first week of the third course of DeepLearning.AI&amp;rsquo;s Deep Learning Specialization offered on Coursera. This course is less technical than the previous two, and focuses instead on general principles and intuition related to machine learning projects.
This week&amp;rsquo;s topics are:
Introduction to ML Strategy Why ML Strategy Orthogonalization Setting Up our Goal Single Number Evaluation Metric Satisficing and Optimizing Metrics Train/Dev/Test Distributions Size of Dev and Test Sets When to Change Dev/Test Sets and Metrics?</description>
    </item>
    
    <item>
      <title>Improving Deep Learning Networks: Week 3 | Hyperparameter Tuning, Batch Optimization, Programming Frameworks</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/</link>
      <pubDate>Wed, 21 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/</guid>
      <description>This is the third and last week of the second course of DeepLearning.AI&amp;rsquo;s Deep Learning Specialization offered on Coursera.
This week&amp;rsquo;s topics are:
Hyperparameter Tuning Tuning Process Random Search Coarse-to-fine Grained Search Using an Appropriate Scale when Searching Python Implementation Hyperparameter Tuning in Practice: Pandas vs. Caviar Batch Normalization Normalizing Activations in a Network Fitting Batch Norm into a Neural Network Why does Batch Norm work? Batch Norm at Test Time Multi-class Classification Softmax Regression Training a Softmax Classifier Programming Frameworks Summary Hyperparameter TuningLink to headingWe have seen by now that neural networks have a lot of hyperparameters.</description>
    </item>
    
    <item>
      <title>Improving Deep Learning Networks: Week 2 | Optimization Algorithms</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/</link>
      <pubDate>Mon, 19 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/</guid>
      <description>This week is focused on the optimization or training process. Particularly, this week goes over ways to make the training process faster and more efficient, allowing us to iterate quicker when trying different approaches.
This week&amp;rsquo;s topics are:
Mini-batch Gradient Descent Exponentially Weighted Moving Averages (EWMA) Bias Correction Gradient Descent with Momentum RMSProp Adam Learning Rate Decay Summary Mini-batch Gradient DescentLink to headingWe discussed gradient descent briefly on the second week of the first course.</description>
    </item>
    
    <item>
      <title>Improving Deep Learning Networks: Week 1 | Practical Aspects of Deep Learning</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/</link>
      <pubDate>Sun, 18 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/</guid>
      <description>This is the first week in the second course of DeepLearning.AI&amp;rsquo;s Deep Learning Specialization offered on Coursera. The course deals with hyperparameter tuning, regularization and the optimization of the training process. The optimization ranges from computational complexity to performance. This is again a pretty technical week, where you will benefit a lot from doing the programming exercises.
This week&amp;rsquo;s topics are:
Setting up our Machine Learning problem Train / Dev / Test sets Bias-Variance Tradeoff Basic Recipe for Machine Learning Regularizing our Neural Network Regularization Why Does Regularization Reduce Overfitting?</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning: Week 4 | Deep Neural Networks</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/</link>
      <pubDate>Fri, 16 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/</guid>
      <description>Final week of this course. Again this week is pretty technical and a lot of the learning is done while coding up your own examples via the weekly programming assignments. The purpose of this week is to extend previous&amp;rsquo; weeks ideas into $L$-layered networks.
This week&amp;rsquo;s topics are:
Deep L-Layer neural network Getting your matrix dimensions right Why deep representations? Parameters and Hyperparameters Summary Deep L-Layer neural networkLink to headingThe number of hidden layers in a neural network determine whether it is &amp;ldquo;shallow&amp;rdquo; or &amp;ldquo;deep&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/</link>
      <pubDate>Thu, 15 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/</guid>
      <description>This week&amp;rsquo;s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.
This week&amp;rsquo;s topics are:
Overview Neural Network Representation Computing a Neural Network&amp;rsquo;s Output Vectorizing across multiple examples Activation functions Random Initialization Summary OverviewLink to headingIt&amp;rsquo;s time to refine our notation and to disambiguate some concepts introduced in week 2.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning: Week 2 | Neural Network Basics</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week2/</link>
      <pubDate>Wed, 14 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week2/</guid>
      <description>Here we kick off the second week of the first course in the specialization. This week is very technical and many of the details that are shown in the course will be lost in the summarization. Also, a lot of the content is based on you doing programming assignments. There is simply no substitute to getting your hands dirty.
This week&amp;rsquo;s topics are:
Binary Classification Logistic Regression Logistic Function Gradient Descent Computation Graph Python and Vectorization Broadcasting Summary Binary ClassificationLink to headingBinary classification is a supervised learning approach where you train what&amp;rsquo;s called a classifier.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning: Week 1 | Introduction to Deep Learning</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week1/</link>
      <pubDate>Tue, 13 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week1/</guid>
      <description>Introduction to Deep LearningLink to headingThis is the first course in Coursera&amp;rsquo;s Deep Learning Specialization. I will try to summarize the major topics presented in each of the weeks of each course in a series of posts. The purpose of this is to both deepen my own understanding by explaining, but also for people who have not taken the specialization. Hopefully these posts will inspire you to do so.</description>
    </item>
    
  </channel>
</rss>
