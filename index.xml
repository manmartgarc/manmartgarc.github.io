<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Manuel Martinez</title>
    <link>https://manmartgarc.github.io/</link>
    <description>Recent content on Manuel Martinez</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Mon, 23 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://manmartgarc.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Lambda Functions with Go and CDK</title>
      <link>https://manmartgarc.github.io/posts/tech-support/go-lambda/</link>
      <pubDate>Mon, 23 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/tech-support/go-lambda/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;&#xA;  Introduction&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#introduction&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/lambda/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AWS Lambda&lt;/a&gt; is a serverless computing service that lets you run code without provisioning or managing servers. Lambdas are very flexible because you can run them on a &lt;a href=&#34;https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;schedule&lt;/a&gt;, in response to &lt;a href=&#34;https://docs.aws.amazon.com/lambda/latest/dg/lambda-services.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;events&lt;/a&gt;, or even as an &lt;a href=&#34;https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;API endpoint&lt;/a&gt; that responds to an HTTP request. In this post, we&amp;rsquo;ll look at how to create a Lambda function using Go and the AWS Cloud Development Kit (CDK) - with a focus on using CDK to speed up the development process and improve operational efficiency.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sequence Models: Week 4 | Transformers</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week4/</link>
      <pubDate>Sun, 13 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week4/</guid>
      <description>&lt;p&gt;This is the fourth and last week of the &lt;a href=&#34;https://www.coursera.org/learn/nlp-sequence-models?specialization=deep-learning/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fifth course&lt;/a&gt; of DeepLearning.AI&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning Specialization&lt;/a&gt; offered on Coursera. The main topic for this week are transformers, a generalization of the attention model that has taken the deep learning world by storm since its inception in 2017.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#transformer-network-intuition&#34; &gt;Transformer Network Intuition&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#self-attention&#34; &gt;Self-Attention&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#multi-head-attention&#34; &gt;Multi-Head Attention&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#transformer-network-architecture&#34; &gt;Transformer Network Architecture&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#more-information&#34; &gt;More Information&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;transformer-network-intuition&#34;&gt;&#xA;  Transformer Network Intuition&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#transformer-network-intuition&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;We started with RNNs (known as part of the &lt;a href=&#34;https://www.youtube.com/watch?v=XfpMkf4rD6E&amp;amp;t=1436s&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;prehistoric era now&lt;/a&gt;), a simple model that reutilizes the same weights at each time steps; allowing to combine previous step&amp;rsquo;s hidden states with the current one. To solve some issues with vanilla RNNs, we introduced GRUs and LSTMs; both more flexible and more complex than simple RNNs. However, one of the things that they all share in common is that the &lt;em&gt;input&lt;/em&gt; must be processed sequentially, i.e. one token at a time. This is a problem with large models, where we want to parallelize computation as much as possible. &lt;a href=&#34;https://en.wikipedia.org/wiki/Amdahl%27s_law&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amdahl&amp;rsquo;s Law&lt;/a&gt; gives us a theoretical speed up limit based on the fraction of parallelizable compute in a computer program. Unfortunately, since the entire model is sequential the speed-ups are miniscule. The transformer architecture allows us to process &lt;em&gt;the entire&lt;/em&gt; input at once, and in parallel; allowing us to train much more complex models which in turn generate richer feature representations of our sequences.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sequence Models: Week 3 | Sequence Models &amp; Attention Mechanism</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week3/</link>
      <pubDate>Sat, 12 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week3/</guid>
      <description>&lt;p&gt;This is the third week of the &lt;a href=&#34;https://www.coursera.org/learn/nlp-sequence-models?specialization=deep-learning/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fifth course&lt;/a&gt; of DeepLearning.AI&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning Specialization&lt;/a&gt; offered on Coursera. This week goes over sequence-to-sequence models using beam search to optimize the classification step. We also go over the important concept of attention which generalizes a couple of things seen in the last week.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#sequence-to-sequence-architectures&#34; &gt;Sequence to Sequence Architectures&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#basic-seq2seq-models&#34; &gt;Basic Seq2Seq Models&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#picking-the-most-likely-sentence&#34; &gt;Picking the Most Likely Sentence&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#why-not-greedy-search&#34; &gt;Why not Greedy Search?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#beam-search&#34; &gt;Beam Search&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#refinements&#34; &gt;Refinements&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#error-analysis&#34; &gt;Error Analysis&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#attention&#34; &gt;Attention&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#developing-intuition&#34; &gt;Developing Intuition&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#defining-the-attention-model&#34; &gt;Defining the Attention Model&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;sequence-to-sequence-architectures&#34;&gt;&#xA;  Sequence to Sequence Architectures&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#sequence-to-sequence-architectures&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The basic example for sequence-to-sequence approaches was also covered in the &lt;a href=&#34;https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week1/&#34; &gt;first week&lt;/a&gt; of the course; where we discussed the many-to-many RNN approach where $T_x \neq T_y$. This encoder-decoder approach is what we will start discussing in the context of machine translation, a sequence-to-sequence application example.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sequence Models: Week 2 | NLP &amp; Word Embeddings</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week2/</link>
      <pubDate>Thu, 10 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week2/</guid>
      <description>&lt;p&gt;This is the second week of the &lt;a href=&#34;https://www.coursera.org/learn/nlp-sequence-models?specialization=deep-learning/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fifth course&lt;/a&gt; of DeepLearning.AI&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning Specialization&lt;/a&gt; offered on Coursera. In this week we go over a little more in depth into natural language applications with sequence models, and also discuss word embeddings; an amazing technique for extracting semantic meaning from words.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction-to-word-embeddings&#34; &gt;Introduction to Word Embeddings&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#word-representation&#34; &gt;Word Representation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#using-word-embeddings&#34; &gt;Using Word Embeddings&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#properties-of-word-embeddings&#34; &gt;Properties of Word Embeddings&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#cosine-similarity&#34; &gt;Cosine Similarity&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#embedding-matrix&#34; &gt;Embedding Matrix&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#word-embeddings&#34; &gt;Word Embeddings&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#learning-word-embeddings&#34; &gt;Learning Word Embeddings&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#word2vec&#34; &gt;Word2Vec&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#negative-sampling&#34; &gt;Negative Sampling&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#glove-word-vectors&#34; &gt;GloVe Word Vectors&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#applications-using-word-embeddings&#34; &gt;Applications Using Word Embeddings&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#sentiment-classification&#34; &gt;Sentiment Classification&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#de-biasing-word-embeddings&#34; &gt;De-biasing Word Embeddings&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;introduction-to-word-embeddings&#34;&gt;&#xA;  Introduction to Word Embeddings&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#introduction-to-word-embeddings&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;word-representation&#34;&gt;&#xA;  Word Representation&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#word-representation&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Word embeddings are a way of representing words. The approach borrows from dimensionality reduction, and combines it with optimization. These two things allow us to create new word representations that are empirically good with respect to some task. Let&amp;rsquo;s go over how this is possible.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sequence Models: Week 1 | Recurrent Neural Networks</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week1/</link>
      <pubDate>Tue, 08 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/sequence-models/week1/</guid>
      <description>&lt;p&gt;This is the first week of the &lt;a href=&#34;https://www.coursera.org/learn/nlp-sequence-models?specialization=deep-learning/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fifth course&lt;/a&gt; of DeepLearning.AI&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning Specialization&lt;/a&gt; offered on Coursera. In this week we go over some motivation for sequence models. These are models that are designed to work with sequential data, otherwise known as time-series. Let&amp;rsquo;s get started.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#why-sequence-models&#34; &gt;Why Sequence Models?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#notation&#34; &gt;Notation&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#representing-words&#34; &gt;Representing Words&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#recurrent-neural-network&#34; &gt;Recurrent Neural Network&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#forward-propagation&#34; &gt;Forward Propagation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#different-types-of-rnns&#34; &gt;Different Types of RNNs&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#language-model-and-sequence-generation&#34; &gt;Language Model and Sequence Generation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#vanishing-gradients-with-rnns&#34; &gt;Vanishing Gradients with RNNs&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gated-recurrent-unit&#34; &gt;Gated Recurrent Unit&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#long-short-term-memory&#34; &gt;Long Short-Term Memory&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#bidirectional-rnn&#34; &gt;Bidirectional RNN&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;why-sequence-models&#34;&gt;&#xA;  Why Sequence Models?&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#why-sequence-models&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Time-series get to be their own thing, just like in regression analysis. This time, since we are focusing on prediction instead of inference, we are less concerned about the statistical properties of the parameters we estimate, but we&amp;rsquo;d like our models to do very well in their prediction tasks. But how can we exploit temporal information, without using classical methods such as AR methods? The current bag of tricks we have developed so far will only take us some distance. Here are a couple of hiccups:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Convolutional Neural Networks: Week 4 | Face Recognition &amp; Neural Style Transfer</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week4/</link>
      <pubDate>Wed, 02 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week4/</guid>
      <description>&lt;p&gt;This is the fourth and last week of the &lt;a href=&#34;https://www.coursera.org/learn/convolutional-neural-networks/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fourth course&lt;/a&gt; of DeepLearning.AI&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning Specialization&lt;/a&gt; offered on Coursera. In this week we go over special applications in the field of computer vision with CNNs, face recognition and neural style transfer. This week introduces new important concepts that will be useful even beyond the context of CNNs.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are as follows:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#face-recognition&#34; &gt;Face Recognition&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#what-is-face-recognition&#34; &gt;What is Face Recognition?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#one-shot-learning&#34; &gt;One Shot Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#siamese-network&#34; &gt;Siamese Network&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#triplet-loss&#34; &gt;Triplet Loss&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#face-verification-and-binary-classification&#34; &gt;Face Verification and Binary Classification&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#neural-style-transfer&#34; &gt;Neural Style Transfer&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#what-is-neural-style-transfer&#34; &gt;What is Neural Style Transfer?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#what-are-deep-cnns-learning&#34; &gt;What are deep CNNs learning?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#cost-function&#34; &gt;Cost Function&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#content-cost-function&#34; &gt;Content Cost Function&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#style-cost-function&#34; &gt;Style Cost Function&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;face-recognition&#34;&gt;&#xA;  Face Recognition&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#face-recognition&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;what-is-face-recognition&#34;&gt;&#xA;  What is Face Recognition?&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#what-is-face-recognition&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s start by going over the important distinction between face verification and face recognition.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Convolutional Neural Networks: Week 3 | Detection Algorithms</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week3/</link>
      <pubDate>Tue, 01 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week3/</guid>
      <description>&lt;p&gt;This is the third week of the &lt;a href=&#34;https://www.coursera.org/learn/convolutional-neural-networks/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fourth course&lt;/a&gt; of DeepLearning.AI&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning Specialization&lt;/a&gt; offered on Coursera. The week focuses on object detection and localization, important applications of computer vision where CNNs serve as a building block to more specialized applications.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#object-localization&#34; &gt;Object Localization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#landmark-detection&#34; &gt;Landmark Detection&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#object-detection&#34; &gt;Object Detection&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#sliding-windows-detection&#34; &gt;Sliding Windows Detection&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#convolutional-implementation-of-sliding-windows&#34; &gt;Convolutional Implementation of Sliding Windows&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#turning-fully-connected-layers-into-convolutional-layers&#34; &gt;Turning fully connected layers into convolutional layers&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#implementing-sliding-windows-convolutionally&#34; &gt;Implementing sliding windows convolutionally&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#bounding-box-predictions&#34; &gt;Bounding Box Predictions&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#intersection-over-union&#34; &gt;Intersection Over Union&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#non-max-suppression&#34; &gt;Non-max Suppression&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#anchor-boxes&#34; &gt;Anchor Boxes&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#semantic-segmentation&#34; &gt;Semantic Segmentation&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#transpose-convolutions&#34; &gt;Transpose Convolutions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#u-net-architecture&#34; &gt;U-Net Architecture&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;object-localization&#34;&gt;&#xA;  Object Localization&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#object-localization&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Object localization is, intuitively, not just detecting an object in an image, but also being able to describe its position in the image. We &lt;a href=&#34;https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week2/&#34; &gt;previously&lt;/a&gt; discussed how we can train a classifier using images with CNNs. The new kink is the localization of the object in the image.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Convolutional Neural Networks: Week 2 | Case Studies</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week2/</link>
      <pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week2/</guid>
      <description>&lt;p&gt;This is the second week of the &lt;a href=&#34;https://www.coursera.org/learn/convolutional-neural-networks/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fourth course&lt;/a&gt; of DeepLearning.AI&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning Specialization&lt;/a&gt; offered on Coursera. This week is largely a literature review, going over different architectures and approaches that have made large contributions to the field.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#case-studies&#34; &gt;Case Studies&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#classic-networks&#34; &gt;Classic Networks&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#lenet5&#34; &gt;LeNet5&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#alexnet&#34; &gt;AlexNet&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#vgg16&#34; &gt;VGG16&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#resnets&#34; &gt;ResNets&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#why-do-resnets-work&#34; &gt;Why do ResNets Work?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#networks-in-networks--1x1-convolutions&#34; &gt;Networks in Networks | 1x1 Convolutions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#inception-network&#34; &gt;Inception Network&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#inception-network-architecture&#34; &gt;Inception Network Architecture&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#mobilenet&#34; &gt;MobileNet&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#depth-wise-convolution&#34; &gt;Depth-wise Convolution&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#point-wise-convolution&#34; &gt;Point-wise Convolution&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#mobilenet-architecture&#34; &gt;MobileNet Architecture&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#practical-advice-for-using-convnets&#34; &gt;Practical Advice for Using ConvNets&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#transfer-learning&#34; &gt;Transfer Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#data-augmentation&#34; &gt;Data Augmentation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;case-studies&#34;&gt;&#xA;  Case Studies&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#case-studies&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;We should obviously keep up with the computer vision literature if we are interested in implementing new ideas. However, since CNNs have so many hyperparameters and settings, it&amp;rsquo;s essential to pay attention to the empirically justified advances occurring in the field. Since so many computer vision tasks are similar, many of the core ideas of new approaches can be applied, sometimes identically and sometimes with minor editions, to new applications. Finally, in the age of big data and cheap compute, we can get away with virtually free-riding on somebody else&amp;rsquo;s compute by using their pre-trained model. Let&amp;rsquo;s start with the &amp;ldquo;classic&amp;rdquo; networks that hit the field from the late 90s through the early 10s.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Convolutional Neural Networks: Week 1 | Foundations of CNNs</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week1/</link>
      <pubDate>Tue, 11 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/cnns/week1/</guid>
      <description>&lt;p&gt;This is the first week of the &lt;a href=&#34;https://www.coursera.org/learn/convolutional-neural-networks/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fourth course&lt;/a&gt; of DeepLearning.AI&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning Specialization&lt;/a&gt; offered on Coursera. This course introduces convolutional neural networks, an extremely popular architecture in the field of computer vision.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#computer-vision&#34; &gt;Computer Vision&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#convolution&#34; &gt;Convolution&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#convolution-in-continuous-land&#34; &gt;Convolution in continuous land&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#convolution-in-discrete-land&#34; &gt;Convolution in discrete land&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#back-to-edge-detection&#34; &gt;Back to Edge Detection&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#learning-the-filters&#34; &gt;Learning the Filters&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#padding&#34; &gt;Padding&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#strided-convolutions&#34; &gt;Strided Convolutions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#convolutions-over-volume&#34; &gt;Convolutions Over Volume&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#one-layer-of-a-cnn&#34; &gt;One Layer of a CNN&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#defining-the-notation-and-dimensions&#34; &gt;Defining the Notation and Dimensions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#simple-cnn-example&#34; &gt;Simple CNN Example&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#pooling-layers&#34; &gt;Pooling Layers&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#full-cnn-example&#34; &gt;Full CNN Example&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#why-convolutions&#34; &gt;Why Convolutions?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;computer-vision&#34;&gt;&#xA;  Computer Vision&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#computer-vision&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;If you can think of any computer vision application today: self-driving cars, medical imaging, face recognition and even visual generative AI; it&amp;rsquo;s very likely that they&amp;rsquo;re using some kind of convolutional architecture. Computer vision is a field of computer science that focuses on enabling computers to identify and understand objects and people in images and videos &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Identification and understanding are nebulous words, but the key thing is that computer vision involves processing digital images and videos. Let&amp;rsquo;s think about how we could represent an image and use our existing knowledge about neural networks to design a cat classifier.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Structuring ML Projects: Week 2 | ML Strategy</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week2/</guid>
      <description>&lt;p&gt;This is the second week of the &lt;a href=&#34;https://www.coursera.org/learn/machine-learning-projects?specialization=deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;third course&lt;/a&gt; of DeepLearning.AI&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning Specialization&lt;/a&gt; offered on Coursera. This course is less technical than the previous two, and focuses instead on general principles and intuition related to machine learning projects.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#error-analysis&#34; &gt;Error Analysis&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#carrying-out-error-analysis&#34; &gt;Carrying out Error Analysis&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#cleaning-up-incorrectly-labeled-data&#34; &gt;Cleaning up Incorrectly Labeled Data&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#build-our-first-system-quickly-then-iterate&#34; &gt;Build our First System Quickly, then Iterate&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#mismatched-training-and-devtest-sets&#34; &gt;Mismatched Training and Dev/Test Sets&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#training-and-testing-on-different-distributions&#34; &gt;Training and Testing on Different Distributions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#bias-and-variance-with-mismatched-data-distributions&#34; &gt;Bias and Variance with Mismatched Data Distributions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#addressing-data-mismatch&#34; &gt;Addressing Data Mismatch&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#learning-from-multiple-tasks&#34; &gt;Learning from Multiple Tasks&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#transfer-learning&#34; &gt;Transfer Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#multitask-learning&#34; &gt;Multitask Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#end-to-end-deep-learning&#34; &gt;End-to-end Deep Learning&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#what-is-end-to-end-deep-learning&#34; &gt;What is End-to-end Deep Learning?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#whether-to-use-end-to-end-deep-learning&#34; &gt;Whether to use End-to-end Deep Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;error-analysis&#34;&gt;&#xA;  Error Analysis&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#error-analysis&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;carrying-out-error-analysis&#34;&gt;&#xA;  Carrying out Error Analysis&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#carrying-out-error-analysis&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;One of the things that we can do when our model is performing worse than human-level performance is to carry out error analysis. Error analysis is just a fancy name to trying to ascertain what the sources of errors are. This is critical because if we can quickly come up with a &amp;ldquo;ceiling&amp;rdquo; or upper-bound on the improvement of a particular strategy.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Structuring ML Projects: Week 1 | ML Strategy</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/</link>
      <pubDate>Fri, 23 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/structuring-ml-projects/week1/</guid>
      <description>&lt;p&gt;This is the first week of the &lt;a href=&#34;https://www.coursera.org/learn/machine-learning-projects?specialization=deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;third course&lt;/a&gt; of DeepLearning.AI&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning Specialization&lt;/a&gt; offered on Coursera. This course is less technical than the previous two, and focuses instead on general principles and intuition related to machine learning projects.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#introduction-to-ml-strategy&#34; &gt;Introduction to ML Strategy&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#why-ml-strategy&#34; &gt;Why ML Strategy&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#orthogonalization&#34; &gt;Orthogonalization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#setting-up-our-goal&#34; &gt;Setting Up our Goal&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#single-number-evaluation-metric&#34; &gt;Single Number Evaluation Metric&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#satisficing-and-optimizing-metrics&#34; &gt;Satisficing and Optimizing Metrics&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#traindevtest-distributions&#34; &gt;Train/Dev/Test Distributions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#size-of-dev-and-test-sets&#34; &gt;Size of Dev and Test Sets&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#when-to-change-devtest-sets-and-metrics&#34; &gt;When to Change Dev/Test Sets and Metrics?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#comparing-to-human-level-performance&#34; &gt;Comparing to Human-Level Performance&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#why-human-level-performance&#34; &gt;Why Human-level performance?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#avoidable-bias&#34; &gt;Avoidable Bias&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#understanding-human-level-performance&#34; &gt;Understanding Human-level Performance&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#surpassing-human-level-performance&#34; &gt;Surpassing Human-level Performance&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#improving-your-model-performance&#34; &gt;Improving your Model Performance&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;introduction-to-ml-strategy&#34;&gt;&#xA;  Introduction to ML Strategy&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#introduction-to-ml-strategy&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;why-ml-strategy&#34;&gt;&#xA;  Why ML Strategy&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#why-ml-strategy&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Whenever we are working on a machine learning project, and after we have completed our first iteration on the approach, there might be many things to try next. Should we get more data? Try regularization? Try a bigger network? So many things to choose from. This is why we need high-level heuristics to guide our strategy.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Improving Deep Learning Networks: Week 3 | Hyperparameter Tuning, Batch Optimization, Programming Frameworks</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/</link>
      <pubDate>Wed, 21 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week3/</guid>
      <description>&lt;p&gt;This is the third and last week of the &lt;a href=&#34;https://www.coursera.org/learn/deep-neural-network&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;second course&lt;/a&gt; of DeepLearning.AI&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning Specialization&lt;/a&gt; offered on Coursera.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#hyperparameter-tuning&#34; &gt;Hyperparameter Tuning&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#tuning-process&#34; &gt;Tuning Process&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#random-search&#34; &gt;Random Search&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#coarse-to-fine-grained-search&#34; &gt;Coarse-to-fine Grained Search&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#using-an-appropriate-scale-when-searching&#34; &gt;Using an Appropriate Scale when Searching&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#python-implementation&#34; &gt;Python Implementation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#hyperparameter-tuning-in-practice-pandas-vs-caviar&#34; &gt;Hyperparameter Tuning in Practice: Pandas vs. Caviar&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#batch-normalization&#34; &gt;Batch Normalization&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#normalizing-activations-in-a-network&#34; &gt;Normalizing Activations in a Network&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#fitting-batch-norm-into-a-neural-network&#34; &gt;Fitting Batch Norm into a Neural Network&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#why-does-batch-norm-work&#34; &gt;Why does Batch Norm work?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#batch-norm-at-test-time&#34; &gt;Batch Norm at Test Time&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#multi-class-classification&#34; &gt;Multi-class Classification&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#softmax-regression&#34; &gt;Softmax Regression&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#training-a-softmax-classifier&#34; &gt;Training a Softmax Classifier&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#programming-frameworks&#34; &gt;Programming Frameworks&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;hyperparameter-tuning&#34;&gt;&#xA;  Hyperparameter Tuning&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#hyperparameter-tuning&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;We have seen by now that neural networks have a lot of hyperparameters. Remember that hyperparameters remain fixed during training. This means that the process of finding reasonable hyperparameters, called hyperparameter tuning, is a process that is separate from training your model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Improving Deep Learning Networks: Week 2 | Optimization Algorithms</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/</link>
      <pubDate>Mon, 19 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week2/</guid>
      <description>&lt;p&gt;This week is focused on the optimization or training process. Particularly, this week goes over ways to make the training process faster and more efficient, allowing us to iterate quicker when trying different approaches.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#mini-batch-gradient-descent&#34; &gt;Mini-batch Gradient Descent&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#exponentially-weighted-moving-averages-ewma&#34; &gt;Exponentially Weighted Moving Averages (EWMA)&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#bias-correction&#34; &gt;Bias Correction&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gradient-descent-with-momentum&#34; &gt;Gradient Descent with Momentum&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#rmsprop&#34; &gt;RMSProp&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#adam&#34; &gt;Adam&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#learning-rate-decay&#34; &gt;Learning Rate Decay&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;mini-batch-gradient-descent&#34;&gt;&#xA;  Mini-batch Gradient Descent&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#mini-batch-gradient-descent&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;We discussed &lt;a href=&#34;https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week2/#gradient-descent&#34; &gt;gradient descent&lt;/a&gt; briefly on the second week of the first course. A key takeaway is that vanilla gradient descent is vectorized across all our training samples $x^{(m)}$. That is, every gradient step is taken after doing a forward and back propagation over our entire training data. This is relatively efficient when our data is small, but as our data grows it becomes a very big challenge. This is especially true with large neural networks or complex architectures.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Improving Deep Learning Networks: Week 1 | Practical Aspects of Deep Learning</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/</link>
      <pubDate>Sun, 18 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/improving-dnn-nets/week1/</guid>
      <description>&lt;p&gt;This is the first week in the &lt;a href=&#34;https://www.coursera.org/learn/deep-neural-network&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;second course&lt;/a&gt; of DeepLearning.AI&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning Specialization&lt;/a&gt; offered on Coursera. The course deals with hyperparameter tuning, regularization and the optimization of the training process. The optimization ranges from computational complexity to performance. This is again a pretty technical week, where you will benefit a lot from doing the programming exercises.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#setting-up-our-machine-learning-problem&#34; &gt;Setting up our Machine Learning problem&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#train--dev--test-sets&#34; &gt;Train / Dev / Test sets&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#bias-variance-tradeoff&#34; &gt;Bias-Variance Tradeoff&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#basic-recipe-for-machine-learning&#34; &gt;Basic Recipe for Machine Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#regularizing-our-neural-network&#34; &gt;Regularizing our Neural Network&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#regularization&#34; &gt;Regularization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#why-does-regularization-reduce-overfitting&#34; &gt;Why Does Regularization Reduce Overfitting?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#dropout-regularization&#34; &gt;Dropout Regularization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#understanding-dropout&#34; &gt;Understanding Dropout&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#other-regularization-methods&#34; &gt;Other Regularization Methods&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#setting-up-our-optimization-problem&#34; &gt;Setting up our Optimization Problem&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#normalizing-inputs&#34; &gt;Normalizing Inputs&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#vanishing-and-exploding-gradients&#34; &gt;Vanishing and Exploding Gradients&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#weight-initialization&#34; &gt;Weight Initialization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;setting-up-our-machine-learning-problem&#34;&gt;&#xA;  Setting up our Machine Learning problem&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#setting-up-our-machine-learning-problem&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;train--dev--test-sets&#34;&gt;&#xA;  Train / Dev / Test sets&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#train--dev--test-sets&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Machine learning projects are highly iterative. That is, you try something new, see how it does and then adjust; very much like gradient descent. Therefore, you want the iteration time to be quick so that you can try as many things as quickly as possible, without affecting the final performance of the model. Part of this is setting up your datasets correctly so that you can efficiently iterate over different approaches.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Neural Networks and Deep Learning: Week 4 | Deep Neural Networks</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/</link>
      <pubDate>Fri, 16 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week4/</guid>
      <description>&lt;p&gt;Final week of this course. Again this week is pretty technical and a lot of the learning is done while coding up your own examples via the weekly programming assignments. The purpose of this week is to extend previous&amp;rsquo; weeks ideas into $L$-layered networks.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#deep-l-layer-neural-network&#34; &gt;Deep L-Layer neural network&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#getting-your-matrix-dimensions-right&#34; &gt;Getting your matrix dimensions right&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#why-deep-representations&#34; &gt;Why deep representations?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#parameters-and-hyperparameters&#34; &gt;Parameters and Hyperparameters&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;deep-l-layer-neural-network&#34;&gt;&#xA;  Deep L-Layer neural network&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#deep-l-layer-neural-network&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The number of hidden layers in a neural network determine whether it is &amp;ldquo;shallow&amp;rdquo; or &amp;ldquo;deep&amp;rdquo;. Exactly how many layers is deep or shallow is not set in stone.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Neural Networks and Deep Learning: Week 3 | Shallow Neural Networks</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/</link>
      <pubDate>Thu, 15 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week3/</guid>
      <description>&lt;p&gt;This week&amp;rsquo;s focus is again very technical. Similar to the previous week, the focus is on the implementation of neural networks and on how to generalize your code from single-layer network to a multi-layer network, and also how to extend the ideas presented previously into deeper neural networks.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#overview&#34; &gt;Overview&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#neural-network-representation&#34; &gt;Neural Network Representation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#computing-a-neural-networks-output&#34; &gt;Computing a Neural Network&amp;rsquo;s Output&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#vectorizing-across-multiple-examples&#34; &gt;Vectorizing across multiple examples&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#activation-functions&#34; &gt;Activation functions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#random-initialization&#34; &gt;Random Initialization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;overview&#34;&gt;&#xA;  Overview&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#overview&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;It&amp;rsquo;s time to refine our notation and to disambiguate some concepts introduced in week 2. Let&amp;rsquo;s start with the notation used in the course.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Neural Networks and Deep Learning: Week 2 | Neural Network Basics</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week2/</link>
      <pubDate>Wed, 14 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week2/</guid>
      <description>&lt;p&gt;Here we kick off the second week of the first course in the specialization. This week is very technical and many of the details that are shown in the course will be lost in the summarization. Also, a lot of the content is based on you doing programming assignments. There is simply no substitute to getting your hands dirty.&lt;/p&gt;&#xA;&lt;p&gt;This week&amp;rsquo;s topics are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#binary-classification&#34; &gt;Binary Classification&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34; &gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#logistic-function&#34; &gt;Logistic Function&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#gradient-descent&#34; &gt;Gradient Descent&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#computation-graph&#34; &gt;Computation Graph&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#python-and-vectorization&#34; &gt;Python and Vectorization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#broadcasting&#34; &gt;Broadcasting&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;binary-classification&#34;&gt;&#xA;  Binary Classification&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#binary-classification&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Binary classification is a supervised learning approach where you train what&amp;rsquo;s called a &lt;em&gt;classifier&lt;/em&gt;. The binary classifier is a model that learns how to discriminate between two classes from the features, think about cats and dogs. A key concept is that of &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_separability&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;linearly separability&lt;/a&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Neural Networks and Deep Learning: Week 1 | Introduction to Deep Learning</title>
      <link>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week1/</link>
      <pubDate>Tue, 13 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/coursera/deep-learning-specialization/nn-dl/week1/</guid>
      <description>&lt;h2 id=&#34;introduction-to-deep-learning&#34;&gt;&#xA;  Introduction to Deep Learning&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#introduction-to-deep-learning&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;This is the first course in Coursera&amp;rsquo;s &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning Specialization&lt;/a&gt;. I will try to summarize the major topics presented in each of the weeks of each course in a series of posts. The purpose of this is to both deepen my own understanding by explaining, but also for people who have not taken the specialization. Hopefully these posts will inspire you to do so.&lt;/p&gt;</description>
    </item>
    <item>
      <title>VSCode and Tiny Instances over Remote SSH</title>
      <link>https://manmartgarc.github.io/posts/tech-support/vscode-ec2/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/tech-support/vscode-ec2/</guid>
      <description>&lt;h2 id=&#34;vscode-is-very-popular&#34;&gt;&#xA;  VSCode is very popular&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#vscode-is-very-popular&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;A lot of people like VSCode, 74% of respondents in &lt;a href=&#34;https://survey.stackoverflow.co/2022/#section-most-popular-technologies-integrated-development-environment&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StackOverflow&amp;rsquo;s 2022 Developer Survey&lt;/a&gt; said that VSCode is the IDE that they&amp;rsquo;ve used in the past year and also plan to keep using it. Why is VSCode liked so, and what do people like about it is not the focus of the post; but for sure it has to do with its extendable functionality, much of which comes from extensions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Application Mistakes and Information frictions in College Admissions</title>
      <link>https://manmartgarc.github.io/posts/mistakes/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/posts/mistakes/</guid>
      <description>&lt;p&gt;This is a working paper with &lt;a href=&#34;https://sites.google.com/view/anais-fabre&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anaïs Fabre&lt;/a&gt;, &lt;a href=&#34;https://tlarroucau.github.io/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tomas Larroucau&lt;/a&gt;, &lt;a href=&#34;https://christopherneilson.github.io/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Christopher Neilson&lt;/a&gt; and &lt;a href=&#34;https://iriosu.github.io/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ignacio Rios&lt;/a&gt;. The paper explores how information and beliefs have an effect on outcomes within the tertiary education market in Chile, which is a centralized college admissions system. The paper includes two waves of surveys done in 2019 and 2020 and an RCT done in 2021 with both government and NGOs. The abstract is below, and you can find the latest version of the working paper &lt;a href=&#34;https://tlarroucau.github.io/Mistakes_College.pdf&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Application Mistakes and Information frictions in College Admissions</title>
      <link>https://manmartgarc.github.io/projects/mistakes/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/projects/mistakes/</guid>
      <description></description>
    </item>
    <item>
      <title>About</title>
      <link>https://manmartgarc.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/about/</guid>
      <description>&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt; $\rightarrow$ &lt;a href=&#34;https://www.dropbox.com/s/oif6726jvsb3j15/martinez_cv.pdf?dl=0&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view my CV here&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;I am currently working as a data engineer at &lt;a href=&#34;https://aws.amazon.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon Web Services&lt;/a&gt;, where I focus on building large-scale, efficient, and maintainable data and machine learning pipelines.&lt;/p&gt;&#xA;&lt;p&gt;Back in June 2023 I graduated from the &lt;a href=&#34;https://capp.cs.uchicago.edu/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MSCAPP&lt;/a&gt; program, a graduate degree offered by the &lt;a href=&#34;https://harris.uchicago.edu/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Chicago Harris School of Public Policy&lt;/a&gt; and the &lt;a href=&#34;https://cs.uchicago.edu/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Chicago Department of Computer Science&lt;/a&gt;. The &lt;a href=&#34;https://capp.uchicago.edu/curriculum/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;curriculum&lt;/a&gt; offers a high degree of flexibility, allowing students to specialize; I chose to focus on computer science, machine learning and software engineering. While a student, I worked as a Research Engineer at the &lt;a href=&#34;https://miurban.uchicago.edu/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mansueto Institute for Urban Innovation at the University of Chicago&lt;/a&gt; helping researchers do their work more efficiently and maintainable.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CV</title>
      <link>https://manmartgarc.github.io/cv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://manmartgarc.github.io/cv/</guid>
      <description>&lt;p&gt;You can view my CV on &lt;a href=&#34;https://www.dropbox.com/s/oif6726jvsb3j15/martinez_cv.pdf?dl=0&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dropbox&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
